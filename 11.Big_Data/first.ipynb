{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644c8c98",
   "metadata": {},
   "source": [
    "No, we **cannot** divide the whole Big Data ecosystem into just two types: tools using **Spark engine** and tools using **MapReduce engine**. That’s **oversimplified** and **not accurate**.\n",
    "\n",
    "Instead, Big Data tools span **multiple layers** with **varied roles**:\n",
    "\n",
    "### 1. **Storage Layer**\n",
    "\n",
    "* **HDFS** (used by both MapReduce and Spark)\n",
    "* **HBase**, **Hive tables**, **NoSQL DBs**\n",
    "\n",
    "### 2. **Processing Engines**\n",
    "\n",
    "* **MapReduce** → older batch processing\n",
    "* **Spark** → fast, in-memory, batch + stream\n",
    "* **Flink**, **Storm**, **Tez**, **Samza** → other engines\n",
    "\n",
    "### 3. **Query Engines**\n",
    "\n",
    "* **Hive** (can run on MapReduce, Tez, or Spark)\n",
    "* **Presto**, **Impala** → not Spark or MapReduce\n",
    "* **Drill**\n",
    "\n",
    "### 4. **Workflow Schedulers**\n",
    "\n",
    "* **Oozie**, **Airflow**, **Azkaban**\n",
    "\n",
    "### 5. **Ingestion Tools**\n",
    "\n",
    "* **Sqoop**, **Flume**, **Kafka**, **NiFi**\n",
    "\n",
    "### 6. **Cluster Managers**\n",
    "\n",
    "* **YARN**, **Mesos**, **Kubernetes**\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Some tools **can use both engines** (e.g., Hive can run with MapReduce, Tez, or Spark). So, the correct view is:\n",
    "\n",
    "> **There are multiple engines** (Spark, MapReduce, Tez, Flink, etc.), and **many tools** that either sit on top of them or work independently.\n",
    "\n",
    "You **cannot** cleanly divide all tools into \"Spark-based\" and \"MapReduce-based\" buckets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e6b51e",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78719584",
   "metadata": {},
   "source": [
    "\n",
    "## Complete Technology Matrix by Use Case\n",
    "\n",
    "| **Use Case** | **Primary Technologies** |\n",
    "|--------------|-------------------------|\n",
    "| **Real-time Analytics** | Kafka + Flink + Druid/ClickHouse |\n",
    "| **Data Lake Architecture** | S3/HDFS + Spark + Airflow + Glue/Atlas |\n",
    "| **ML Pipeline** | Spark + TensorFlow/PyTorch + MLflow + SageMaker |\n",
    "| **BI Dashboard** | BigQuery/Snowflake + dbt + Tableau/Power BI |\n",
    "| **Event-driven Architecture** | Kafka + Flink + Cassandra/DynamoDB |\n",
    "| **Traditional Data Warehouse** | Sqoop + Hive + Airflow + Tableau |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448c9cd",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b36624",
   "metadata": {},
   "source": [
    "# Big Data Ecosystem - Complete Guide\n",
    "\n",
    "## Section 1: Foundation Concepts\n",
    "\n",
    "### What is Big Data\n",
    "- **Volume, Velocity, Variety** - the 3 V's\n",
    "- **Distributed computing basics** - spreading work across multiple machines\n",
    "- **CAP theorem** - Consistency, Availability, Partition tolerance (pick 2)\n",
    "- **Horizontal vs Vertical scaling** - adding more machines vs bigger machines\n",
    "- **Master-slave architecture patterns** - coordinator nodes + worker nodes\n",
    "\n",
    "### Core Insight: Web vs Big Data Similarities\n",
    "Both domains solve the same fundamental challenges:\n",
    "- **Real-Time Messaging**: WebSockets ↔ Kafka (bidirectional communication)\n",
    "- **Performance**: CDN ↔ Redis/Caching (distribute closer to consumers)  \n",
    "- **Architecture**: Microservices ↔ Data Mesh (independent, manageable services)\n",
    "- **Communication**: REST APIs ↔ Data APIs (request-response patterns)\n",
    "- **Events**: Pub/Sub ↔ Kafka Topics (broadcast to subscribers)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c994d",
   "metadata": {},
   "source": [
    "## section 2: Key Technology Evolution Summary\n",
    "\n",
    "**Storage Evolution**: Local disks → HDFS → Cloud object storage (S3, etc.)\n",
    "\n",
    "**Processing Evolution**: MapReduce (disk) → Spark engine (memory) → Specialized engines → Cloud services\n",
    "\n",
    "**Architecture Evolution**: Monolithic → Hadoop ecosystem → Cloud-native → Lakehouse architecture\n",
    "\n",
    "### Modern Architecture Patterns\n",
    "\n",
    "**Traditional Data Warehouse**:\n",
    "```\n",
    "Data Sources → Sqoop → HDFS → Hive → Airflow → Tableau\n",
    "```\n",
    "\n",
    "**Modern Lakehouse (Databricks)**:\n",
    "```\n",
    "Data Sources → Databricks Lakehouse → All Analytics & AI\n",
    "```\n",
    "\n",
    "**Real-time Analytics**:\n",
    "```\n",
    "Kafka + Flink + Druid/ClickHouse\n",
    "```\n",
    "\n",
    "**ML Pipeline**:\n",
    "```\n",
    "Spark engine + TensorFlow/PyTorch + MLflow + SageMaker\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Sqoop Deep Dive\n",
    "\n",
    "### What Sqoop Actually Uses\n",
    "- **JDBC drivers** to connect to databases (MySQL, PostgreSQL, Oracle)\n",
    "- **MapReduce jobs** to parallelize data transfer\n",
    "- **Code generation** - creates Java POJOs (not full ORM)\n",
    "\n",
    "### Why POJOs, Not ORM?\n",
    "Sqoop moves millions/billions of rows efficiently. ORM overhead would hurt performance.\n",
    "\n",
    "| Aspect | ORM Approach | Sqoop's POJO Approach |\n",
    "|--------|--------------|----------------------|\n",
    "| **Speed** | Slower - ORM overhead | Faster - Direct mapping |\n",
    "| **Memory** | Higher - metadata/proxies | Lower - just data fields |\n",
    "| **Parallelization** | Complex - session conflicts | Simple - stateless POJOs |\n",
    "| **Scalability** | Limited - app-level | Unlimited - scales with cluster |\n",
    "\n",
    "### Sqoop Import Example\n",
    "```bash\n",
    "sqoop import \\\n",
    "  --connect jdbc:mysql://localhost:3306/retail_db \\\n",
    "  --username root --password cloudera \\\n",
    "  --m 2 --split-by customer_id \\\n",
    "  --table customers \\\n",
    "  --target-dir /user/cloudera/data_import\n",
    "```\n",
    "\n",
    "**Key Points**:\n",
    "- Default mappers = 4 if not specified\n",
    "- Must specify --split-by when mappers > 1\n",
    "- Uses primary key as split-by if available\n",
    "- Map-only jobs (no reducers needed for direct copy)\n",
    "\n",
    "### Why Map-Only Jobs for Sqoop?\n",
    "1. **No aggregation needed** - direct copy from source to target\n",
    "2. **Parallel direct writes** - each mapper writes to HDFS\n",
    "3. **Maximum performance** - no shuffle/sort overhead\n",
    "4. **Data partitioning** - splits by primary key ranges\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99fca0",
   "metadata": {},
   "source": [
    "## Section 3: Processing Engines\n",
    "\n",
    "### Hadoop Evolution Context\n",
    "**Hadoop 1.x (2011-2012)**:\n",
    "- Core: HDFS + MapReduce only\n",
    "- MapReduce handles both processing AND resource management\n",
    "- Limitation: Single point of failure, batch processing only\n",
    "\n",
    "**Hadoop 2.x (2013-2017)**:\n",
    "- Major Addition: YARN introduced\n",
    "- Key Change: Separated resource management from processing\n",
    "- Benefits: Multi-tenancy, better resource utilization, NameNode HA\n",
    "\n",
    "**Hadoop 3.x (2017-present)**:\n",
    "- Storage: Erasure coding reduces overhead from 200% to 50%\n",
    "- Performance: Better resource utilization, GPU scheduling\n",
    "- Features: Multiple NameNodes, enhanced security, Java 8+\n",
    "\n",
    "### 1. MapReduce Engine\n",
    "**What it is**: Original distributed processing engine (2004)\n",
    "\n",
    "**How it works**:\n",
    "- **Map phase** → process data in parallel across nodes\n",
    "- **Reduce phase** → aggregate results from map tasks\n",
    "\n",
    "**Key characteristics**:\n",
    "- Disk-based processing (slow but reliable)\n",
    "- Fault tolerant through task re-execution\n",
    "- Simple programming model\n",
    "- Batch processing only\n",
    "- Status: Largely replaced by Spark engine\n",
    "\n",
    "**MapReduce Example Workflow**:\n",
    "```python\n",
    "# Mapper reads CSV, computes revenue per country\n",
    "Country → Revenue (parallel processing)\n",
    "\n",
    "# Hadoop framework shuffles and sorts by country\n",
    "# Reducer aggregates total revenue per country\n",
    "Country → Total_Revenue\n",
    "```\n",
    "\n",
    "### 2. Spark Engine  \n",
    "**What it is**: Distributed processing engine with in-memory capabilities\n",
    "\n",
    "**Key advantages over MapReduce**:\n",
    "- **In-memory processing** vs disk-based\n",
    "- **Faster execution** - keeps data in memory between operations\n",
    "- **Unified engine** that supports multiple interfaces and APIs\n",
    "\n",
    "**Technologies that run on Spark engine**:\n",
    "- **Spark SQL** - SQL interface for structured data\n",
    "- **Spark Streaming** - real-time stream processing interface\n",
    "- **MLlib** - machine learning library\n",
    "- **GraphX** - graph processing library\n",
    "\n",
    "### 3. Other Processing Engines\n",
    "**Apache Flink**:\n",
    "- Stream-first processing engine\n",
    "- Low-latency stream processing\n",
    "- Event time processing\n",
    "\n",
    "**Apache Storm**:\n",
    "- Real-time stream processing engine\n",
    "- Tuple-based processing model\n",
    "- Low-latency guarantees\n",
    "\n",
    "**Presto/Trino**:\n",
    "- Distributed SQL query engine\n",
    "- Interactive analytics\n",
    "- Cross-data source queries\n",
    "\n",
    "**Apache Beam**:\n",
    "- Unified programming model\n",
    "- Batch and stream processing\n",
    "- Portable across engines\n",
    "\n",
    "### Hive vs Spark SQL Comparison\n",
    "| Feature | Hive (on MapReduce) | Spark SQL |\n",
    "|---------|-------------------|-----------|\n",
    "| **Engine** | Uses MapReduce | Uses Spark engine (in-memory) |\n",
    "| **Speed** | Slower (reads/writes to disk) | Faster (in-memory processing) |\n",
    "| **Latency** | High (batch only) | Low (supports real-time) |\n",
    "| **Use Case** | Good for batch processing | Good for batch + streaming |\n",
    "\n",
    "**Bottom line**: Use Hive for slow batch jobs, Spark SQL for fast flexible SQL processing.\n",
    "\n",
    "**Note**: Spark is the **engine**, Spark SQL is the **SQL interface** on top of the Spark engine.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf09d05",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Section 4: Distributed Storage\n",
    "\n",
    "### HDFS (Hadoop Distributed File System)\n",
    "**What it is**: Distributed file system (not a database) - foundational component\n",
    "\n",
    "**Key Characteristics**:\n",
    "- **Distributed storage** across commodity hardware\n",
    "- **Fault tolerant** with automatic replication (typically 3x)\n",
    "- **Large file optimization** for big files & sequential reads\n",
    "- **Write-once, read-many** access pattern\n",
    "- **Block-based** with default 128MB block size\n",
    "\n",
    "#### 1. MapReduce Engine Technologies on HDFS:\n",
    "- **Apache Hive** → SQL-like queries over HDFS using MapReduce\n",
    "- **Apache HBase** → NoSQL database on HDFS with MapReduce integration\n",
    "- **Traditional Hadoop jobs** → Direct MapReduce programming\n",
    "\n",
    "#### 2. Spark Engine Technologies on HDFS:\n",
    "- **Spark SQL** → Fast SQL queries on HDFS data\n",
    "- **Spark Core** → Direct Spark applications reading/writing HDFS\n",
    "- **MLlib** → Machine learning on HDFS datasets\n",
    "- **Spark Streaming** → Stream processing with HDFS checkpointing\n",
    "\n",
    "#### 3. Other Engine Technologies on HDFS:\n",
    "- **Apache Impala** → Real-time SQL with own MPP engine\n",
    "- **Presto/Trino** → Distributed SQL query engine\n",
    "- **Apache Drill** → Schema-free SQL query engine\n",
    "\n",
    "### Cloud Storage Solutions\n",
    "| **Provider** | **Storage Service** |\n",
    "|--------------|-------------------|\n",
    "| **AWS** | S3, Redshift, DynamoDB |\n",
    "| **Azure** | Data Lake, Synapse, Cosmos DB |\n",
    "| **Google Cloud** | Cloud Storage, BigQuery |\n",
    "| **Commercial** | Snowflake, MongoDB |\n",
    "\n",
    "### Other Distributed Storage Systems\n",
    "- **Apache Cassandra** (NoSQL distributed)\n",
    "- **Amazon S3** (object storage service)\n",
    "- **Apache Ceph** (unified storage system)\n",
    "- **MinIO** (S3-compatible object storage)\n",
    "\n",
    "### Big Data Sequence File Formats\n",
    "\n",
    "**Columnar Storage Formats** (60-80% compression):\n",
    "- **Apache Parquet** - Open source columnar format, excellent for analytics\n",
    "- **Apache ORC** (Optimized Row Columnar) - Hadoop-optimized columnar format\n",
    "- **Apache Arrow** - In-memory columnar format for fast processing\n",
    "\n",
    "**Row-based Storage Formats** (40-60% compression):\n",
    "- **Apache Avro** - Row-based with schema evolution support\n",
    "- **Hadoop Sequence Files** - Key-value pairs, Hadoop-native binary format\n",
    "- **Apache Thrift** - Cross-language serialization framework\n",
    "- **Protocol Buffers (protobuf)** - Google's binary serialization format\n",
    "\n",
    "**Hybrid/Table Formats** (60-75% compression):\n",
    "- **Delta Lake** - ACID transactions on data lakes\n",
    "- **Apache Iceberg** - Table format with schema evolution and time travel\n",
    "- **Apache Hudi** - Incremental data processing on data lakes\n",
    "\n",
    "**Compressed Binary Formats**:\n",
    "- **MessagePack** - Efficient binary serialization\n",
    "- **Apache Arrow Flight** - High-performance data transport\n",
    "- **FlatBuffers** - Zero-copy serialization library\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261215c5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Section 5: Resource Managers\n",
    "\n",
    "### YARN (Yet Another Resource Negotiator)\n",
    "**What it does**: Manages cluster resources and job scheduling\n",
    "\n",
    "**Key functions**:\n",
    "- Allocates containers on cluster nodes for map/reduce tasks\n",
    "- Tracks task status and handles failures\n",
    "- Enables multiple applications to run simultaneously\n",
    "- Provides better resource utilization than Hadoop 1.x\n",
    "\n",
    "#### 1. MapReduce Engine on YARN:\n",
    "- **Traditional Hadoop MapReduce** → Native YARN application\n",
    "- **Apache Hive on MapReduce** → Uses YARN for resource allocation\n",
    "- **Apache Pig** → MapReduce-based data flow language on YARN\n",
    "\n",
    "#### 2. Spark Engine on YARN:\n",
    "- **Spark applications** → Run as YARN applications\n",
    "- **Spark SQL** → Managed by YARN resource scheduler\n",
    "- **Spark Streaming** → Long-running YARN applications\n",
    "- **MLlib jobs** → Distributed ML training on YARN cluster\n",
    "\n",
    "#### 3. Other Engines on YARN:\n",
    "- **Apache Flink** → Can run on YARN cluster\n",
    "- **Apache Storm** → YARN integration available\n",
    "- **Apache Tez** → Optimized execution engine on YARN\n",
    "\n",
    "### Other Resource Management Systems\n",
    "\n",
    "#### Apache Mesos\n",
    "- **What it is**: Datacenter operating system for resource abstraction\n",
    "- **Engines supported**: Spark, Marathon, Chronos, Hadoop\n",
    "\n",
    "#### Kubernetes  \n",
    "- **What it is**: Container orchestration platform\n",
    "- **Engines supported**: Spark (Spark on K8s), Flink, custom containerized applications\n",
    "\n",
    "#### Standalone Cluster Managers\n",
    "- **Spark Standalone** → Built-in cluster manager for Spark engine only\n",
    "- **Flink Standalone** → Native cluster manager for Flink applications\n",
    "\n",
    "### Cloud Resource Managers\n",
    "| **Provider** | **Service** | **Engines Supported** |\n",
    "|--------------|-------------|----------------------|\n",
    "| **AWS** | EMR, ECS, EKS | Spark, Hadoop, Flink, Presto |\n",
    "| **Azure** | HDInsight, AKS | Spark, Hadoop, Kafka, HBase |\n",
    "| **Google Cloud** | Dataproc, GKE | Spark, Hadoop, Flink, Beam |\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca43ce6",
   "metadata": {},
   "source": [
    "\n",
    "## Section 6: Types of Processing (Categorized by Engine)\n",
    "\n",
    "### 1. Batch Processing\n",
    "**What**: Process large volumes of data at rest\n",
    "**When**: Scheduled jobs, ETL, historical analysis\n",
    "**Example**: Daily sales reports, data warehouse loading\n",
    "\n",
    "#### MapReduce Engine Technologies:\n",
    "- **Hadoop MapReduce** → Traditional batch processing\n",
    "- **Apache Hive** → SQL on MapReduce for batch analytics\n",
    "- **Apache Pig** → Data flow scripting on MapReduce\n",
    "\n",
    "#### Spark Engine Technologies:\n",
    "- **Spark Core** → Fast in-memory batch processing\n",
    "- **Spark SQL** → SQL-based batch analytics\n",
    "- **MLlib** → Batch machine learning training\n",
    "\n",
    "#### Other Engine Technologies:\n",
    "- **Apache Tez** → Optimized batch execution engine\n",
    "- **AWS EMR** → Managed batch processing service\n",
    "- **Google Dataflow** → Serverless batch processing\n",
    "\n",
    "### 2. Stream Processing  \n",
    "**What**: Process continuous data streams in real-time\n",
    "**When**: Real-time analytics, monitoring, alerts\n",
    "**Example**: Fraud detection, live dashboards\n",
    "\n",
    "#### MapReduce Engine Technologies:\n",
    "- **None** → MapReduce not suitable for stream processing\n",
    "\n",
    "#### Spark Engine Technologies:\n",
    "- **Spark Streaming** → Micro-batch stream processing on Spark\n",
    "- **Structured Streaming** → Unified batch/stream API\n",
    "\n",
    "#### Other Engine Technologies:\n",
    "- **Apache Flink** → Native stream processing engine\n",
    "- **Apache Storm** → Real-time stream processing\n",
    "- **Kafka Streams** → Stream processing library\n",
    "- **AWS Kinesis Analytics** → Managed stream processing\n",
    "- **Azure Stream Analytics** → Cloud stream processing\n",
    "- **Google Dataflow** → Unified batch/stream processing\n",
    "\n",
    "### 3. SQL Processing\n",
    "**What**: Distributed SQL queries across large datasets  \n",
    "**When**: Interactive analytics, business intelligence\n",
    "**Example**: Ad-hoc queries, reporting\n",
    "\n",
    "#### MapReduce Engine Technologies:\n",
    "- **Apache Hive** → SQL on MapReduce (HiveQL)\n",
    "- **Apache Drill** → Schema-free SQL on various engines\n",
    "\n",
    "#### Spark Engine Technologies:\n",
    "- **Spark SQL** → SQL interface on Spark engine\n",
    "- **DataFrame API** → Programmatic SQL on Spark\n",
    "\n",
    "#### Other Engine Technologies:\n",
    "- **Presto/Trino** → Distributed SQL query engine\n",
    "- **Apache Impala** → Real-time SQL with MPP architecture\n",
    "- **AWS Redshift** → Cloud data warehouse\n",
    "- **Azure Synapse** → Analytics service\n",
    "- **Google BigQuery** → Serverless data warehouse\n",
    "- **Snowflake** → Cloud data platform\n",
    "\n",
    "### 4. Machine Learning Processing\n",
    "**What**: Distributed training and inference\n",
    "**When**: Large-scale ML model training\n",
    "**Example**: Training recommendation systems\n",
    "\n",
    "#### MapReduce Engine Technologies:\n",
    "- **Apache Mahout** → ML algorithms on MapReduce (mostly deprecated)\n",
    "\n",
    "#### Spark Engine Technologies:\n",
    "- **MLlib** → Spark's machine learning library\n",
    "- **Spark ML Pipelines** → ML workflow management\n",
    "\n",
    "#### Other Engine Technologies:\n",
    "- **TensorFlow Distributed** → Google's ML framework\n",
    "- **PyTorch Distributed** → Facebook's ML framework\n",
    "- **Apache MXNet** → Flexible deep learning framework\n",
    "- **AWS SageMaker** → Managed ML platform\n",
    "- **Azure Machine Learning** → Cloud ML service\n",
    "- **Google AI Platform** → Managed ML service\n",
    "\n",
    "### 5. Graph Processing\n",
    "**What**: Analyze relationships and connections\n",
    "**When**: Social networks, recommendation engines\n",
    "**Example**: Friend recommendations, fraud networks\n",
    "\n",
    "#### MapReduce Engine Technologies:\n",
    "- **Apache Giraph** → Graph processing on Hadoop\n",
    "\n",
    "#### Spark Engine Technologies:\n",
    "- **GraphX** → Graph processing library on Spark\n",
    "\n",
    "#### Other Engine Technologies:\n",
    "- **Apache TinkerPop** → Graph computing framework\n",
    "- **Neo4j** → Native graph database\n",
    "- **Amazon Neptune** → Managed graph database\n",
    "\n",
    "### Data Movement & Integration Technologies\n",
    "**Data Ingestion**:\n",
    "- Open Source: Apache Kafka, NiFi, Sqoop\n",
    "- AWS: Kinesis, DMS\n",
    "- Azure: Event Hubs, Data Factory\n",
    "- Google Cloud: Pub/Sub, Dataflow\n",
    "\n",
    "### OLTP vs OLAP\n",
    "**OLTP (Online Transaction Processing)**: Real-time high-volume transactions\n",
    "- Tools: Cassandra, HBase, DynamoDB, MongoDB, CockroachDB\n",
    "\n",
    "**OLAP (Online Analytical Processing)**: Complex analytical queries  \n",
    "- Tools: Spark, Hive, Redshift, BigQuery, Druid, Snowflake\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3d1f9",
   "metadata": {},
   "source": [
    "**Spark vs PyTorch Distributed - Completely Different Engines:**\n",
    "\n",
    "**Apache Spark:**\n",
    "- Data processing engine (ETL, analytics)\n",
    "- Processes large datasets across clusters\n",
    "- Used for data preprocessing, feature engineering\n",
    "\n",
    "**PyTorch Distributed:**\n",
    "- Deep learning training framework\n",
    "- Trains neural networks across multiple GPUs/nodes\n",
    "- Handles gradient synchronization, model parallelism\n",
    "\n",
    "**They're Different but Complementary:**\n",
    "- Spark: Prepares data at scale\n",
    "- PyTorch: Trains models on that data\n",
    "- Often used together in ML pipelines\n",
    "\n",
    "**Bottom line:** Spark = data engine, PyTorch = model training engine. Different purposes, can work together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f92eac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
