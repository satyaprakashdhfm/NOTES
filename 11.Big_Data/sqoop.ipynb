{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca461ef9",
   "metadata": {},
   "source": [
    "- sqoop is only useful for moving data into and out of hdfs. thats it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48179806",
   "metadata": {},
   "source": [
    "## Sqoop Complete Reference Notes\n",
    "\n",
    "**1. Importing from SQL Database Table**\n",
    "\n",
    "• Basic import syntax uses JDBC connection to relational databases\n",
    "  - `sqoop import --connect jdbc:mysql://localhost/database_name --username user --password password --table table_name --target-dir /user/hdfs/target_directory`\n",
    "  - Sqoop uses JDBC drivers to connect to all major databases (MySQL, PostgreSQL, Oracle, SQL Server)\n",
    "  - Data imported as text files by default into HDFS\n",
    "  - Creates one file per mapper in the target directory\n",
    "  - Automatically generates Java classes for the imported table structure\n",
    "\n",
    "• Key connection parameters\n",
    "  - `--connect`: JDBC URL specifying database type, host, port, and database name\n",
    "  - `--username` / `--password`: Database authentication credentials  \n",
    "  - `--table`: Source table name in the database\n",
    "  - `--target-dir`: Specific HDFS destination directory path\n",
    "  - `--warehouse-dir`: Parent directory where multiple table imports are stored\n",
    "\n",
    "**2. Mappers and Primary Key Configuration**\n",
    "\n",
    "• Default mapper behavior\n",
    "  - Sqoop uses 4 mappers by default for parallel data processing\n",
    "  - Automatically uses table's primary key for data splitting across mappers\n",
    "  - Each mapper processes a specific range of primary key values\n",
    "  - Import fails if no primary key exists and multiple mappers specified\n",
    "  - Primary key splitting ensures even data distribution across mappers\n",
    "\n",
    "• Controlling mapper configuration\n",
    "  - `--num-mappers 2` or `-m 2`: Specify exact number of mappers\n",
    "  - `--split-by employee_id`: Use custom column for data splitting instead of primary key\n",
    "  - `-m 1`: Single mapper mode (no primary key requirement)\n",
    "    * Useful for small tables or tables without primary keys\n",
    "    * Slower but safer for problematic data distributions\n",
    "\n",
    "• Split column requirements and considerations\n",
    "  - Must be numeric, date, or timestamp data type\n",
    "  - Should have good data distribution across value ranges\n",
    "  - Avoid columns with many null values or highly skewed data\n",
    "  - More mappers increase speed but also increase database server load\n",
    "  - Optimal mapper count usually 2-4 per CPU core in cluster\n",
    "\n",
    "**3. Importing Partial Data**\n",
    "\n",
    "• WHERE clause filtering\n",
    "  - `--where \"salary > 50000 AND department = 'IT'\"`: Filters data at database source\n",
    "  - More efficient than importing all data and filtering in Hadoop\n",
    "  - Reduces network traffic and storage requirements\n",
    "  - Can use any valid SQL WHERE clause conditions\n",
    "  - Filtering happens before data transfer, improving performance\n",
    "\n",
    "• Column selection\n",
    "  - `--columns \"id,name,salary,department\"`: Import only specified columns\n",
    "  - Significantly reduces data transfer and storage needs\n",
    "  - Column names must match exact database column names\n",
    "  - Order of columns in output follows the specified sequence\n",
    "  - Useful for excluding large text or binary columns\n",
    "\n",
    "• Custom query imports\n",
    "  - `--query \"SELECT id, name, salary FROM employees WHERE salary > 50000 AND \\$CONDITIONS\"`: Free-form SQL queries\n",
    "  - Allows complex joins, calculations, and transformations\n",
    "  - `$CONDITIONS` placeholder is mandatory for parallel execution\n",
    "    * Sqoop replaces this with appropriate conditions for each mapper\n",
    "    * Required even if query doesn't actually need additional conditions\n",
    "  - Must specify `--target-dir` explicitly (cannot use `--warehouse-dir`)\n",
    "  - Cannot be used with `--table` parameter\n",
    "\n",
    "**4. Field Separators and Delimiters**\n",
    "\n",
    "• Default delimiter behavior\n",
    "  - Field separator: Comma (,)\n",
    "  - Record separator: Newline (\\n)\n",
    "  - Escape character: Backslash (\\)\n",
    "  - No field enclosure by default\n",
    "\n",
    "• Custom delimiter configuration\n",
    "  - `--fields-terminated-by '\\t'`: Tab-separated values (cleaner for most data)\n",
    "  - `--lines-terminated-by '\\n'`: Custom record separator\n",
    "  - `--escaped-by '\\\\'`: Character to escape special characters\n",
    "  - `--enclosed-by '\"'`: Enclose all fields with specified character\n",
    "  - `--optionally-enclosed-by '\"'`: Enclose only fields containing delimiters\n",
    "\n",
    "• Delimiter selection best practices\n",
    "  - Choose delimiters that don't appear in your actual data\n",
    "  - Tab separation (\\t) is common choice for cleaner data files\n",
    "  - Use field enclosure when data contains delimiter characters\n",
    "  - Proper escaping prevents data corruption during import\n",
    "  - Consider downstream processing tool requirements\n",
    "\n",
    "**5. Sqoop Eval and Connectors**\n",
    "\n",
    "• Sqoop eval functionality\n",
    "  - `sqoop eval --connect jdbc:mysql://localhost/testdb --username root --password password --query \"SELECT COUNT(*) FROM employees\"`\n",
    "  - Executes simple queries without creating HDFS files\n",
    "  - Uses single database connection (no parallelism)\n",
    "  - Read-only operations only\n",
    "  - Lightweight alternative for quick data validation\n",
    "\n",
    "• Primary use cases\n",
    "  - Testing database connectivity before full imports\n",
    "  - Quick data validation and sample queries\n",
    "  - Schema verification and column analysis\n",
    "  - Simple aggregations and counts\n",
    "  - Debugging connection and authentication issues\n",
    "\n",
    "• Limitations and considerations\n",
    "  - No parallel execution capabilities\n",
    "  - Cannot create HDFS files or Java classes\n",
    "  - Limited to simple SELECT statements\n",
    "  - Not suitable for large result sets\n",
    "  - Results displayed in console only\n",
    "\n",
    "**6. Incremental Import and Sqoop Jobs**\n",
    "\n",
    "• Incremental import modes\n",
    "  - **Append mode**: For insert-only tables that only grow\n",
    "    * `--incremental append --check-column order_id --last-value 1000`\n",
    "    * Imports only records where check column > last value\n",
    "    * Suitable for log tables, transaction tables\n",
    "    * Check column should be auto-incrementing integer\n",
    "  - **Last-modified mode**: For tables with updates and inserts\n",
    "    * `--incremental lastmodified --check-column last_update --last-value \"2023-01-01 00:00:00\"`\n",
    "    * Imports records modified after specified timestamp\n",
    "    * Requires timestamp or date column tracking modifications\n",
    "    * Handles both new and updated records\n",
    "\n",
    "• Sqoop jobs for automation\n",
    "  - `sqoop job --create job_name -- import --table employees --incremental append --check-column id --last-value 0`\n",
    "  - `sqoop job --exec job_name`: Execute previously created job\n",
    "  - `sqoop job --list`: Display all created jobs\n",
    "  - `sqoop job --show job_name`: Show job configuration\n",
    "  - `sqoop job --delete job_name`: Remove job definition\n",
    "\n",
    "• Job benefits and considerations\n",
    "  - Automatically tracks and updates last imported value\n",
    "  - Saves significant time and bandwidth for large tables\n",
    "  - Jobs stored in metastore for persistence\n",
    "  - Check column should be indexed for optimal performance\n",
    "  - Suitable for regular ETL processes and data synchronization\n",
    "\n",
    "**7. Password Protection Methods**\n",
    "\n",
    "• Password file approach\n",
    "  - Create password file: `echo \"mypassword\" | hdfs dfs -put - /user/sqoop/password.txt`\n",
    "  - Set restrictive permissions: `hdfs dfs -chmod 600 /user/sqoop/password.txt`\n",
    "  - Use in sqoop command: `--password-file /user/sqoop/password.txt`\n",
    "  - Password not visible in command history or process lists\n",
    "  - File can be stored in HDFS or local filesystem\n",
    "\n",
    "• Security best practices\n",
    "  - Never use plain text passwords in production scripts\n",
    "  - Set file permissions to 600 (read-write for owner only)\n",
    "  - Consider using Hadoop credential providers for additional encryption\n",
    "  - Store password files in secure HDFS locations\n",
    "  - Regularly rotate passwords and update files accordingly\n",
    "\n",
    "• Alternative security methods\n",
    "  - Kerberos authentication for enterprise environments\n",
    "  - Database connection pooling with encrypted credentials\n",
    "  - Integration with external key management systems\n",
    "  - Using database service accounts with minimal privileges\n",
    "\n",
    "**8. Last Modified Column Handling**\n",
    "\n",
    "• Implementation requirements\n",
    "  - `--incremental lastmodified --check-column last_modified_date --last-value \"2023-12-01 00:00:00\"`\n",
    "  - Table must have timestamp, datetime, or date column\n",
    "  - Column should be automatically updated on record modifications\n",
    "  - `--merge-key product_id`: Specify primary key for handling updates\n",
    "\n",
    "• Technical considerations\n",
    "  - Supported data types: timestamp, datetime, date columns\n",
    "  - Time zone consistency crucial for accurate incremental imports\n",
    "  - Last-modified column should be indexed for performance\n",
    "  - Handle database triggers or application-level timestamp updates\n",
    "  - Consider clock synchronization between database and Hadoop cluster\n",
    "\n",
    "• Merge operations for updates\n",
    "  - Use `--merge-key` to handle updated records properly\n",
    "  - Merge combines new and updated records with existing data\n",
    "  - Requires additional MapReduce job for merge operation\n",
    "  - More complex but handles data changes accurately\n",
    "\n",
    "**9. File Formats and Storage Options**\n",
    "\n",
    "• Text format (default)\n",
    "  - Human readable and debuggable\n",
    "  - Compatible with all Hadoop ecosystem tools\n",
    "  - Larger file sizes and slower processing\n",
    "  - Good for initial development and testing\n",
    "\n",
    "• Sequence file format\n",
    "  - `--as-sequencefile`: Binary format optimized for MapReduce\n",
    "  - Splittable for parallel processing\n",
    "  - Supports compression efficiently\n",
    "  - Not human readable but better performance\n",
    "\n",
    "• Avro format\n",
    "  - `--as-avrodatafile`: Schema evolution support\n",
    "  - Cross-platform compatibility\n",
    "  - Schema embedded in file header\n",
    "  - Compact binary format good for streaming\n",
    "\n",
    "• Parquet format\n",
    "  - `--as-parquetfile`: Columnar storage optimized for analytics\n",
    "  - Excellent compression ratios\n",
    "  - Fast query performance for analytical workloads\n",
    "  - Best choice for read-heavy analytical processing\n",
    "\n",
    "• ORC format\n",
    "  - `--as-orcfile`: Optimized Row Columnar format\n",
    "  - Designed for Hive integration\n",
    "  - Supports ACID transactions\n",
    "  - Excellent compression and performance in Hive ecosystem\n",
    "  - Built-in indexing and statistics\n",
    "\n",
    "• Format selection guidelines\n",
    "  - **Text**: Development, debugging, simple processing\n",
    "  - **Sequence**: General MapReduce processing\n",
    "  - **Avro**: Schema evolution, cross-platform needs\n",
    "  - **Parquet**: Analytics, BI tools, columnar analysis\n",
    "  - **ORC**: Hive-centric environments, ACID requirements\n",
    "\n",
    "**10. Multiple Tables and Null Value Handling**\n",
    "\n",
    "• Importing all tables\n",
    "  - `sqoop import-all-tables --connect jdbc:mysql://localhost/testdb --username root --password password --warehouse-dir /user/data/`\n",
    "  - Imports every table in specified database\n",
    "  - `--exclude-tables table1,table2`: Skip specific tables\n",
    "  - Tables imported sequentially, not in parallel\n",
    "  - Each table creates separate directory under warehouse\n",
    "\n",
    "• Null value representation\n",
    "  - `--null-string 'NULL'`: Representation for null string values\n",
    "  - `--null-non-string '-999'`: Representation for null numeric/date values\n",
    "  - Choose representations not present in actual data\n",
    "  - Consistent null handling crucial for downstream processing\n",
    "  - Different settings for string vs non-string data types\n",
    "\n",
    "• Multi-table considerations\n",
    "  - Each table import runs as separate job\n",
    "  - Total time is sum of individual table import times\n",
    "  - Database connections used efficiently across imports\n",
    "  - Monitor database server load during multi-table imports\n",
    "  - Consider foreign key relationships and import order\n",
    "\n",
    "**11. Sqoop Export with Staging Tables**\n",
    "\n",
    "• Basic export functionality\n",
    "  - `sqoop export --connect jdbc:mysql://localhost/testdb --table target_table --export-dir /user/data/employees`\n",
    "  - Transfers data from HDFS to relational database tables\n",
    "  - Reverse operation of import process\n",
    "  - Requires pre-existing target table with compatible schema\n",
    "\n",
    "• Staging table approach\n",
    "  - `--staging-table staging_table --clear-staging-table`: Use intermediate staging table\n",
    "  - Data loaded to staging table first, then moved to production table\n",
    "  - Provides atomic operation - all data or none\n",
    "  - If export fails, production table remains unchanged\n",
    "  - `--clear-staging-table`: Cleans staging table before export\n",
    "\n",
    "• Export safety and reliability\n",
    "  - Staging tables prevent partial data in case of failures\n",
    "  - Production table remains available during export process\n",
    "  - Allows validation of staging data before final move\n",
    "  - Adds overhead but provides data consistency guarantees\n",
    "  - Essential for critical production systems\n",
    "\n",
    "• Performance considerations\n",
    "  - `--batch`: Groups multiple records per statement\n",
    "  - `--records-per-statement 1000`: Balance memory usage and performance\n",
    "  - Larger batches reduce database round trips\n",
    "  - Monitor database transaction log growth during exports\n",
    "\n",
    "**12. Sqoop Performance Tuning**\n",
    "\n",
    "• Mapper and connection optimization\n",
    "  - Optimal mapper count: Usually 2-4 per CPU core in cluster\n",
    "  - `--num-mappers 8`: Match cluster capacity and database capability\n",
    "  - `--fetch-size 1000`: Larger fetch sizes reduce database round trips\n",
    "  - `--split-by id`: Choose well-distributed numeric columns for splitting\n",
    "\n",
    "• Database-level optimizations\n",
    "  - Use indexes on split columns and WHERE clause conditions\n",
    "  - `--connect \"jdbc:mysql://localhost/testdb?useSSL=false&rewriteBatchedStatements=true\"`: Connection parameter tuning\n",
    "  - Connection pooling parameters for efficiency\n",
    "  - Database server configuration for concurrent connections\n",
    "\n",
    "• Native vs JDBC drivers\n",
    "  - `--direct`: Use native database utilities (MySQL, PostgreSQL)\n",
    "    * Bypasses JDBC for bulk operations\n",
    "    * Often 2-3x faster than JDBC approach\n",
    "    * Uses database-specific bulk loading tools\n",
    "    * Limited to specific databases with native support\n",
    "  - JDBC drivers: Universal compatibility but potentially slower\n",
    "  - Choose based on database type and performance requirements\n",
    "\n",
    "• Compression and storage optimization\n",
    "  - `--compression-codec gzip`: Reduce I/O and storage requirements\n",
    "  - Available codecs: gzip, bzip2, snappy, lzo\n",
    "  - Balance compression ratio vs CPU overhead\n",
    "  - Essential for large datasets and limited network bandwidth\n",
    "\n",
    "• Resource management tuning\n",
    "  - Increase mapper memory for large records or complex processing\n",
    "  - Consider network bandwidth between database and Hadoop cluster\n",
    "  - Monitor cluster resource usage during imports\n",
    "  - Balance database server load with import parallelism\n",
    "\n",
    "• Statement and transaction optimization\n",
    "  - `--batch`: Enable batch mode for exports (groups multiple records)\n",
    "  - `--records-per-statement 1000`: Optimize batch size for performance\n",
    "  - Larger transactions reduce overhead but increase lock time\n",
    "  - Monitor database transaction logs and lock contention\n",
    "\n",
    "• Performance monitoring and best practices\n",
    "  - Use evenly distributed numeric columns for data splitting\n",
    "  - Match mapper allocation to cluster capacity\n",
    "  - Optimize database for concurrent read operations\n",
    "  - Always use compression for large datasets\n",
    "  - Monitor both database and cluster resources during operations\n",
    "  - Test different mapper counts and fetch sizes for optimal performance\n",
    "\n",
    "  ---\n",
    "  ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c6a56",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
