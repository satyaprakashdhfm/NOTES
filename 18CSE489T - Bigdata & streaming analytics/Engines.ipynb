{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0364a3",
   "metadata": {},
   "source": [
    "# MapReduce Engine (old Hadoop style)\n",
    "\n",
    "- Batch-oriented, disk-heavy (each step writes to HDFS).\n",
    "\n",
    "- High latency, slower (good for huge but not real-time jobs).\n",
    "\n",
    "- Example: Counting words in a 1TB log → multiple map/shuffle/reduce steps, writing to disk between each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376cf11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73643166",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62c92de3",
   "metadata": {},
   "source": [
    "# Spark Engine :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa79d3e",
   "metadata": {},
   "source": [
    "<img src=\"/workspaces/ML--DL--NOTES/assets/image.png\" alt=\"Alt Text\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b188c",
   "metadata": {},
   "source": [
    "1. **Local (single JVM)** — `local`, `local[n]`, `local[*]` — run everything on your laptop.\n",
    "2. **Client vs Cluster deploy mode** — `--deploy-mode client|cluster` (where the driver runs).\n",
    "3. **Standalone cluster** — Spark’s own master + workers (lightweight cluster).\n",
    "4. **YARN** — run on Hadoop YARN (common on on-prem Hadoop clusters).\n",
    "5. **Kubernetes** — executors & driver run as k8s pods (modern production).\n",
    "6. **Mesos** — cluster manager (rare today).\n",
    "7. **Interactive shells / notebooks** — `spark-shell`, `pyspark`, Jupyter/Zeppelin (REPL-style experimenting).\n",
    "8. **Embedded (IDE / sbt / Metals)** — Spark runs inside your app JVM (good for debugging/tutorials).\n",
    "9. **Cloud-managed** — Databricks, AWS EMR, Google Dataproc (managed clusters & production).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255c3a3",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e88662",
   "metadata": {},
   "source": [
    "Got it — you want a **single compact Spark notes sheet** covering **RDDs, DataFrames, SQL, Streaming, MLlib, and GraphX** so it’s the whole Spark picture in one place.\n",
    "\n",
    "Here’s the **small, precise, all-in-one** version:\n",
    "\n",
    "---\n",
    "\n",
    "## **Apache Spark Quick Notes (All APIs)**\n",
    "\n",
    "### **1. RDD (Low-level API)**\n",
    "\n",
    "* Immutable, distributed collections.\n",
    "* **Create:** `sc.parallelize(list)`, `sc.textFile(\"path\")`\n",
    "* **Transformations:** `map`, `filter`, `flatMap`, `distinct`, `union`\n",
    "* **Actions:** `collect`, `count`, `first`, `take(n)`, `saveAsTextFile`\n",
    "* **Key-Value Ops:** `reduceByKey`, `mapValues`, `join`\n",
    "* **Cache/Persist:** `cache()`, `persist()`\n",
    "\n",
    "---\n",
    "\n",
    "### **2. DataFrame (High-level API)**\n",
    "\n",
    "* Tabular data with named columns.\n",
    "* **Create:** `spark.read.csv(\"file\")`, `spark.createDataFrame(data)`\n",
    "* **Select & Filter:** `.select(\"col\")`, `.filter(\"col > 10\")`\n",
    "* **Group & Aggregate:** `.groupBy(\"col\").agg({\"col2\": \"sum\"})`\n",
    "* **Write:** `.write.csv(\"path\")`\n",
    "* **SQL Support:** `df.createOrReplaceTempView(\"table\")`\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Spark SQL**\n",
    "\n",
    "* Run SQL queries directly:\n",
    "\n",
    "```python\n",
    "spark.sql(\"SELECT col1, COUNT(*) FROM table GROUP BY col1\")\n",
    "```\n",
    "\n",
    "* Integrates with DataFrames.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Structured Streaming**\n",
    "\n",
    "* Real-time processing using DataFrames.\n",
    "* **Source:** `.readStream.format(\"kafka\")`, `.readStream.text(\"dir\")`\n",
    "* **Sink:** `.writeStream.format(\"console\")`, `.writeStream.parquet(\"dir\")`\n",
    "* Always runs in micro-batches or continuous mode.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. MLlib (Machine Learning)**\n",
    "\n",
    "* Algorithms: classification, regression, clustering, recommendation.\n",
    "* **Pipeline:**\n",
    "\n",
    "  * Data → Feature transformers (`VectorAssembler`)\n",
    "  * Estimator (`LogisticRegression`)\n",
    "  * Evaluator (`MulticlassClassificationEvaluator`)\n",
    "* Example:\n",
    "\n",
    "```python\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. GraphX (Graphs in Spark — Scala/Java)**\n",
    "\n",
    "* Graph processing library.\n",
    "* Vertices + Edges RDDs.\n",
    "* Algorithms: PageRank, Connected Components, Triangle Count.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key Points to Remember:**\n",
    "\n",
    "* **RDD** = low-level, flexible, more code.\n",
    "* **DataFrame** = high-level, optimized, preferred for most work.\n",
    "* **SQL API** = SQL syntax on DataFrames.\n",
    "* **Structured Streaming** = real-time data with DataFrame API.\n",
    "* **MLlib** = machine learning library.\n",
    "* **GraphX** = graph analytics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27065e4",
   "metadata": {},
   "source": [
    "\n",
    "| Feature            | RDD (Low-Level API)                                                                          | DataFrame (High-Level API)                     | Spark SQL (Declarative API)                  |\n",
    "| ------------------ | -------------------------------------------------------------------------------------------- | ---------------------------------------------- | -------------------------------------------- |\n",
    "| **Data Type**      | Distributed collection of objects                                                            | Distributed collection of rows with schema     | Structured data via SQL queries              |\n",
    "| **Schema**         | No schema                                                                                    | Has schema                                     | Has schema                                   |\n",
    "| **Ease of Use**    | Complex Scala/Python code                                                                    | API methods (select, filter, groupBy)          | SQL syntax                                   |\n",
    "| **Performance**    | Slowest (no query optimization)                                                              | Optimized by Catalyst                          | Optimized + integrated with DataFrame engine |\n",
    "| **Industry Usage** | Complex data preprocessing in ML pipelines (e.g., parsing raw logs, custom graph algorithms) | ETL jobs, feature engineering, batch analytics | BI dashboards, reporting, ad-hoc analytics   |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece534b",
   "metadata": {},
   "source": [
    "## - Spark RDD:\n",
    "- rdd reading and writing a file\n",
    "- deploying code to cluster:\n",
    "    - Create RDD locally with `.master(\"local[*]\")` and `spark.sparkContext.parallelize(data)`.\n",
    "    - Deploy to cluster by changing `.master()` to cluster URL and running via `spark-submit --master <cluster-url> script.py`.\n",
    "\n",
    "- common rdd tranformations and actions\n",
    "- pair rdd\n",
    "- using schema rdd\n",
    "- using row rdd\n",
    "- rdd tuning:\n",
    "    | Tuning Aspect           | What it Controls                    | How to Set in RDD                                                                      | Industry-Level Practice                                          | Limits / Risks                                     |\n",
    "    | ----------------------- | ----------------------------------- | -------------------------------------------------------------------------------------- | ---------------------------------------------------------------- | -------------------------------------------------- |\n",
    "    | **Partitions**          | Number of RDD splits across cluster | `minPartitions`, `.repartition()`, `.coalesce()`                                       | `2–4 × total CPU cores` for balanced load                        | Too many → scheduler overhead; too few → idle CPUs |\n",
    "    | **Persistence Level**   | How RDD is cached in memory/disk    | `.persist(StorageLevel.MEMORY_ONLY)`                                                   | Use memory-only for iterative ML; use memory+disk for large RDDs | Memory pressure → GC pauses / spilling             |\n",
    "    | **Serialization**       | Data encoding format                | `spark.serializer=KryoSerializer`                                                      | Kryo for faster, smaller shuffle                                 | Incompatible if not registered                     |\n",
    "    | **Shuffle Parallelism** | Parallelism in wide transformations | `spark.sql.shuffle.partitions` (for DataFrame) or `reduceByKey(numPartitions)` for RDD | Match to partition count to avoid skew                           | Too low → stragglers; too high → shuffle overhead  |\n",
    "    | **Cluster Resources**   | CPU & memory allocation             | `--executor-memory`, `--executor-cores`                                                | Right-size to dataset & job pattern                              | Over-allocation → wasted nodes; under → slow jobs  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991cad83",
   "metadata": {},
   "source": [
    "## - spark DataFrames:\n",
    "- creating dataframes seamlessly and form rdd\n",
    "- reading and writing avro data , xml data...default parquet format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae3b21",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
