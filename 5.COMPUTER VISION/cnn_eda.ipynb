{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Class Imbalance in Image Classification\n",
    "\n",
    "Handling class imbalance in image classification involves techniques to ensure that the model doesn't become biased toward the majority class. Here are common approaches:\n",
    "\n",
    "**1. Data-Level Techniques**\n",
    "\n",
    "   - **Oversampling the Minority Class**: Duplicate or augment images from the minority classes to increase their representation. Data augmentation techniques (e.g., rotation, cropping, flipping) can help create diverse samples for minority classes without introducing exact duplicates.\n",
    "   - **Undersampling the Majority Class**: Randomly reduce the number of samples in majority classes to match the minority class size. This is more feasible with larger datasets, though it risks losing important information.\n",
    "\n",
    "**2. Algorithm-Level Techniques**\n",
    "\n",
    "   - **Class Weights Adjustment**: Many deep learning frameworks allow specifying a weight for each class in the loss function. This penalizes misclassifications of the minority class more than the majority class, encouraging the model to pay more attention to the minority class.\n",
    "   - **Focal Loss**: Focal loss is designed for class imbalance by dynamically scaling the loss for hard-to-classify examples, typically from minority classes. It modifies the cross-entropy loss by adding a scaling factor that reduces the loss for well-classified examples and focuses on hard examples.\n",
    "\n",
    "   $$ \n",
    "   \\text{Focal Loss} = -\\alpha (1 - p_t)^\\gamma \\log(p_t)\n",
    "   $$\n",
    "\n",
    "   where \\( p_t \\) is the predicted probability for the true class, \\( \\alpha \\) is a balancing factor for class imbalance, and \\( \\gamma \\) controls the focus on hard examples.\n",
    "\n",
    "**3. Hybrid and Advanced Techniques**\n",
    "\n",
    "   - **Two-Stage Training**: Train the model first on the original data, then fine-tune with balanced classes or using only the minority class. This approach helps retain information while enhancing sensitivity to minority classes.\n",
    "   - **Synthetic Data Generation**: Use techniques like **Generative Adversarial Networks (GANs)** to generate synthetic images for the minority class. GANs can create realistic, diverse images that augment the dataset.\n",
    "   - **Self-Supervised Learning**: In self-supervised learning, the model learns from unlabeled data, which can later be fine-tuned on a smaller, balanced labeled dataset, improving minority class recognition.\n",
    "\n",
    "**4. Evaluation Adjustments**\n",
    "\n",
    "   - **Metrics Beyond Accuracy**: Use metrics like precision, recall, F1-score, or area under the ROC curve (AUC) to get a more balanced view of performance on imbalanced data, as accuracy can be misleading with class imbalance.\n",
    "   - **Confusion Matrix Analysis**: Reviewing the confusion matrix helps identify if the model is biased toward majority classes, guiding further balancing efforts.\n",
    "\n",
    "Each technique can be combined depending on the severity of imbalance, dataset size, and model complexity, but balancing data effectively often requires experimenting with several methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# While both data augmentation and oversampling aim to improve model performance, they address different challenges in machine learning. Data augmentation enhances dataset diversity, whereas oversampling focuses on correcting class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Techniques in Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Data augmentation is a crucial technique used to artificially expand the size of a training dataset by applying various transformations to the original data. This helps improve the generalization of CNNs and reduces overfitting. Here are some common data augmentation techniques:\n",
    "\n",
    "**1. Geometric Transformations**\n",
    "- **Rotation**: Rotate images by a certain angle.\n",
    "  - Example: Rotate by 15, 30, or 45 degrees.\n",
    "  \n",
    "- **Translation**: Shift images along the x or y axis.\n",
    "  - Example: Shift images by a few pixels left, right, up, or down.\n",
    "\n",
    "- **Scaling**: Zoom in or out on images.\n",
    "  - Example: Scale images to 90% or 110% of their original size.\n",
    "  If the image pixel values are originally in the range of RGB values, typically between 0 and 255, and you scale them to be between -1 and 1, this would effectively change the color intensity scale.\n",
    "  Here's the process:\n",
    "    1. Original RGB Values (0-255):\n",
    "      ○ Each pixel in an RGB image has values for Red, Green, and Blue that range from 0 to 255. These values represent the intensity of each color channel.\n",
    "    2. Scaling to [-1, 1]:\n",
    "\n",
    "      ○ To scale the values from the range [0, 255] to [-1, 1], you can use the following formula for each color channel (R, G, and B):\n",
    "    $$\\text{scaled\\_value} = \\frac{\\text{original\\_value}}{127.5} - 1$$\n",
    "\n",
    "\n",
    "  This transforms:\n",
    "      ○ 0 → -1 (black)\n",
    "\n",
    "      ○ 255 → 1 (white)\n",
    "\n",
    "      ○ 127.5 → 0 (mid-gray)\n",
    "\n",
    "  Essentially, the value of 0 becomes -1, and 255 becomes 1, with all other values mapped accordingly. This scaling ensures the entire image falls within the range [-1, 1].\n",
    "\n",
    "  What Happens:\n",
    "    • Intensities and contrast: The scaling operation changes the contrast and overall intensity of the image. For example, pixel values close to 255 (or the brightest) will become close to 1, and pixels near 0 will become close to -1.\n",
    "\n",
    "    • Effect on Machine Learning/Deep Learning: When working with neural networks, normalizing image data to a range of [-1, 1] is a common preprocessing step, as it allows the model to handle input more effectively. This normalization helps with gradient descent optimization by ensuring that the features of the image have a consistent range and prevents issues like vanishing/exploding gradients.\n",
    "\n",
    "  \n",
    "\n",
    "- **Flipping**: Flip images horizontally or vertically.\n",
    "  - Example: Horizontal flips are common for many tasks.\n",
    "\n",
    "**2. Color Space Transformations**\n",
    "- **Brightness Adjustment**: Change the brightness of images.\n",
    "  - Example: Increase or decrease brightness by a fixed factor.\n",
    "\n",
    "- **Contrast Adjustment**: Modify the contrast of images.\n",
    "  - Example: Enhance or reduce the contrast of images.\n",
    "\n",
    "- **Saturation Adjustment**: Alter the saturation levels of images.\n",
    "  - Example: Make images more or less colorful.\n",
    "\n",
    "- **Hue Adjustment**: Shift the hue of colors in images.\n",
    "  - Example: Change colors to see how the model reacts to different color variations.\n",
    "\n",
    "**3. Noise Injection**\n",
    "- **Gaussian Noise**: Add random noise to images to make them more robust.\n",
    "  - Example: Add small Gaussian noise to pixel values.\n",
    "\n",
    "- **Salt-and-Pepper Noise**: Introduce random white and black pixels.\n",
    "  - Example: Randomly set a percentage of pixels to maximum or minimum values.\n",
    "\n",
    "**4. Random Erasing**\n",
    "- **Random Erasing**: Randomly remove sections of an image to make the model learn to focus on different features.\n",
    "  - Example: Select a random rectangle in the image and set it to a constant value or noise.\n",
    "\n",
    "**5. Elastic Transformations**\n",
    "- **Elastic Deformations**: Apply random elastic deformations to images.\n",
    "  - Example: Distort images to create variations while preserving overall structure.\n",
    "\n",
    "**6. Cutout**\n",
    "- **Cutout**: Randomly mask out square regions in images.\n",
    "  - Example: Set square patches in an image to zero or the mean pixel value.\n",
    "\n",
    "**7. Mixup**\n",
    "- **Mixup**: Create new training examples by mixing two images and their corresponding labels.\n",
    "  - Example: For images A and B with labels \\(y_A\\) and \\(y_B\\), create a new image \n",
    "  $$\n",
    "  \\text{Image}_{new} = \\lambda \\cdot \\text{Image}_A + (1 - \\lambda) \\cdot \\text{Image}_B\n",
    "  $$ \n",
    "  where \\( \\lambda \\) is a random value between 0 and 1.\n",
    "\n",
    "**8. Random Cropping**\n",
    "- **Random Cropping**: Randomly crop images to create variations in scale and aspect ratio.\n",
    "  - Example: Crop a random section of the original image.\n",
    "\n",
    "**Conclusion**\n",
    "Data augmentation helps increase the diversity of the training dataset, making CNNs more robust and improving their performance on unseen data. Many deep learning frameworks (like TensorFlow and PyTorch) provide built-in support for these augmentation techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Shot Learning\n",
    "\n",
    "**One Shot Learning** is a machine learning approach that enables a recognition system to identify or classify objects based on a single example or image. This is particularly challenging in face recognition, where traditionally, deep learning models require large datasets to achieve good performance.\n",
    "\n",
    "**Definition**\n",
    "- **One Shot Learning**: A recognition system can recognize a person by learning from just one image.\n",
    "\n",
    "**Challenges**\n",
    "Historically, deep learning has not performed well when the amount of training data is small. One Shot Learning addresses this challenge by learning a **similarity function** rather than traditional classification.\n",
    "\n",
    "**Similarity Function**\n",
    "To evaluate the similarity between two images, we define a function \\( d \\):\n",
    "$$\n",
    "d(\\text{img1}, \\text{img2}) = \\text{degree of difference between img1 and img2}\n",
    "$$\n",
    "Where:\n",
    "- **img1** and **img2** are the images being compared.\n",
    "- **d** outputs a value representing how similar or different the images are.\n",
    "\n",
    "**Key Points:**\n",
    "- A lower value of \\( d \\) indicates that the images are likely of the same person (i.e., faces are similar).\n",
    "- We introduce a threshold \\( T \\) to make a decision:\n",
    "$$\n",
    "\\text{If } d(\\text{img1}, \\text{img2}) \\leq T \\text{, then the faces are considered the same.}\n",
    "$$\n",
    "\n",
    "**Advantages of One Shot Learning**\n",
    "- **Efficiency**: It allows for effective recognition with minimal training data, which is crucial in scenarios where data collection is limited.\n",
    "- **Robustness**: The similarity function can generalize well to new inputs, making it adaptable to various situations.\n",
    "\n",
    "**Conclusion**\n",
    "One Shot Learning provides a solution to the challenge of recognizing individuals from very limited data. By focusing on learning a similarity function, it allows for effective face recognition even with just a single example image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Loss\n",
    "\n",
    "Triplet loss is a loss function commonly used in deep learning, particularly in tasks involving similarity learning, such as face recognition and image retrieval. It aims to ensure that the distance between an anchor sample and a positive sample (similar) is smaller than the distance between the anchor sample and a negative sample (dissimilar) by a predefined margin. \n",
    "\n",
    "**Definition**\n",
    "- Given three inputs: an anchor $x_a$, a positive sample $x_p$ (similar to the anchor), and a negative sample $x_n$ (dissimilar to the anchor), the triplet loss can be defined as:\n",
    "\n",
    "$$\n",
    "L(x_a, x_p, x_n) = \\max(0, d(x_a, x_p) - d(x_a, x_n) + \\alpha)\n",
    "$$\n",
    "\n",
    "where:\n",
    "-  $d(x_i, x_j)$ is a distance metric (e.g., Euclidean distance) between samples $x_i$ and $x_j$,\n",
    "-  $\\alpha$ is the margin that is enforced between positive and negative pairs.\n",
    "\n",
    "**Importance for CNNs**\n",
    "- **Learning Discriminative Features**: Triplet loss helps CNNs learn embeddings that are well-separated for different classes while bringing similar classes closer together in the feature space. This is particularly useful in applications where distinguishing between classes is challenging.\n",
    "- **Robustness to Variations**: It provides a robust mechanism for the model to learn invariant features despite variations in pose, lighting, or other conditions, making it suitable for real-world applications.\n",
    "\n",
    "**Applications of Triplet Loss**\n",
    "1. **Face Recognition**: In face recognition systems, triplet loss can be used to ensure that images of the same person are close in the embedding space, while images of different people are far apart.\n",
    "2. **Image Retrieval**: For systems that retrieve images based on similarity, triplet loss helps improve the ranking of images based on user queries.\n",
    "3. **Object Tracking**: In object tracking, triplet loss can help to distinguish the target object from background clutter or other objects.\n",
    "4. **Speaker Verification**: In audio processing, triplet loss can be applied to ensure that recordings of the same speaker are closer together than recordings from different speakers.\n",
    "\n",
    "By applying triplet loss in CNNs, models can achieve higher accuracy and robustness in distinguishing between classes based on learned embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### EDA Questions for CV (Image Data)\n",
    "\n",
    "1. What are the common dimensions of the images (width, height)?\n",
    "2. How does the aspect ratio vary across the dataset?\n",
    "3. What is the color distribution across images?\n",
    "4. Are there differences in brightness or contrast among the images?\n",
    "5. What is the edge distribution in the images (sharp vs. smooth regions)?\n",
    "6. What common textures or patterns are present in different image categories?\n",
    "7. Is there a class imbalance in the number of images per category?\n",
    "8. What are the most common objects detected in the images?\n",
    "9. Are there patterns in metadata, such as capture date, location, or resolution?\n",
    "10. Do images have similarities in background, lighting, or occlusion within classes? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical Example:\n",
    "red object before...after some brightness is more\n",
    "Let’s compare how RGB and HSV react to changes in lighting:\n",
    "\n",
    "1. RGB Before and After Lighting Change:\n",
    "Before: A bright red object has RGB values (255, 0, 0).\n",
    "After: Under dim lighting, the same red object might have RGB values like (150, 0, 0), where the object is still perceived as red but the RGB values have changed drastically.\n",
    "2. HSV Before and After Lighting Change:\n",
    "Before: The object is a bright red, so its HSV values might be (0°, 255, 255).\n",
    "After: Under dim lighting, the object’s HSV values might change to (0°, 100, 100), but the Hue (H) remains the same (0°), indicating that it is still red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how **dlib** detects faces and facial features like lips and nose:\n",
    "\n",
    "### 1. **Face Detection in dlib**:\n",
    "   - **HOG-based or CNN-based detectors** are used for detecting faces.\n",
    "   - The detector works by scanning the image for faces using a sliding window approach.\n",
    "   - A **classifier** (typically an SVM) is applied to classify each region as a face or not.\n",
    "   - It outputs **bounding boxes** around detected faces.\n",
    "\n",
    "### 2. **Facial Landmark Detection**:\n",
    "   - After detecting the face, **dlib's shape predictor** (e.g., `shape_predictor_68_face_landmarks.dat`) detects specific **facial landmarks**.\n",
    "   - The model identifies **68 key points** on the face, including eyes, eyebrows, nose, lips, and jawline.\n",
    "\n",
    "### 3. **Marking Facial Features**:\n",
    "   - **Lips**: Points 48-67.\n",
    "   - **Nose**: Points 27-35.\n",
    "   - Each landmark point corresponds to a specific location on the face (e.g., corners of the lips, tip of the nose).\n",
    "   - These points are marked by drawing small circles or used for further facial analysis.\n",
    "\n",
    "### 4. **Applications**:\n",
    "   - **Emotion recognition** based on lip and facial expression analysis.\n",
    "   - **Face alignment** for improving face recognition accuracy.\n",
    "   - **Augmented Reality (AR)** for overlaying virtual makeup or other features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an updated list with multiple useful functions for each topic:\n",
    "\n",
    "### 1. Getting Started with OpenCV  \n",
    "**Definition:** OpenCV is an open-source computer vision library that provides tools for image and video processing.  \n",
    "```python\n",
    "import cv2  # Import OpenCV\n",
    "image = cv2.imread(\"image.jpg\")  # Read an image\n",
    "cv2.imshow(\"Image\", image)  # Display an image\n",
    "cv2.imwrite(\"output.jpg\", image)  # Save an image\n",
    "cv2.waitKey(0)  # Wait for a key press\n",
    "```  \n",
    "\n",
    "### 2. Grey-scaling Images  \n",
    "**Definition:** Converting a color image to grayscale reduces it to a single intensity channel.  \n",
    "```python\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "cv2.imshow(\"Gray Image\", gray)  # Show grayscale image\n",
    "gray_inverted = cv2.bitwise_not(gray)  # Invert grayscale image\n",
    "blurred_gray = cv2.GaussianBlur(gray, (5,5), 0)  # Apply Gaussian blur\n",
    "cv2.waitKey(0)\n",
    "```  \n",
    "\n",
    "### 3. Color Spaces (HSV & RGB)  \n",
    "**Definition:** Changing color representations between RGB, HSV, and other formats.  \n",
    "```python\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # Convert to HSV\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "h, s, v = cv2.split(hsv)  # Split into Hue, Saturation, and Value channels\n",
    "image_hue = cv2.merge([h, s, v])  # Reconstruct HSV image\n",
    "cv2.imshow(\"HSV Image\", hsv)  # Show HSV image\n",
    "```  \n",
    "\n",
    "### 4. Drawing on Images  \n",
    "**Definition:** OpenCV allows drawing shapes like lines, circles, and rectangles on images.  \n",
    "```python\n",
    "cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Draw a line\n",
    "cv2.rectangle(image, (50, 50), (200, 200), (0, 255, 0), 3)  # Draw a rectangle\n",
    "cv2.circle(image, (center_x, center_y), radius, (0, 0, 255), 3)  # Draw a circle\n",
    "cv2.putText(image, \"Hello\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)  # Write text\n",
    "cv2.imshow(\"Drawings\", image)  # Show image with drawings\n",
    "```  \n",
    "\n",
    "### 5. Transformations - Translations and Rotations  \n",
    "**Definition:** Moving or rotating an image using affine transformations.  \n",
    "```python\n",
    "M = cv2.getRotationMatrix2D((center_x, center_y), angle, scale)  # Get rotation matrix\n",
    "rotated = cv2.warpAffine(image, M, (width, height))  # Apply rotation\n",
    "M_translation = np.float32([[1, 0, 100], [0, 1, 50]])  # Define translation matrix\n",
    "translated = cv2.warpAffine(image, M_translation, (width, height))  # Apply translation\n",
    "cv2.imshow(\"Rotated\", rotated)  # Show rotated image\n",
    "```  \n",
    "\n",
    "### 6. Scaling, Resizing, and Cropping  \n",
    "**Definition:** Changing image size and extracting a region of interest.  \n",
    "```python\n",
    "resized = cv2.resize(image, (width, height))  # Resize image\n",
    "cropped = image[50:200, 50:200]  # Crop region of interest\n",
    "scaled = cv2.resize(image, None, fx=0.5, fy=0.5)  # Scale image by 50%\n",
    "aspect_ratio_resized = cv2.resize(image, (width, int(height * 0.5)))  # Maintain aspect ratio\n",
    "cv2.imshow(\"Cropped Image\", cropped)  # Show cropped image\n",
    "```  \n",
    "\n",
    "### 7. Arithmetic and Bitwise Operations  \n",
    "**Definition:** Performing pixel-wise operations like addition, subtraction, AND, OR, XOR.  \n",
    "```python\n",
    "result_add = cv2.add(image1, image2)  # Add two images\n",
    "result_sub = cv2.subtract(image1, image2)  # Subtract image2 from image1\n",
    "result_and = cv2.bitwise_and(image1, image2)  # Bitwise AND\n",
    "result_or = cv2.bitwise_or(image1, image2)  # Bitwise OR\n",
    "result_xor = cv2.bitwise_xor(image1, image2)  # Bitwise XOR\n",
    "```  \n",
    "\n",
    "### 8. Convolutions, Blurring, and Sharpening  \n",
    "**Definition:** Applying filters to smooth or sharpen images using kernels.  \n",
    "```python\n",
    "blurred = cv2.GaussianBlur(image, (5,5), 0)  # Apply Gaussian blur\n",
    "sharpened = cv2.filter2D(image, -1, kernel_sharpen)  # Apply sharpening filter\n",
    "median_blurred = cv2.medianBlur(image, 5)  # Apply median blur\n",
    "bilateral_blurred = cv2.bilateralFilter(image, 9, 75, 75)  # Apply bilateral filter\n",
    "cv2.imshow(\"Blurred Image\", blurred)  # Show blurred image\n",
    "```  \n",
    "\n",
    "### 9. Thresholding & Binarization  \n",
    "**Definition:** Converting an image to binary using a threshold.  \n",
    "```python\n",
    "_, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)  # Apply binary threshold\n",
    "_, binary_inv = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)  # Inverted binary threshold\n",
    "adaptive_thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)  # Adaptive threshold\n",
    "otsu_thresh, otsu_binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)  # Otsu's thresholding\n",
    "cv2.imshow(\"Thresholded\", binary)  # Show thresholded image\n",
    "```  \n",
    "\n",
    "### 10. Dilation, Erosion, and Edge Detection  \n",
    "**Definition:** Morphological operations to enhance or remove image features.  \n",
    "```python\n",
    "dilated = cv2.dilate(binary, None, iterations=2)  # Apply dilation\n",
    "eroded = cv2.erode(binary, None, iterations=2)  # Apply erosion\n",
    "edges = cv2.Canny(image, 100, 200)  # Apply Canny edge detection\n",
    "grad_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)  # Apply Sobel edge detection in X direction\n",
    "grad_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)  # Apply Sobel edge detection in Y direction\n",
    "```  \n",
    "\n",
    "### 11. Contours - Drawing, Hierarchy, and Modes  \n",
    "**Definition:** Finding and drawing contours (outlines) of objects in an image.  \n",
    "```python\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)  # Find contours\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 3)  # Draw contours\n",
    "cv2.drawContours(image, contours, 2, (0, 0, 255), 2)  # Draw specific contour\n",
    "cv2.imshow(\"Contours\", image)  # Show image with contours\n",
    "```  \n",
    "\n",
    "### 12. Moments, Matching, and Sorting Contours  \n",
    "**Definition:** Extracting shape features, matching contours, and sorting them.  \n",
    "```python\n",
    "M = cv2.moments(contour)  # Get moments of a contour\n",
    "cx = int(M['m10'] / M['m00'])  # Find centroid X\n",
    "cy = int(M['m01'] / M['m00'])  # Find centroid Y\n",
    "sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)  # Sort contours by area\n",
    "cv2.drawContours(image, sorted_contours, 0, (255, 0, 0), 2)  # Draw sorted contours\n",
    "cv2.imshow(\"Sorted Contours\", image)  # Show sorted contours\n",
    "```  \n",
    "\n",
    "### 13. Line, Circle, Blob Detection  \n",
    "**Definition:** Detecting geometric shapes in an image.  \n",
    "```python\n",
    "circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 20)  # Detect circles\n",
    "cv2.circle(image, (x, y), radius, (0, 255, 0), 4)  # Draw detected circle\n",
    "lines = cv2.HoughLinesP(image, 1, np.pi / 180, 100, minLineLength=50, maxLineGap=10)  # Detect lines\n",
    "cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Draw detected line\n",
    "```  \n",
    "\n",
    "### 14. Counting Circles, Ellipses, and Finding Waldo  \n",
    "**Definition:** Identifying circular and elliptical objects using contour properties.  \n",
    "```python\n",
    "ellipse = cv2.fitEllipse(contour)  # Fit ellipse to contour\n",
    "cv2.ellipse(image, ellipse, (0, 255, 0), 2)  # Draw ellipse\n",
    "circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 20)  # Detect circles\n",
    "cv2.circle(image, (x, y), radius, (255, 0, 0), 2)  # Draw circle\n",
    "```  \n",
    "\n",
    "### 15. Finding Corners  \n",
    "**Definition:** Detecting corners using Harris or Shi-Tomasi corner detection.  \n",
    "```python\n",
    "corners = cv2.goodFeaturesToTrack(gray, 100, 0.01, 10)  # Shi-Tomasi corner detection\n",
    "for corner in corners:\n",
    "    x, y = corner.ravel()  # Get corner coordinates\n",
    "    cv2.circle(image, (x, y), 3, 255, -1)  # Draw corner\n",
    "cv2.imshow(\"Corners\", image)  # Show image with corners\n",
    "```  \n",
    "\n",
    "### 16. Face and Eye Detection with HAAR Cascade Classifiers  \n",
    "**Definition:** Detecting faces and eyes using pre-trained HAAR cascades.  \n",
    "```python\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)  # Detect faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x\n",
    "\n",
    " + w, y + h), (255, 0, 0), 2)  # Draw face bounding box\n",
    "eyes = eye_cascade.detectMultiScale(gray)  # Detect eyes\n",
    "for (ex, ey, ew, eh) in eyes:\n",
    "    cv2.rectangle(image, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)  # Draw eye bounding box\n",
    "cv2.imshow(\"Face and Eye Detection\", image)  # Show image with detections\n",
    "```  \n",
    "\n",
    "### 17. Vehicle & Pedestrian Detection  \n",
    "**Definition:** Identifying cars and people in images using pre-trained classifiers.  \n",
    "```python\n",
    "pedestrians = hog.detectMultiScale(image, winStride=(4, 4))  # Detect pedestrians\n",
    "vehicles = car_cascade.detectMultiScale(gray, 1.1, 3)  # Detect vehicles\n",
    "```  \n",
    "\n",
    "### 18. Perspective Transforms  \n",
    "**Definition:** Adjusting the perspective of an image using four points.  \n",
    "```python\n",
    "warped = cv2.warpPerspective(image, M, (width, height))  # Apply perspective transform\n",
    "```  \n",
    "\n",
    "### 19. Histograms and K-means Clustering for Finding Dominant Colors  \n",
    "**Definition:** Analyzing pixel distributions and clustering colors.  \n",
    "```python\n",
    "hist = cv2.calcHist([image], [0], None, [256], [0,256])  # Calculate histogram\n",
    "```  \n",
    "\n",
    "### 20. Comparing Images with MSE and Structural Similarity  \n",
    "**Definition:** Measuring the similarity between two images.  \n",
    "```python\n",
    "score, diff = ssim(image1, image2, full=True)  # Compute SSIM\n",
    "```  \n",
    "- **MSE** measures pixel-level differences, penalizing small changes even if they're not perceptually significant.\n",
    "- **MSE** gives a more direct numerical error but lacks human-like interpretation.\n",
    "- **SSIM** evaluates structural, luminance, and contrast changes, aligning more with human perception.\n",
    "- **SSIM** reflects perceptual similarity, making it more reliable for visual quality assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the requested topics, expanded with useful functions for each one:\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.transform import resize\n",
    "from pyzbar.pyzbar import decode\n",
    "import pytesseract\n",
    "import easyocr\n",
    "import time\n",
    "\n",
    "# 21. Filtering Colors  \n",
    "lower_blue = np.array([100, 50, 50])  # Define lower bound for blue color\n",
    "upper_blue = np.array([140, 255, 255])  # Define upper bound for blue color\n",
    "mask = cv2.inRange(hsv, lower_blue, upper_blue)  # Filter blue color\n",
    "filtered = cv2.bitwise_and(image, image, mask=mask)  # Apply filter\n",
    "result = cv2.bitwise_not(mask)  # Invert the mask\n",
    "cv2.imshow('Filtered', filtered)  # Display filtered result\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 22. Watershed Algorithm marker-based image segmentation  \n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  \n",
    "_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)  # Binary inverse threshold\n",
    "dist_transform = cv2.distanceTransform(thresh, cv2.DIST_L2, 5)  # Compute distance transform\n",
    "_, markers = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)  # Marker image\n",
    "markers = np.int32(markers)  # Convert to int32\n",
    "cv2.watershed(image, markers)  # Apply watershed\n",
    "image[markers == -1] = [0, 0, 255]  # Mark boundaries\n",
    "cv2.imshow('Watershed Segmentation', image)  # Display segmented image\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 23. Background and Foreground Subtraction  \n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()  # Background subtractor\n",
    "fgmask = fgbg.apply(image)  # Get foreground mask\n",
    "cv2.imshow('Foreground Mask', fgmask)  # Show the mask\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 24. Motion tracking using Mean Shift and CAM-Shift-The Mean Shift algorithm iterates and adjusts the position of the search window until the window is centered on the region with the highest likelihood of matching the object's features.\n",
    "roi = (200, 200, 100, 100)  # Define region of interest (ROI)\n",
    "hsv_roi = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # Convert ROI to HSV\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0, 180])  # Compute histogram\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)  # Normalize histogram\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)  # Termination criteria\n",
    "track_window = roi  # Initial tracking window\n",
    "ret, frame = video_capture.read()  # Read video frame\n",
    "hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)  # Convert to HSV\n",
    "dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)  # Back projection\n",
    "ret, track_window = cv2.CamShift(dst, track_window, term_crit)  # Apply CAM-Shift\n",
    "cv2.rectangle(frame, (track_window[0], track_window[1]), \n",
    "              (track_window[0] + track_window[2], track_window[1] + track_window[3]), \n",
    "              (0, 0, 255), 2)  # Draw rectangle around tracked object\n",
    "cv2.imshow('Tracking', frame)  # Display frame with tracking\n",
    "cv2.waitKey(1)\n",
    "\n",
    "# 25. Optical Flow Object Tracking  \n",
    "old_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)  # Feature parameters\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)  # Detect features\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))  \n",
    "p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, gray, p0, None, **lk_params)  # Calculate optical flow\n",
    "cv2.line(image, (p0[0][0], p0[0][1]), (p1[0][0], p1[0][1]), (0, 255, 0), 2)  # Draw line for tracking\n",
    "cv2.imshow('Optical Flow', image)  # Display optical flow result\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 26. Simple Object Tracking by Colour  \n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # Convert to HSV\n",
    "lower_color = np.array([30, 150, 50])  # Lower bound for color tracking\n",
    "upper_color = np.array([85, 255, 255])  # Upper bound for color tracking\n",
    "mask = cv2.inRange(hsv, lower_color, upper_color)  # Mask for tracking color\n",
    "res = cv2.bitwise_and(image, image, mask=mask)  # Apply mask to original image\n",
    "cv2.imshow('Color Tracking', res)  # Show tracked object\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 27. Facial Landmarks Detection with Dlib  \n",
    "detector = dlib.get_frontal_face_detector()  # Face detector\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Landmark predictor\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "faces = detector(gray)  # Detect faces\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)  # Get landmarks\n",
    "    for n in range(0, 68):\n",
    "        x, y = landmarks.part(n).x, landmarks.part(n).y  # Get coordinates\n",
    "        cv2.circle(image, (x, y), 1, (0, 255, 0), -1)  # Mark landmarks\n",
    "cv2.imshow('Facial Landmarks', image)  # Display image with landmarks\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 28. Face Swapping with Dlib  \n",
    "image1 = cv2.imread(\"face1.jpg\")  # Load first image\n",
    "image2 = cv2.imread(\"face2.jpg\")  # Load second image\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)  # Convert first image to grayscale\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)  # Convert second image to grayscale\n",
    "faces1 = detector(gray1)  # Detect faces in image1\n",
    "faces2 = detector(gray2)  # Detect faces in image2\n",
    "landmarks1 = predictor(gray1, faces1[0])  # Get landmarks from image1\n",
    "landmarks2 = predictor(gray2, faces2[0])  # Get landmarks from image2\n",
    "# Example swapping steps would include facial feature matching and blending (omitted for simplicity)\n",
    "\n",
    "# 29. Tilt Shift Effect  \n",
    "def tilt_shift(image):\n",
    "    mask = np.zeros_like(image)  # Create blank mask\n",
    "    mask[150:350, 150:350] = 255  # Define region for tilt-shift\n",
    "    blurred = cv2.GaussianBlur(image, (15, 15), 0)  # Apply blur\n",
    "    result = cv2.addWeighted(image, 0.7, blurred, 0.3, 0)  # Combine images\n",
    "    return result\n",
    "\n",
    "tilt_shifted = tilt_shift(image)  # Apply tilt-shift effect\n",
    "cv2.imshow('Tilt Shift', tilt_shifted)  # Display result\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 30. Grabcut Algorithm for Background Removal  \n",
    "mask = np.zeros(image.shape[:2], np.uint8)  # Create mask\n",
    "bgd_model = np.zeros((1, 65), np.float64)  # Background model\n",
    "fgd_model = np.zeros((1, 65), np.float64)  # Foreground model\n",
    "rect = (50, 50, 450, 290)  # Define rectangle for grabcut\n",
    "cv2.grabCut(image, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)  # Apply grabcut\n",
    "mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')  # Mask out background\n",
    "grabcut_result = image * mask2[:, :, np.newaxis]  # Apply mask to image\n",
    "cv2.imshow('Grabcut Result', grabcut_result)  # Display result\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 31. OCR with PyTesseract and EasyOCR  \n",
    "text_pytesseract = pytesseract.image_to_string(image)  # OCR with pytesseract\n",
    "reader = easyocr.Reader(['en'])  # Initialize EasyOCR reader\n",
    "text_easyocr = reader.readtext(image)  # OCR with EasyOCR\n",
    "print(\"PyTesseract Text:\", text_pytesseract)\n",
    "print(\"EasyOCR Text:\", text_easyocr)\n",
    "\n",
    "# 32. Barcode and QR generation and reading  \n",
    "# Generate Barcode\n",
    "barcode = cv2.barcode_BarcodeDetector()\n",
    "# Read QR code\n",
    "decoded_objects = decode(image)  # QR decoding\n",
    "for obj in decoded_objects:\n",
    "    print(f\"Data: {obj.data.decode('utf-8')}\")\n",
    "    print(f\"Type: {obj.type}\")\n",
    "\n",
    "# 33. YOLOv3 in OpenCV  \n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")  # Load YOLOv3 model\n",
    "layer_names = net.getLayerNames()  # Get layer names\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getLayers()]  # Get output layers\n",
    "blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)  # Prepare image for YOLO\n",
    "net.setInput(blob)  # Set input\n",
    "outs = net.forward(output_layers)  # Run forward\n",
    "\n",
    " pass\n",
    "\n",
    "# 34. Neural Style Transfer with OpenCV  \n",
    "# Apply neural style transfer using pre-trained model (omitted for simplicity)\n",
    "\n",
    "# 35. SSDs in OpenCV  \n",
    "net_ssd = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"res10_300x300_ssd_iter_140000.caffemodel\")  # Load SSD\n",
    "# Apply SSD object detection on image\n",
    "\n",
    "# 36. Colorise Black and White Photos  \n",
    "# Neural network approach for colorizing black and white photos\n",
    "\n",
    "# 37. Repair Damaged Photos with Inpainting  \n",
    "damaged_image = cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)  # Inpainting\n",
    "cv2.imshow('Inpainting Result', damaged_image)  # Display inpainting result\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 38. Add and remove Noise, Fix Contrast with Histogram Equalisation  \n",
    "noise_img = cv2.randn(image.copy(), (0, 0, 0), (20, 20, 20))  # Add noise\n",
    "contrast_img = cv2.equalizeHist(image)  # Histogram equalization\n",
    "\n",
    "# 39. Detect Blur in Images  \n",
    "blurred = cv2.Laplacian(image, cv2.CV_64F).var()  # Detect blur\n",
    "if blurred < 100:\n",
    "    print(\"Image is blurry\")\n",
    "else:\n",
    "    print(\"Image is clear\")\n",
    "\n",
    "# 40. Facial Recognition  \n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()  # Initialize face recognizer\n",
    "```\n",
    "\n",
    "These are common approaches you can take for each topic. Let me know if you'd like a more detailed breakdown of any particular one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
