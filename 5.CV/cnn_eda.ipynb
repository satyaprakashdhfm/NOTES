{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index:\n",
    "1. Scaling possibility in cnns\n",
    "2. cnn loss functions and transfere learning\n",
    "3. NOise Handling\n",
    "4. Image comparasion\n",
    "5. Outlier detection and mitigations\n",
    "6. Encoding of images\n",
    "7. Feature engineering:\n",
    "    normalization\n",
    "    dimensionality reduction\n",
    "8. Importance of different features\n",
    "9. autoencoders and types used in cnn\n",
    "10. Imbalance treating\n",
    "11. Data agumentation techniques\n",
    "12. Opencv\n",
    "13. pytorch\n",
    "14. keras- tensorflow\n",
    "15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Compound Scaling in Neural Networks üî¨\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "### 1. Traditional Scaling Dimensions\n",
    "- **Depth (d)**: Number of network layers\n",
    "- **Width (w)**: Number of channels/filters\n",
    "- **Resolution (r)**: Input image size\n",
    "\n",
    "### 2. Compound Scaling Formula\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{depth}: & d = \\alpha^{\\phi} \\\\\n",
    "\\text{width}: & w = \\beta^{\\phi} \\\\\n",
    "\\text{resolution}: & r = \\gamma^{\\phi}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 3. Constraint Equation\n",
    "$$\n",
    "\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2\n",
    "$$\n",
    "\n",
    "### 4. Key Parameters\n",
    "- $\\phi$: Compound scaling coefficient\n",
    "- $\\alpha, \\beta, \\gamma$: Dimension-specific scaling coefficients\n",
    "\n",
    "## Practical Insights\n",
    "- Systematically increases model capacity\n",
    "- Balances computational complexity\n",
    "- Prevents performance degradation\n",
    "- Maintains model efficiency\n",
    "\n",
    "## Visualization of Scaling Impact\n",
    "```python\n",
    "def compound_scaling(phi, alpha=1.2, beta=1.1, gamma=1.15):\n",
    "    depth = alpha ** phi\n",
    "    width = beta ** phi\n",
    "    resolution = gamma ** phi\n",
    "    return depth, width, resolution\n",
    "\n",
    "# Example scaling progression\n",
    "for phi in range(5):\n",
    "    d, w, r = compound_scaling(phi)\n",
    "    print(f\"œÜ={phi}: Depth={d:.2f}, Width={w:.2f}, Resolution={r:.2f}\")\n",
    "```\n",
    "\n",
    "\n",
    "### **Important Terms in CNNs**  \n",
    "\n",
    "1Ô∏è‚É£ **Depthwise Separable Convolution** (xception-2017)‚Äì Breaks standard convolution into **Depthwise Convolution** (one filter per channel) and **Pointwise Convolution** (1√ó1 conv to mix channels), reducing computation. \n",
    "\n",
    " **Mathematical Understanding of Spatially and Depthwise Separable Convolutions with an Example**\n",
    "\n",
    " **Step 1: Standard Convolution (Baseline for Comparison)**\n",
    "\n",
    " **Given:**\n",
    "- Input image: **4√ó4** with **3 channels (RGB)**\n",
    "- Kernel: **3√ó3** with **3 input channels and 2 output channels**\n",
    "- **Stride = 1**, No Padding (valid convolution)\n",
    "- **Output size** = **2√ó2**\n",
    "\n",
    "**Convolution Calculation:**\n",
    "Each **output pixel** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Sum of element-wise multiplication of } (3\\times3\\times3) \\text{ filter with } (3\\times3\\times3) \\text{ region of input.}\n",
    "$$\n",
    "\n",
    " **Total multiplications per output pixel:**\n",
    "$$\n",
    "3 \\times 3 \\times 3 = 27\n",
    "$$\n",
    "\n",
    " **Total multiplications for entire output** (**$2 \\times 2$ output per channel, 2 output channels**):\n",
    "$$\n",
    "2 \\times 2 \\times 2 \\times 27 = 216\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Step 2: Spatially Separable Convolution**\n",
    "**Goal:** Instead of using a **single 2D filter**, we **split** it into two smaller 1D filters.\n",
    "\n",
    "- Instead of a **$3 \\times 3$** filter, we use:\n",
    "  - A **$3 \\times 1$** filter (vertical)\n",
    "  - A **$1 \\times 3$** filter (horizontal)\n",
    "\n",
    " **Calculation Breakdown**\n",
    "1. **First, apply a $3 \\times 1$ filter (vertical pass)**\n",
    "   - Each pixel involves: **3 multiplications** (instead of 9)\n",
    "   - Total multiplications:\n",
    "     $$\n",
    "     2 \\times 2 \\times 3 \\times 3 = 36\n",
    "     $$\n",
    "\n",
    "2. **Next, apply a $1 \\times 3$ filter (horizontal pass)**\n",
    "   - Each pixel involves: **3 multiplications**\n",
    "   - Total multiplications:\n",
    "     $$\n",
    "     2 \\times 2 \\times 3 \\times 3 = 36\n",
    "     $$\n",
    "\n",
    "**Total for Spatially Separable Convolution:**\n",
    "$$\n",
    "36 + 36 = 72\n",
    "$$\n",
    "\n",
    "‚úÖ **Reduction from 216 (standard) ‚Üí 72 (spatially separable) üöÄ**\n",
    "\n",
    "\n",
    " **Step 3: Depthwise Separable Convolution**\n",
    "**Goal:** Instead of applying a single **3D filter**, we split it into:\n",
    "1. **Depthwise Convolution** (applies a small filter to each channel separately)\n",
    "2. **Pointwise Convolution** (uses \\(1 \\times 1\\) convolutions to mix channels)\n",
    "\n",
    " **Depthwise Convolution Calculation**\n",
    "- Each input channel has **its own** \\(3 \\times 3\\) filter.\n",
    "- Each **output pixel** now does:\n",
    "\n",
    "$$\n",
    "3 \\times 3 = 9\n",
    "$$\n",
    "\n",
    "- **Total multiplications for depthwise step:**\n",
    "  $$\n",
    "  2 \\times 2 \\times 3 \\times 9 = 108\n",
    "  $$\n",
    "\n",
    " **Pointwise Convolution Calculation (1√ó1 Convolution)**\n",
    "- A **\\(1 \\times 1\\) filter** is applied across all **3 input channels** for **each of the 2 output channels**.\n",
    "- Each **output pixel** now does:\n",
    "\n",
    "$$\n",
    "3 \\times 1 \\times 1 = 3\n",
    "$$\n",
    "\n",
    "- **Total multiplications for pointwise step:**\n",
    "  $$\n",
    "  2 \\times 2 \\times 2 \\times 3 = 24\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    " **Final Comparison**\n",
    "| **Method**                   | **Multiplications** |\n",
    "|------------------------------|--------------------|\n",
    "| **Standard Convolution**      | **216**           |\n",
    "| **Spatially Separable Conv**  | **72**            |\n",
    "| **Depthwise Separable Conv**  | **108 + 24 = 132** |\n",
    "\n",
    " **Key Observations:**\n",
    "1. **Spatially Separable Convolution is the most efficient** (~67% fewer operations than standard).\n",
    "2. **Depthwise Separable Convolution also reduces computation but not as much as spatially separable.**\n",
    "3. **Depthwise Separable is more commonly used** (e.g., in MobileNet) because it still **captures depth-wise features effectively**.\n",
    "\n",
    "\n",
    "\n",
    " **Which One to Use?**\n",
    "- **Use Spatially Separable Convolutions** when the kernel can be **decomposed** into two smaller 1D kernels.\n",
    "- **Use Depthwise Separable Convolutions** when you want to process each input channel **independently** before mixing them.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "https://youtu.be/c1RBQzKsDCk?si=abMyem2hW2o9pWBC\n",
    "\n",
    "\n",
    "2Ô∏è‚É£ **Bottleneck Residual Block** ‚Äì A shortcut connection that allows deeper networks to train efficiently by preventing gradient vanishing.  \n",
    "\n",
    " **Bottleneck Layers vs. Residual Layers**  \n",
    "\n",
    "**Short Answer:** No, bottleneck layers and residual layers are **not the same**, but they are often used together, especially in architectures like **ResNet**.  \n",
    "\n",
    "---\n",
    "\n",
    " **1. Bottleneck Layers**\n",
    " **Definition:**  \n",
    "A **bottleneck layer** is a special type of layer designed to **reduce** the number of parameters and computations while preserving important information.  \n",
    "\n",
    " **Structure:**\n",
    "A common **bottleneck block** in CNNs consists of:\n",
    "1. **1√ó1 convolution (dimension reduction)** ‚Üí Reduces channels.\n",
    "2. **3√ó3 convolution (feature extraction)** ‚Üí Standard processing.\n",
    "3. **1√ó1 convolution (dimension expansion)** ‚Üí Restores channel depth.\n",
    "\n",
    "This reduces computational cost while maintaining accuracy.\n",
    "\n",
    " **Example in ResNet:**\n",
    "$$\n",
    "\\text{Conv}(1 \\times 1, \\text{reduce channels}) \\rightarrow \\text{Conv}(3 \\times 3) \\rightarrow \\text{Conv}(1 \\times 1, \\text{restore channels})\n",
    "$$\n",
    "\n",
    " **Why use Bottlenecks?**\n",
    "- Reduces parameters (e.g., **ResNet-50, ResNet-101**).\n",
    "- Improves efficiency in deep networks.\n",
    "\n",
    "---\n",
    "\n",
    " **2. Residual Layers (Residual Connections)**\n",
    " **Definition:**\n",
    "A **residual layer** (or residual block) is a layer that **skips connections** to allow information to flow directly through the network. This helps combat the **vanishing gradient problem**.\n",
    "\n",
    " **Structure:**\n",
    "A standard **residual block** follows:\n",
    "$$\n",
    "\\text{Conv}(3 \\times 3) \\rightarrow \\text{Conv}(3 \\times 3) + \\text{Input (Skip Connection)}\n",
    "$$\n",
    "\n",
    "Instead of directly passing output from convolution, it **adds the input (skip connection)** to help preserve original information.\n",
    "\n",
    " **Why use Residual Layers?**\n",
    "- Helps very deep networks **train effectively**.\n",
    "- Avoids **vanishing gradients**.\n",
    "- Used in **ResNet, EfficientNet, and Transformer models**.\n",
    "\n",
    "\n",
    "\n",
    " **Are Bottleneck and Residual Layers the Same?**\n",
    "No, but **bottleneck layers are often used inside residual layers** to improve efficiency.  \n",
    "- **ResNet-18 & ResNet-34** ‚Üí Use **basic residual blocks**.\n",
    "- **ResNet-50 & deeper** ‚Üí Use **bottleneck residual blocks** (1√ó1 ‚Üí 3√ó3 ‚Üí 1√ó1).\n",
    "\n",
    "\n",
    "\n",
    " **Comparison Table**\n",
    "| Feature            | Bottleneck Layer | Residual Layer |\n",
    "|--------------------|----------------|---------------|\n",
    "| **Purpose**       | Reduce computation | Improve gradient flow |\n",
    "| **Key Idea**      | 1√ó1 ‚Üí 3√ó3 ‚Üí 1√ó1 convolution | Skip connection (input added to output) |\n",
    "| **Used in**       | ResNet-50, MobileNet | ResNet (all versions) |\n",
    "| **Effect**        | Smaller models, efficient computation | Helps train deep networks |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "3Ô∏è‚É£ **Inverted Residual Block** ‚Äì Expands channels before applying depthwise convolution, making computations more efficient in mobile networks.  \n",
    "\n",
    "\n",
    "**Inverse Residual Networks: Detailed Explanation**\n",
    "\n",
    "**Inverse Residual Networks** (IRNs) are a specific type of neural network architecture that uses **inverted residual blocks** as their building blocks. They are a significant component of **MobileNetV2**, a lightweight and efficient neural network model for mobile and embedded devices.\n",
    "\n",
    " **Key Features of Inverted Residual Networks:**\n",
    "1. **Inverted Residual Block**: The core building block of an inverse residual network. It consists of a combination of **expansion**, **depthwise separable convolution**, and **projection** with **residual connections**.\n",
    "2. **Residual Connections**: The shortcut connections that add the input to the output of the block, enabling faster convergence and reducing the vanishing gradient problem.\n",
    "3. **Lightweight Design**: The use of efficient convolutions (such as **depthwise separable convolutions**) and the inversion of residual blocks helps to reduce computation and memory usage, making the network very efficient, especially for edge devices.\n",
    "\n",
    " **How Inverse Residual Networks Work:**\n",
    "\n",
    "Inverse residual networks are designed to improve the performance of convolutional neural networks (CNNs) by using **efficient operations** while maintaining high accuracy. The key aspects of these networks are:\n",
    "\n",
    "1. **Expansion (1x1 Convolution)**:\n",
    "   - The input is first expanded in the number of channels using a **1x1 convolution**. This increases the network's ability to represent complex features by boosting the depth of the feature maps.\n",
    "   \n",
    "2. **Depthwise Separable Convolution**:\n",
    "   - The expanded feature map undergoes **depthwise separable convolutions**, where a separate convolution is applied to each channel. This reduces the computation cost compared to traditional convolutions (which combine all input channels in a single convolution).\n",
    "   \n",
    "3. **Projection (1x1 Convolution)**:\n",
    "   - After the depthwise convolution, the number of channels is reduced back to a smaller value using another **1x1 convolution**. This projection operation reduces the size of the output and focuses on the essential features.\n",
    "   \n",
    "4. **Residual Connection**:\n",
    "   - The **input** is added back to the **output** through a **skip connection** (residual connection). This helps the network preserve important information from earlier layers and reduces the chance of overfitting by making the training process easier.\n",
    "\n",
    " **Why Inverted Residual Networks?**\n",
    "- **Efficiency**: The key innovation of IRNs lies in the **depthwise separable convolution**, which reduces computational complexity without significantly sacrificing accuracy. The **expansion** and **projection** operations allow the network to use **a relatively small number of parameters** compared to traditional convolutional networks.\n",
    "  \n",
    "- **Mobile and Edge Applications**: Since IRNs use **fewer parameters** and **less computation**, they are particularly suitable for **mobile and embedded devices**, where computational power and memory are limited.\n",
    "\n",
    " **Example Numbers:**\n",
    "\n",
    "Let‚Äôs assume the following for a simple example:\n",
    "\n",
    "- Input feature map: $ 4 \\times 4 \\times 3 $ (height 4, width 4, and 3 channels).\n",
    "- Expansion to 6 channels per input channel.\n",
    "  \n",
    "**Step 1: Expansion (1x1 Conv)**\n",
    "- $ 4 \\times 4 \\times 3 $ becomes $ 4 \\times 4 \\times 18 $ after expanding the channels.\n",
    "  \n",
    "**Step 2: Depthwise Separable Convolution (3x3 Conv)**\n",
    "- Apply a $ 3 \\times 3 $ depthwise convolution (one filter per channel). The output shape is still $ 4 \\times 4 \\times 18 $, where each of the 18 channels is processed separately.\n",
    "  \n",
    "**Step 3: Projection (1x1 Conv)**\n",
    "- Apply another $ 1 \\times 1 $ convolution to reduce the 18 channels to 3. So, the output shape becomes $ 4 \\times 4 \\times 3 $.\n",
    "  \n",
    "**Step 4: Add Residual**\n",
    "- Finally, the input $ 4 \\times 4 \\times 3 $ is added to the output $ 4 \\times 4 \\times 3 $, resulting in the same dimensions $ 4 \\times 4 \\times 3 $.\n",
    "\n",
    "\n",
    "\n",
    " **Advantages of Inverted Residual Networks:**\n",
    "1. **Faster Computation**: By using depthwise separable convolutions, IRNs require fewer computations compared to standard convolutions.\n",
    "2. **Smaller Model Size**: The number of parameters is reduced because depthwise convolutions only use one filter per input channel, which reduces memory usage.\n",
    "3. **Improved Efficiency**: The use of **expansion** and **projection** enables the network to capture complex features without being computationally expensive, making it ideal for mobile applications.\n",
    "\n",
    "\n",
    "\n",
    " **Applications of Inverted Residual Networks:**\n",
    "1. **MobileNetV2**: The most well-known use of inverted residual blocks is in **MobileNetV2**, which uses these blocks to build a lightweight, efficient, and accurate model suitable for mobile and embedded devices.\n",
    "2. **Real-time Object Detection**: In tasks where fast processing is required (e.g., object detection on mobile devices), inverted residual networks are used to process images efficiently.\n",
    "3. **Edge Computing**: IRNs are widely used in edge devices where computation and memory resources are limited.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "4Ô∏è‚É£ **Squeeze-and-Excitation (SE) Block** ‚Äì Learns to assign different importance to different feature channels, improving feature extraction.  \n",
    "\n",
    "5Ô∏è‚É£ **Self-Attention Mechanism** ‚Äì Enables the model to focus on important image regions instead of relying on fixed-size filters, used in Vision Transformers (ViT).  \n",
    "\n",
    "6Ô∏è‚É£ **Feature Pyramid Network (FPN)** ‚Äì Extracts multi-scale features to improve object detection across different object sizes.  \n",
    "\n",
    "7Ô∏è‚É£ **Stochastic Depth** ‚Äì Randomly skips some layers during training to reduce overfitting and improve efficiency in deep networks.  \n",
    "\n",
    "8Ô∏è‚É£ **DropBlock** ‚Äì A form of dropout that removes **contiguous** regions instead of random individual pixels, improving CNN generalization.  \n",
    "\n",
    "9Ô∏è‚É£ **Ghost Modules** ‚Äì Reduces redundant feature maps by generating some from existing ones, creating lightweight CNNs.  \n",
    "\n",
    "üîü **Dynamic Convolutions** ‚Äì Uses input-dependent filters instead of fixed ones, making convolutions more adaptive and efficient.  \n",
    "\n",
    "\n",
    "\n",
    "# CNN Loss Functions and Transfer Learning\n",
    "\n",
    "## Loss Functions in CNNs\n",
    "\n",
    "\n",
    "\n",
    "| **Task**                             | **Common CNN Architectures**    | **Loss Function**                          |  \n",
    "|--------------------------------------|--------------------------------|--------------------------------|  \n",
    "| Image Classification                 | ResNet, VGG, MobileNet         | Cross-Entropy Loss (Softmax)  |  \n",
    "| OCR (Character Recognition)          | CRNN, CNN-LSTM                 | CTC Loss (Connectionist Temporal Classification) |  \n",
    "| Medical Imaging (Segmentation)       | UNet, DenseNet                 | Dice Loss, IoU Loss, Focal Loss |  \n",
    "| Object Detection                     | YOLO, Faster R-CNN, SSD        | Smooth L1 Loss, Focal Loss, GIoU Loss |  \n",
    "| Image Segmentation                   | DeepLabV3, UNet                | Cross-Entropy Loss, Dice Loss |  \n",
    "| Face Recognition                     | FaceNet, ArcFace               | Triplet Loss, ArcFace Loss, Contrastive Loss |  \n",
    "| Style Transfer                        | CNN-based GANs                 | Perceptual Loss, Content Loss, Style Loss |  \n",
    "| Super-Resolution                     | ESRGAN, SRCNN                  | L1 Loss, Perceptual Loss, SSIM Loss |  \n",
    "| Image Inpainting                     | Context Encoder, Partial Conv  | L1 Loss, Perceptual Loss, Adversarial Loss |  \n",
    "| Diffusion Models                     | DDPM, Stable Diffusion         | Variational Lower Bound Loss (ELBO), MSE Loss |  \n",
    "\n",
    "### **Key Observations**:  \n",
    "- **Classification** ‚Üí Cross-Entropy Loss  \n",
    "- **Object Detection** ‚Üí Smooth L1, IoU-based Losses  \n",
    "- **Segmentation** ‚Üí Dice, IoU, Cross-Entropy  \n",
    "- **Face Recognition** ‚Üí Contrastive & Metric Learning Losses  \n",
    "- **Generative Models** ‚Üí L1, Perceptual, Adversarial Loss  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Classification Tasks\n",
    "1. **Cross-Entropy Loss**\n",
    "   - Primary loss for multi-class classification\n",
    "   - Measures prediction probability distribution\n",
    "   - Formula: $L = -\\sum_{i} y_i \\log(\\hat{y}_i)$\n",
    "\n",
    "2. **Focal Loss**\n",
    "   - Handles class imbalance\n",
    "   - Reduces loss for well-classified examples\n",
    "   - Emphasizes hard-to-classify samples\n",
    "\n",
    "3. **Categorical Cross-Entropy**\n",
    "   - For mutually exclusive classes\n",
    "   - One-hot encoded target vectors\n",
    "\n",
    "### Segmentation Tasks\n",
    "1. **Dice Loss**\n",
    "   - Measures overlap between prediction and ground truth\n",
    "   - Effective for imbalanced segmentation\n",
    "\n",
    "2. **Intersection over Union (IoU) Loss**\n",
    "   - Measures segmentation accuracy\n",
    "   - Commonly used in medical image segmentation\n",
    "\n",
    "## Transfer Learning in CNNs\n",
    "\n",
    "### Why Transfer Learning?\n",
    "- Reduces training time\n",
    "- Improves performance on small datasets\n",
    "- Leverages pre-trained network features\n",
    "\n",
    "### Implementation Strategies\n",
    "1. **Feature Extraction**\n",
    "   - Freeze pre-trained layers\n",
    "   - Add custom classification layer\n",
    "   ```python\n",
    "   base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n",
    "   base_model.trainable = False\n",
    "   \n",
    "   model = tf.keras.Sequential([\n",
    "       base_model,\n",
    "       tf.keras.layers.GlobalAveragePooling2D(),\n",
    "       tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "2. **Fine-Tuning**\n",
    "   - Unfreeze later layers\n",
    "   - Low learning rate for fine-tuning\n",
    "   ```python\n",
    "   base_model.trainable = True\n",
    "   model.compile(optimizer=tf.keras.optimizers.Adam(1e-5))\n",
    "   ```\n",
    "\n",
    "### Popular Pre-trained Models\n",
    "- ResNet\n",
    "- VGG\n",
    "- Inception\n",
    "- EfficientNet\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Handling:\n",
    "\n",
    "\n",
    "| **Task**                             | **Noise Handling Needed?** | **Reason**                                      |\n",
    "|--------------------------------------|--------------------------|------------------------------------------------|\n",
    "| Image Classification (ResNet, VGG)   | ‚úÖ Yes                   | Noise distorts feature extraction.             |\n",
    "| OCR (CRNN)                           | ‚úÖ Yes                   | Noisy text affects character recognition.      |\n",
    "| Medical Imaging (UNet, DenseNet)     | ‚úÖ Yes                   | Artifacts reduce diagnostic accuracy.         |\n",
    "| Object Detection (YOLO, Faster R-CNN)| ‚úÖ Yes                   | Small objects get lost in noise.              |\n",
    "| Image Segmentation (DeepLabV3)       | ‚úÖ Yes                   | Boundary details get blurred by noise.        |\n",
    "| Face Recognition (FaceNet, ArcFace)  | ‚úÖ Yes                   | Noise can alter key facial features.          |\n",
    "| Style Transfer (CNN-based GANs)      | ‚ùå No                    | Noise can enhance artistic effects.           |\n",
    "| Super-Resolution (ESRGAN)            | ‚ùå No                    | Models are trained to restore noisy images.   |\n",
    "| Image Inpainting (Context Encoder)   | ‚ùå No                    | Model learns to reconstruct missing pixels.   |\n",
    "| Diffusion Models (DDPM, Stable Diff.)| ‚ùå No                    | Training process already involves noise.      |  \n",
    "\n",
    " \n",
    "\n",
    "### **1. Image Quality Assessment (IQA)**  \n",
    "- Use metrics like **BRISQUE, NIQE, or PIQE** to quantify image quality.  \n",
    "- Use deep learning-based **no-reference IQA models** to filter out bad-quality images.  \n",
    "\n",
    "### **2. Preprocessing and Enhancement**  \n",
    "- **Deblurring**: Use deconvolution, Wiener filters, or deep learning models like **DeblurGAN**.  \n",
    "- **Denoising**: Use **Gaussian filters, Non-Local Means, or DnCNN (deep learning)**.  \n",
    "- **Contrast Enhancement**: Use **CLAHE (Contrast Limited Adaptive Histogram Equalization)**.  \n",
    "\n",
    "### **3. Data Augmentation for Variability**  \n",
    "- Simulate real-world distortions to make the model robust (motion blur, rotations, occlusions).  \n",
    "\n",
    "### **4. Removing Low-Quality Images**  \n",
    "- Automatically filter out images with extreme blurriness, poor lighting, or occlusions.  \n",
    "\n",
    "### **5. Synthetic Data & Inpainting**  \n",
    "- Use **GANs or diffusion models** to generate missing parts of an image.  \n",
    "- **Image inpainting models** like **DeepFill** can reconstruct damaged or missing areas.  \n",
    "\n",
    "### **6. Active Learning & Human-in-the-Loop**  \n",
    "- Have a manual or semi-automated review system where bad images are labeled and handled.  \n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **similar but not exact duplicate images** (near-duplicates)\n",
    "\n",
    "### **how we can compare one image with another image** \n",
    "\n",
    "Here‚Äôs how you can handle it:  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " **1. Perceptual Hashing (pHash, dHash, aHash)**\n",
    "- Converts images into a hash value and compares hashes.  \n",
    "- Works well for detecting near-duplicates with small modifications (resizing, slight noise, brightness changes).  \n",
    "- **Use Case:** Removing redundant images in large datasets (e.g., Google reverse image search).  \n",
    "\n",
    "**2. Structural Similarity Index (SSIM)**\n",
    "- Measures similarity between two images based on luminance, contrast, and structure.  \n",
    "- **Use Case:** Detecting similar images in medical scans, satellite imagery, or product databases.  \n",
    "\n",
    "**3. Feature Extraction + Cosine Similarity**\n",
    "- Extract deep features from CNNs (ResNet, VGG, EfficientNet) and compute cosine similarity.  \n",
    "- **Use Case:** Large-scale image deduplication (e.g., removing nearly identical faces in facial recognition datasets).  \n",
    "\n",
    " **4. Autoencoder-Based Embeddings**\n",
    "- Train an autoencoder to learn compact image representations and use Euclidean distance to detect similar images.  \n",
    "- **Use Case:** Removing similar medical X-ray scans or industrial defect images.  \n",
    "\n",
    "**5. Clustering-Based Deduplication (K-Means, DBSCAN)**\n",
    "- Extract features from images and cluster similar ones together.  \n",
    "- **Use Case:** Grouping near-duplicate images in e-commerce datasets (similar product images).  \n",
    "\n",
    " \n",
    "\n",
    "\n",
    " **üöÄ What You Can Do With Near-Duplicates?**  \n",
    "‚úÖ **Remove them** ‚Äì If redundancy is high, drop similar images.  \n",
    "‚úÖ **Merge them** ‚Äì Keep only representative images from clusters.  \n",
    "‚úÖ **Augment the dataset** ‚Äì Keep variations if they add value.  \n",
    "‚úÖ **Reweight them** ‚Äì Reduce their impact during training.  \n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### outlier treatment\n",
    " \n",
    "\n",
    "### **How to Detect Outliers in Images?**  \n",
    "\n",
    "#### **1. Statistical & Traditional Methods**  \n",
    "- **Histogram Analysis**: Check pixel intensity distributions for anomalies.  \n",
    "- **Edge Detection (Sobel, Canny)**: Detect unusual patterns in images.  \n",
    "- **Z-Score on Features**: Extract features (like color histograms, texture) and apply Z-score to detect outliers.  \n",
    "\n",
    "#### **2. Feature-Based Methods (Using CNNs or Pretrained Models)**  \n",
    "- Extract deep features using **ResNet, VGG, or MobileNet** and apply:  \n",
    "  - **t-SNE or PCA**: Visualize clusters and find outliers.  \n",
    "  - **K-Means Clustering**: Identify rare classes.  \n",
    "  - **Isolation Forest / One-Class SVM**: Detect anomalies in feature space.  \n",
    "\n",
    "#### **3. Autoencoders (Unsupervised Anomaly Detection)**  \n",
    "- Train an **autoencoder to reconstruct normal images**, then measure reconstruction error.  \n",
    "- High error = outlier image.  \n",
    "\n",
    "#### **4. Deep Learning-Based Anomaly Detection**  \n",
    "- **GAN-Based Methods (AnoGAN, f-AnoGAN)**: Learn the normal data distribution and detect anomalies.  \n",
    "- **Vision Transformers (ViTs) for Anomaly Detection**: Detect spatial inconsistencies in images.  \n",
    "\n",
    "#### **5. Object-Based Outlier Detection**  \n",
    "- Use **object detection models (YOLO, Faster R-CNN)** to check if an unexpected object is present in the image.  \n",
    "\n",
    "\n",
    "\n",
    "| **Task**                             | **Outliers Affect Performance?** | **What to Do?**                              |  \n",
    "|--------------------------------------|-------------------------------|--------------------------------|  \n",
    "| Image Classification (ResNet, VGG)   | ‚úÖ Yes                        | Remove extreme outliers, augment data |  \n",
    "| OCR (CRNN)                           | ‚úÖ Yes                        | Normalize text size, remove distortions |  \n",
    "| Medical Imaging (UNet, DenseNet)     | ‚úÖ Yes                        | Use anomaly detection, filter extreme cases |  \n",
    "| Object Detection (YOLO, Faster R-CNN)| ‚úÖ Yes                        | Remove irrelevant objects, balance dataset |  \n",
    "| Image Segmentation (DeepLabV3)       | ‚úÖ Yes                        | Handle class imbalance, smooth noisy edges |  \n",
    "| Face Recognition (FaceNet, ArcFace)  | ‚úÖ Yes                        | Exclude extreme angles/blurred faces |  \n",
    "| Style Transfer (CNN-based GANs)      | ‚ùå No                         | No special handling needed |  \n",
    "| Super-Resolution (ESRGAN)            | ‚ùå No                         | Model already trained to restore images |  \n",
    "| Image Inpainting (Context Encoder)   | ‚ùå No                         | Model learns to reconstruct missing parts |  \n",
    "| Diffusion Models (DDPM, Stable Diff.)| ‚ùå No                         | No filtering needed; noise is part of training |  \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoding is often required in Image EDA** to transform raw images into a form suitable for analysis.\n",
    "\n",
    "\n",
    " Here are key encoding techniques:  \n",
    "\n",
    "### **1. Pixel-Level Encoding**  \n",
    "- **Grayscale Conversion**: Converts RGB images to single-channel grayscale for easier analysis.  \n",
    "- **Color Spaces (RGB ‚Üí HSV, LAB, YUV)**: Different representations help highlight specific features (e.g., HSV is good for color-based analysis).  \n",
    "- **Histogram Encoding**: Represents image intensity distribution for statistical analysis.  \n",
    "\n",
    "### **2. Feature Extraction (Encoding Images into Vectors)**  \n",
    "- **CNN Feature Maps**: Extract deep features using pre-trained models (ResNet, VGG, etc.).  \n",
    "- **Histogram of Oriented Gradients (HOG)**: Encodes shape and edge information.  \n",
    "- **Local Binary Patterns (LBP)**: Captures texture information.  \n",
    "- **SIFT, SURF, ORB**: Encodes keypoints and descriptors for object detection.  \n",
    "\n",
    "### **3. Dimensionality Reduction (Encoding High-Dimensional Features into Lower Dimensions)**  \n",
    "- **PCA/t-SNE/UMAP**: Encodes image features into a lower-dimensional space for visualization and clustering.  \n",
    "\n",
    "### **4. Hashing & Compact Representations**  \n",
    "- **Perceptual Hashing (pHash, aHash, dHash)**: Converts images into hash values for similarity detection.  \n",
    "\n",
    "### **5. Autoencoder Representations**  \n",
    "- **Latent Space Encoding**: Compress images into lower-dimensional embeddings using autoencoders for anomaly detection.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **feature engineering** techniques is **used in different computer vision (CV) applications and architectures**:  \n",
    "\n",
    "\n",
    "\n",
    "| **Task**                             | **Feature Scaling Needed?** | **What to Do?**                          |  \n",
    "|--------------------------------------|--------------------------|--------------------------------|  \n",
    "| Image Classification (ResNet, VGG)   | ‚úÖ Yes                   | Normalize pixel values (0-1 or -1 to 1) |  \n",
    "| OCR (CRNN)                           | ‚úÖ Yes                   | Resize text images, normalize intensities |  \n",
    "| Medical Imaging (UNet, DenseNet)     | ‚úÖ Yes                   | Normalize per dataset (e.g., Z-score, min-max) |  \n",
    "| Object Detection (YOLO, Faster R-CNN)| ‚úÖ Yes                   | Resize images, normalize pixel values |  \n",
    "| Image Segmentation (DeepLabV3)       | ‚úÖ Yes                   | Normalize pixel values per channel |  \n",
    "| Face Recognition (FaceNet, ArcFace)  | ‚úÖ Yes                   | Normalize facial embeddings (L2 norm) |  \n",
    "| Style Transfer (CNN-based GANs)      | ‚ùå No                    | No strict scaling needed |  \n",
    "| Super-Resolution (ESRGAN)            | ‚ùå No                    | Works on raw pixel values |  \n",
    "| Image Inpainting (Context Encoder)   | ‚ùå No                    | Model learns to reconstruct missing parts |  \n",
    "| Diffusion Models (DDPM, Stable Diff.)| ‚ùå No                    | No strict scaling, models process noise naturally |  \n",
    "\n",
    "**Key Takeaways:**  \n",
    "- **Discriminative models** (classification, detection, segmentation) need **feature scaling** to improve convergence and stability.  \n",
    "- **Generative models** (super-resolution, inpainting, diffusion) often work directly with raw pixel values. üöÄ\n",
    "\n",
    "## **1. Feature Scaling & Normalization (Preprocessing)**\n",
    "### **Where It‚Äôs Used?**\n",
    "‚úÖ **Deep Learning Models (CNNs, ViTs, etc.)** ‚Äì Normalization ensures stable and faster convergence.  \n",
    "‚úÖ **Image Generation (GANs, VAEs)** ‚Äì Helps models learn a balanced distribution of pixel intensities.  \n",
    "‚úÖ **Medical Imaging** ‚Äì Standardization ensures that pixel intensities remain comparable across scans.  \n",
    "\n",
    "\n",
    "\n",
    "## **2. Feature Transformation (Extracting Meaningful Representations)**\n",
    "### **Where It‚Äôs Used?**\n",
    "‚úÖ **Color Space Transformations** ‚Äì  \n",
    "   - **Face Recognition** (HSV color space improves skin tone detection).  \n",
    "   - **Autonomous Vehicles** (LAB helps in road/lane detection).  \n",
    "‚úÖ **Fourier Transform (FFT)** ‚Äì  \n",
    "   - **Texture Analysis** (detects repetitive patterns, used in defect detection).  \n",
    "   - **Watermark Detection** (extracts frequency-domain features).  \n",
    "‚úÖ **Wavelet Transform** ‚Äì  \n",
    "   - **Medical Image Analysis** (detects tumors in MRI scans).  \n",
    "   - **Steganography** (hides data in images using frequency transformations).  \n",
    "‚úÖ **Edge Detection (Sobel, Canny)** ‚Äì  \n",
    "   - **Object Detection & Segmentation** (outlines key structures in an image).  \n",
    "   - **Autonomous Navigation** (detects obstacles in self-driving cars).  \n",
    "‚úÖ **Histogram Equalization / CLAHE** ‚Äì  \n",
    "   - **Satellite Image Enhancement** (improves contrast for feature detection).  \n",
    "   - **Surveillance Cameras** (enhances night-vision footage).  \n",
    "\n",
    "### **dimensionality reduction** \n",
    "fits within the context of **computer vision** specifically. Let's dive into how **dimensionality reduction** methods are applied directly to **images** and how they benefit **computer vision tasks**.\n",
    "\n",
    "### **Dimensionality Reduction in Computer Vision**\n",
    "\n",
    "In computer vision, images are often represented as **high-dimensional data** (especially after feature extraction), which can be **computationally expensive** and **difficult to manage**. Dimensionality reduction techniques are used to handle this challenge by **compressing the data** while preserving key information. Here's how dimensionality reduction is used in **computer vision**:\n",
    "\n",
    "\n",
    "\n",
    "## **1. Image Compression**  \n",
    "### **Why it‚Äôs Needed:**\n",
    "- **Reducing storage space** for images or videos.\n",
    "- **Improving transmission speeds** for large datasets or when sending images over the internet.\n",
    "\n",
    "### **How It‚Äôs Done:**\n",
    "- **PCA**: Can be used to compress images by transforming the data into a lower-dimensional space and keeping only the **most important principal components**. This reduces the number of features without losing too much information.\n",
    "  \n",
    "#### **Where it‚Äôs Used**:\n",
    "- **Medical Imaging**: Storing large volumes of high-resolution images (e.g., MRI scans, X-rays) more efficiently.\n",
    "- **Satellite Imaging**: Compressing high-resolution imagery to reduce storage and transmission needs.\n",
    "\n",
    "\n",
    "## **2. Feature Compression & Speeding Up CNNs**\n",
    "### **Why it‚Äôs Needed:**\n",
    "- **Reducing complexity** in neural networks, particularly when dealing with high-dimensional feature maps from convolutional layers.\n",
    "- **Improving model efficiency** and reducing the risk of overfitting by focusing on the most informative features.\n",
    "\n",
    "### **How It‚Äôs Done:**\n",
    "- **Autoencoders**: Learn to compress feature maps (from CNNs) into a smaller latent representation. The encoder part of the autoencoder reduces the dimensions, while the decoder tries to reconstruct the original image.\n",
    "  \n",
    "#### **Where it‚Äôs Used**:\n",
    "- **Deep Learning**: In deep networks like **VGG**, **ResNet**, and **EfficientNet**, autoencoders can be used to reduce the dimensionality of features extracted by convolutional layers.\n",
    "- **Facial Recognition**: After extracting deep features, dimensionality reduction techniques like PCA or t-SNE can help in visualizing and comparing facial features across a dataset of images.\n",
    "\n",
    "\n",
    "\n",
    "## **3. Visualization and Exploration of Image Features**  \n",
    "### **Why it‚Äôs Needed:**\n",
    "- **Understanding and exploring** the feature space of high-dimensional image data.\n",
    "- **Visualizing complex data** (like embeddings from deep learning models) in a human-readable way.\n",
    "\n",
    "### **How It‚Äôs Done:**\n",
    "- **t-SNE** and **UMAP**: These methods are commonly used to **visualize** high-dimensional embeddings (e.g., deep features from CNNs or pre-trained models) in **2D or 3D** spaces. They help reveal clusters, similarities, and relationships between image features.\n",
    "  \n",
    "#### **Where it‚Äôs Used**:\n",
    "- **Feature Visualization**: Visualizing the learned features of a CNN to better understand how the network is representing different objects or scenes.\n",
    "- **Cluster Visualization**: Grouping similar images together based on extracted features to identify patterns or anomalies.\n",
    "\n",
    "\n",
    "\n",
    "## **4. Image Classification and Feature Selection**  \n",
    "### **Why it‚Äôs Needed:**\n",
    "- **Reducing the dimensionality** of the feature space can make classification tasks more efficient and less prone to overfitting.\n",
    "- **Improving accuracy** by focusing on the most relevant features.\n",
    "\n",
    "### **How It‚Äôs Done:**\n",
    "- **PCA**: Used to reduce the dimensionality of image data after feature extraction (e.g., from CNNs). By selecting only the **top principal components**, PCA helps in **focusing on the most important patterns** for classification tasks.\n",
    "- **LDA**: Applied when we have labeled data and need to **separate classes** by finding features that maximize the distance between class distributions.\n",
    "\n",
    "#### **Where it‚Äôs Used**:\n",
    "- **Object Recognition**: Reducing image features to a lower-dimensional space to improve model performance and classification accuracy.\n",
    "- **Face Recognition**: Using **Eigenfaces** (a PCA variant) for dimensionally reducing facial feature data, which helps identify unique faces in a dataset.\n",
    "\n",
    "\n",
    "## **5. Speeding Up Object Detection**  \n",
    "### **Why it‚Äôs Needed:**\n",
    "- **Faster object detection** for real-time applications (e.g., autonomous driving or security surveillance).\n",
    "- Reducing **background noise** or irrelevant features that could slow down the detection process.\n",
    "\n",
    "### **How It‚Äôs Done:**\n",
    "- **PCA/Autoencoders**: These techniques are used to compress and reduce the number of features in **object detection** models, making them faster without compromising much on accuracy.\n",
    "  \n",
    "#### **Where it‚Äôs Used**:\n",
    "- **Autonomous Driving**: Speeding up the object detection pipeline by reducing the dimensionality of sensor data (e.g., images from cameras).\n",
    "- **Video Surveillance**: Reducing the complexity of object detection in crowded or dynamic scenes to improve performance and detection speed.\n",
    "\n",
    "\n",
    "### **üî• Summary of Dimensionality Reduction in CV**:\n",
    "\n",
    "- **PCA** and **autoencoders** are widely used for **image compression** and **speeding up** deep learning models.\n",
    "- **t-SNE** and **UMAP** are great for **visualizing** high-dimensional image data, especially when working with feature embeddings.\n",
    "- **Dimensionality reduction** helps with **improving model efficiency**, **avoiding overfitting**, and **enhancing the interpretability** of complex datasets.\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "## **3. Feature Extraction (Handcrafted Features vs. Deep Features)**\n",
    "### **Where It‚Äôs Used?**\n",
    "\n",
    "### **Traditional Feature Extraction**\n",
    "‚úÖ **HOG (Histogram of Oriented Gradients)** ‚Äì  \n",
    "   - **Pedestrian Detection** (used in early versions of object detection).  \n",
    "   - **Handwritten Digit Recognition** (captures shape-based features).  \n",
    "‚úÖ **LBP (Local Binary Patterns)** ‚Äì  \n",
    "   - **Face Recognition (LBPH model)** ‚Äì Used in OpenCV face recognition pipelines.  \n",
    "   - **Texture Classification** (helps in fabric defect detection).  \n",
    "‚úÖ **SIFT / SURF / ORB** ‚Äì  \n",
    "   - **Augmented Reality (AR)** (feature matching in real-world applications).  \n",
    "   - **Image Stitching** (used in panorama creation).  \n",
    "\n",
    "### **Deep Learning Feature Extraction**\n",
    "‚úÖ **CNN Feature Maps (ResNet, VGG, EfficientNet, etc.)** ‚Äì  \n",
    "   - **Transfer Learning** (extracts high-level features for new tasks).  \n",
    "   - **Object Recognition & Classification** (used in modern vision models).  \n",
    "‚úÖ **Autoencoder Latent Space** ‚Äì  \n",
    "   - **Anomaly Detection (Industrial Defect Detection, Fraud Detection)**.  \n",
    "   - **Data Compression (learns low-dimensional embeddings for images)**.  \n",
    "\n",
    "\n",
    "\n",
    "## **4. Feature Selection (Choosing the Best Features)**\n",
    "### **Where It‚Äôs Used?**\n",
    "‚úÖ **PCA/t-SNE/UMAP** ‚Äì  \n",
    "   - **Dimensionality Reduction** (used in visualizing large image datasets).  \n",
    "   - **Face Recognition** (Eigenfaces method for efficient feature representation).  \n",
    "‚úÖ **LASSO / RFE on Extracted Features** ‚Äì  \n",
    "   - **Medical Diagnosis (choosing the most relevant MRI/CT scan features).**  \n",
    "   - **Satellite Image Analysis** (selecting the most informative spectral bands).  \n",
    "‚úÖ **Attention Mechanisms (Transformers, CNNs with SE blocks, etc.)** ‚Äì  \n",
    "   - **Vision Transformers (ViTs)** (learn global dependencies between pixels).  \n",
    "   - **Object Detection (YOLO, Faster R-CNN with Attention Layers)** (focus on key regions).  \n",
    "\n",
    "\n",
    "\n",
    "### **üî• Summary**\n",
    "- **Feature scaling & normalization** ‚Üí Used in all deep learning models for stability.  \n",
    "- **Feature transformation** ‚Üí Helps in domain-specific tasks like **medical imaging, AR, remote sensing**.  \n",
    "- **Feature extraction** ‚Üí Used in **object detection, recognition, and AR applications**.  \n",
    "- **Feature selection** ‚Üí Improves efficiency in **medical, industrial, and geospatial tasks**.  \n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance:\n",
    "\n",
    "Yes! **Identifying important features in images** is crucial for many **computer vision tasks**, and there are various ways to extract and define \"important\" features based on the specific problem you're solving. \n",
    "\n",
    "### **What are \"Important Features\" in Images?**\n",
    "\n",
    "**Important features** are the distinctive parts of an image that provide the most **relevant information** for a given task, such as:\n",
    "- **Objects of interest** (e.g., a car in an image for autonomous driving).\n",
    "- **Key points or patterns** (e.g., edges, corners, and textures).\n",
    "- **Regions that distinguish classes** (e.g., a person‚Äôs face in face recognition).\n",
    "\n",
    "These features could be:\n",
    "- **Edges**, **corners**, **textures**, **shapes**, **colors**, or **regions of interest** like specific objects.\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Image Features and Their Importance:**\n",
    "\n",
    "#### **1. Edges**\n",
    "- **Edge Detection (Sobel, Canny, etc.)**:  \n",
    "   - **What it captures**: Boundaries between different regions in an image (useful for object detection).  \n",
    "   - **Where it‚Äôs important**:  \n",
    "     - **Object Detection**: Helps detect object boundaries (e.g., in facial recognition, detecting edges of the face).\n",
    "     - **Segmentation**: Defines regions in an image that can be grouped together.  \n",
    "\n",
    "#### **2. Corners and Key Points**\n",
    "- **Harris Corner Detection / Shi-Tomasi**:  \n",
    "   - **What it captures**: Points where there is a sharp change in direction (useful for object recognition).  \n",
    "   - **Where it‚Äôs important**:  \n",
    "     - **Feature Matching**: Matching corners between two images (e.g., matching features in stereo vision or panorama creation).\n",
    "     - **Tracking Objects**: Detecting points that can be tracked in a video or across images.\n",
    "\n",
    "#### **3. Texture (e.g., LBP, HOG)**\n",
    "- **Local Binary Patterns (LBP)**:  \n",
    "   - **What it captures**: Texture patterns in an image.  \n",
    "   - **Where it‚Äôs important**:  \n",
    "     - **Face Recognition**: Captures local texture features that are unique to each face.\n",
    "     - **Material Classification**: Helps recognize different surfaces (like wood vs. metal).\n",
    "\n",
    "#### **4. Color Features**\n",
    "- **Histograms (e.g., HSV color space)**:  \n",
    "   - **What it captures**: Distribution of colors in the image.  \n",
    "   - **Where it‚Äôs important**:  \n",
    "     - **Object Recognition**: Colors can be distinctive identifiers for objects (e.g., red apples vs. green leaves).\n",
    "     - **Segmentation**: Colors are often used to identify regions or objects.\n",
    "\n",
    "#### **5. Shape and Contours**\n",
    "- **Shape Descriptors (e.g., Hu Moments)**:  \n",
    "   - **What it captures**: The shape of objects in an image, often used to distinguish different objects.  \n",
    "   - **Where it‚Äôs important**:  \n",
    "     - **Object Classification**: Shapes are crucial for recognizing different objects (e.g., recognizing a car or a dog).  \n",
    "     - **Shape-based Retrieval**: Searching for objects based on shape rather than color or texture.\n",
    "\n",
    "#### **6. Deep Features (CNN Extracted Features)**\n",
    "- **Convolutional Neural Networks (CNNs)**:  \n",
    "   - **What it captures**: High-level hierarchical features, such as edges, textures, and object parts.  \n",
    "   - **Where it‚Äôs important**:  \n",
    "     - **Image Classification**: Captures the most relevant features for distinguishing between categories (e.g., distinguishing a cat from a dog).\n",
    "     - **Object Detection**: Helps locate and classify objects within images.\n",
    "     - **Face Recognition**: Captures complex features of faces at multiple levels of abstraction.\n",
    "\n",
    "#### **7. Regions of Interest (ROI)**\n",
    "- **Region Proposal Networks (RPNs) in Faster R-CNN**:  \n",
    "   - **What it captures**: Regions in an image that are most likely to contain objects.  \n",
    "   - **Where it‚Äôs important**:  \n",
    "     - **Object Detection**: Identifies regions in an image where the object is most likely located.\n",
    "     - **Semantic Segmentation**: Helps in isolating meaningful parts of an image.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Find Important Features?**\n",
    "1. **Manual Feature Engineering**:  \n",
    "   - Detect and extract features manually, like **edges, keypoints, and textures**.\n",
    "   - **Example**: Using HOG for capturing the structure of objects in an image.\n",
    "\n",
    "2. **Automated Feature Extraction (Deep Learning)**:  \n",
    "   - Train a **Convolutional Neural Network (CNN)** to automatically learn the most important features during training.  \n",
    "   - **Example**: Using **ResNet** to extract the most important features for object classification.\n",
    "\n",
    "3. **Attention Mechanisms**:  \n",
    "   - In models like **Transformers**, **attention layers** learn which parts of the image are most important for a given task.  \n",
    "   - **Example**: In **Image Captioning**, the attention mechanism helps the model focus on important objects in the image when generating captions.\n",
    "\n",
    "4. **Feature Selection** (Post-Extraction):  \n",
    "   - Once features are extracted, **dimensionality reduction techniques** like **PCA** or **LASSO** can be used to select the most informative features.\n",
    "   - **Example**: Reducing the complexity of an image dataset by selecting only the most important color and texture features.\n",
    "\n",
    "---\n",
    "\n",
    "### **In Summary:**\n",
    "- **Important features** depend on the task (e.g., **edges** for object detection, **textures** for material recognition, **deep features** for classification).\n",
    "- Feature extraction can be **manual** (traditional methods like **HOG, LBP, edges**) or **automatic** (using **CNNs** or **transformers**).\n",
    "- For tasks like **object detection**, **segmentation**, and **classification**, identifying and using the right features can significantly improve model performance.\n",
    "\n",
    "Would you like to dive deeper into **how to extract these features** or **how CNNs automatically learn features**? Let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Autoencoders in Computer Vision**  -- can do anything from features extraction to everything\n",
    "  \n",
    "\n",
    "| **Autoencoder Type**       | **CNN Architectures**         | **Usage/Applications**                     |  \n",
    "|---------------------------|-----------------------------|--------------------------------|  \n",
    "| **Vanilla Autoencoder**   | Simple CNN Encoder-Decoder | Feature extraction, noise removal |  \n",
    "| **Denoising Autoencoder** | CNN, UNet                  | Removes noise from images |  \n",
    "| **Sparse Autoencoder**    | CNN with L1 Regularization | Feature learning, dimensionality reduction |  \n",
    "| **Variational Autoencoder (VAE)** | CNN-based VAE | Image generation, anomaly detection |  \n",
    "| **Convolutional Autoencoder (CAE)** | Deep CNN Encoder-Decoder | Image reconstruction, feature learning |  \n",
    "| **Super-Resolution Autoencoder** | SRCNN, SRGAN | Enhancing image resolution |  \n",
    "| **Anomaly Detection Autoencoder** | CNN-based VAE, CAE | Detecting defects in medical or industrial images |  \n",
    "| **Sequence-to-Sequence Autoencoder** | CNN-LSTM Hybrid | Temporal image processing (e.g., video frames) |  \n",
    "| **3D Convolutional Autoencoder** | 3D CNN | Medical imaging (MRI, CT scan reconstruction) |  \n",
    "| **Adversarial Autoencoder (AAE)** | CNN-GAN Hybrid | Regularized latent space for better representation |  \n",
    "\n",
    "### **Key Takeaways**:  \n",
    "- **Basic Autoencoders** ‚Üí Feature extraction, denoising  \n",
    "- **VAE & AAE** ‚Üí Image generation & anomaly detection  \n",
    "- **CAE & Super-Resolution Autoencoders** ‚Üí Image enhancement  \n",
    "- **3D Autoencoders** ‚Üí Medical & volumetric data  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Autoencoders** are a type of neural network used for unsupervised learning, mainly for **dimensionality reduction, feature learning, and data reconstruction**. They work by encoding an input into a smaller latent representation and then decoding it back to reconstruct the original input.  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. How Autoencoders Work**  \n",
    "Autoencoders consist of two main components:  \n",
    "\n",
    "1. **Encoder**  \n",
    "   - Compresses the input image into a smaller-dimensional latent space.  \n",
    "   - Captures important features while removing noise and redundancy.  \n",
    "   - Example: Converting a **28x28 image to a 32-dimensional vector**.  \n",
    "\n",
    "2. **Decoder**  \n",
    "   - Reconstructs the original image from the latent space representation.  \n",
    "   - Attempts to minimize reconstruction loss (e.g., MSE loss).  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Types of Autoencoders**  \n",
    "\n",
    "#### **A. Vanilla Autoencoder (Basic Autoencoder)**  \n",
    "- Simple **encoder-decoder** structure with fully connected layers.  \n",
    "- Used for basic image compression and reconstruction.  \n",
    "\n",
    "#### **B. Convolutional Autoencoders (CAE)**  \n",
    "- Uses **CNNs** instead of fully connected layers.  \n",
    "- Works well for image-related tasks like **denoising and feature extraction**.  \n",
    "\n",
    "#### **C. Denoising Autoencoder (DAE)**  \n",
    "- Learns to remove noise from corrupted images.  \n",
    "- Takes a **noisy image** as input and reconstructs a clean version.  \n",
    "\n",
    "#### **D. Variational Autoencoder (VAE)**  \n",
    "- Introduces **probabilistic encoding**, learning a smooth latent space distribution.  \n",
    "- Generates **new images similar** to the training data (used in image generation).  \n",
    "\n",
    "#### **E. Sparse Autoencoder**  \n",
    "- Uses a sparsity constraint to learn **important features with fewer neurons**.  \n",
    "- Useful for learning meaningful representations without redundancy.  \n",
    "\n",
    "#### **F. Contractive Autoencoder**  \n",
    "- Adds a regularization term to encourage **robust feature learning**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Applications of Autoencoders in CV**  \n",
    "‚úÖ **Image Denoising** ‚Äì DAE helps remove noise from blurry images.  \n",
    "‚úÖ **Anomaly Detection** ‚Äì Autoencoders detect outliers (e.g., in medical images).  \n",
    "‚úÖ **Dimensionality Reduction** ‚Äì Encoders extract lower-dimensional feature representations.  \n",
    "‚úÖ **Image Generation** ‚Äì VAEs generate new images from latent vectors.  \n",
    "‚úÖ **Super-Resolution** ‚Äì Improve image quality by reconstructing high-resolution images.  \n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handle image imbalance:  \n",
    "\n",
    "1. **Oversampling** ‚Äì Duplicate minority class images to balance the dataset.  \n",
    "2. **Undersampling** ‚Äì Remove images from the majority class to balance the dataset.  \n",
    "3. **Data Augmentation** ‚Äì Apply transformations (rotation, flipping, etc.) to generate more diverse samples.  \n",
    "4. **Synthetic Data Generation (GANs, VAEs)** ‚Äì Use models like GANs or VAEs to create synthetic images.  \n",
    "5. **Class Weighting** ‚Äì Assign higher loss weights to underrepresented classes during training.  \n",
    "6. **Resampling Techniques (SMOTE, ADASYN)** ‚Äì Generate synthetic samples using interpolation.  \n",
    "7. **Transfer Learning** ‚Äì Use pre-trained models that generalize well even with imbalanced data.  \n",
    "8. **Anomaly Detection Approaches** ‚Äì Treat minority class as an anomaly and use detection models.  \n",
    "\n",
    "\n",
    "# Handling Class Imbalance in Image Classification\n",
    "\n",
    "Handling class imbalance in image classification involves techniques to ensure that the model doesn't become biased toward the majority class. Here are common approaches:\n",
    "\n",
    "**1. Data-Level Techniques**\n",
    "\n",
    "   - **Oversampling the Minority Class**: Duplicate or augment images from the minority classes to increase their representation. Data augmentation techniques (e.g., rotation, cropping, flipping) can help create diverse samples for minority classes without introducing exact duplicates.\n",
    "   - **Undersampling the Majority Class**: Randomly reduce the number of samples in majority classes to match the minority class size. This is more feasible with larger datasets, though it risks losing important information.\n",
    "\n",
    "**2. Algorithm-Level Techniques**\n",
    "\n",
    "   - **Class Weights Adjustment**: Many deep learning frameworks allow specifying a weight for each class in the loss function. This penalizes misclassifications of the minority class more than the majority class, encouraging the model to pay more attention to the minority class.\n",
    "   - **Focal Loss**: Focal loss is designed for class imbalance by dynamically scaling the loss for hard-to-classify examples, typically from minority classes. It modifies the cross-entropy loss by adding a scaling factor that reduces the loss for well-classified examples and focuses on hard examples.\n",
    "\n",
    "   $$ \n",
    "   \\text{Focal Loss} = -\\alpha (1 - p_t)^\\gamma \\log(p_t)\n",
    "   $$\n",
    "\n",
    "   where \\( p_t \\) is the predicted probability for the true class, \\( \\alpha \\) is a balancing factor for class imbalance, and \\( \\gamma \\) controls the focus on hard examples.\n",
    "\n",
    "**3. Hybrid and Advanced Techniques**\n",
    "\n",
    "   - **Two-Stage Training**: Train the model first on the original data, then fine-tune with balanced classes or using only the minority class. This approach helps retain information while enhancing sensitivity to minority classes.\n",
    "   - **Synthetic Data Generation**: Use techniques like **Generative Adversarial Networks (GANs)** to generate synthetic images for the minority class. GANs can create realistic, diverse images that augment the dataset.\n",
    "   - **Self-Supervised Learning**: In self-supervised learning, the model learns from unlabeled data, which can later be fine-tuned on a smaller, balanced labeled dataset, improving minority class recognition.\n",
    "\n",
    "**4. Evaluation Adjustments**\n",
    "\n",
    "   - **Metrics Beyond Accuracy**: Use metrics like precision, recall, F1-score, or area under the ROC curve (AUC) to get a more balanced view of performance on imbalanced data, as accuracy can be misleading with class imbalance.\n",
    "   - **Confusion Matrix Analysis**: Reviewing the confusion matrix helps identify if the model is biased toward majority classes, guiding further balancing efforts.\n",
    "\n",
    "Each technique can be combined depending on the severity of imbalance, dataset size, and model complexity, but balancing data effectively often requires experimenting with several methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# While both data augmentation and oversampling aim to improve model performance, they address different challenges in machine learning. Data augmentation enhances dataset diversity, whereas oversampling focuses on correcting class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Techniques in Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Data augmentation is a crucial technique used to artificially expand the size of a training dataset by applying various transformations to the original data. This helps improve the generalization of CNNs and reduces overfitting. Here are some common data augmentation techniques:\n",
    "\n",
    "**1. Geometric Transformations**\n",
    "- **Rotation**: Rotate images by a certain angle.\n",
    "  - Example: Rotate by 15, 30, or 45 degrees.\n",
    "  \n",
    "- **Translation**: Shift images along the x or y axis.\n",
    "  - Example: Shift images by a few pixels left, right, up, or down.\n",
    "\n",
    "- **Scaling**: Zoom in or out on images.\n",
    "  - Example: Scale images to 90% or 110% of their original size.\n",
    "  If the image pixel values are originally in the range of RGB values, typically between 0 and 255, and you scale them to be between -1 and 1, this would effectively change the color intensity scale.\n",
    "  Here's the process:\n",
    "    1. Original RGB Values (0-255):\n",
    "      ‚óã Each pixel in an RGB image has values for Red, Green, and Blue that range from 0 to 255. These values represent the intensity of each color channel.\n",
    "    2. Scaling to [-1, 1]:\n",
    "\n",
    "      ‚óã To scale the values from the range [0, 255] to [-1, 1], you can use the following formula for each color channel (R, G, and B):\n",
    "    $$\\text{scaled\\_value} = \\frac{\\text{original\\_value}}{127.5} - 1$$\n",
    "\n",
    "\n",
    "  This transforms:\n",
    "      ‚óã 0 ‚Üí -1 (black)\n",
    "\n",
    "      ‚óã 255 ‚Üí 1 (white)\n",
    "\n",
    "      ‚óã 127.5 ‚Üí 0 (mid-gray)\n",
    "\n",
    "  Essentially, the value of 0 becomes -1, and 255 becomes 1, with all other values mapped accordingly. This scaling ensures the entire image falls within the range [-1, 1].\n",
    "\n",
    "  What Happens:\n",
    "    ‚Ä¢ Intensities and contrast: The scaling operation changes the contrast and overall intensity of the image. For example, pixel values close to 255 (or the brightest) will become close to 1, and pixels near 0 will become close to -1.\n",
    "\n",
    "    ‚Ä¢ Effect on Machine Learning/Deep Learning: When working with neural networks, normalizing image data to a range of [-1, 1] is a common preprocessing step, as it allows the model to handle input more effectively. This normalization helps with gradient descent optimization by ensuring that the features of the image have a consistent range and prevents issues like vanishing/exploding gradients.\n",
    "\n",
    "  \n",
    "\n",
    "- **Flipping**: Flip images horizontally or vertically.\n",
    "  - Example: Horizontal flips are common for many tasks.\n",
    "\n",
    "**2. Color Space Transformations**\n",
    "- **Brightness Adjustment**: Change the brightness of images.\n",
    "  - Example: Increase or decrease brightness by a fixed factor.\n",
    "\n",
    "- **Contrast Adjustment**: Modify the contrast of images.\n",
    "  - Example: Enhance or reduce the contrast of images.\n",
    "\n",
    "- **Saturation Adjustment**: Alter the saturation levels of images.\n",
    "  - Example: Make images more or less colorful.\n",
    "\n",
    "- **Hue Adjustment**: Shift the hue of colors in images.\n",
    "  - Example: Change colors to see how the model reacts to different color variations.\n",
    "\n",
    "**3. Noise Injection**\n",
    "- **Gaussian Noise**: Add random noise to images to make them more robust.\n",
    "  - Example: Add small Gaussian noise to pixel values.\n",
    "\n",
    "- **Salt-and-Pepper Noise**: Introduce random white and black pixels.\n",
    "  - Example: Randomly set a percentage of pixels to maximum or minimum values.\n",
    "\n",
    "**4. Random Erasing**\n",
    "- **Random Erasing**: Randomly remove sections of an image to make the model learn to focus on different features.\n",
    "  - Example: Select a random rectangle in the image and set it to a constant value or noise.\n",
    "\n",
    "**5. Elastic Transformations**\n",
    "- **Elastic Deformations**: Apply random elastic deformations to images.\n",
    "  - Example: Distort images to create variations while preserving overall structure.\n",
    "\n",
    "**6. Cutout**\n",
    "- **Cutout**: Randomly mask out square regions in images.\n",
    "  - Example: Set square patches in an image to zero or the mean pixel value.\n",
    "\n",
    "**7. Mixup**\n",
    "- **Mixup**: Create new training examples by mixing two images and their corresponding labels.\n",
    "  - Example: For images A and B with labels \\(y_A\\) and \\(y_B\\), create a new image \n",
    "  $$\n",
    "  \\text{Image}_{new} = \\lambda \\cdot \\text{Image}_A + (1 - \\lambda) \\cdot \\text{Image}_B\n",
    "  $$ \n",
    "  where \\( \\lambda \\) is a random value between 0 and 1.\n",
    "\n",
    "**8. Random Cropping**\n",
    "- **Random Cropping**: Randomly crop images to create variations in scale and aspect ratio.\n",
    "  - Example: Crop a random section of the original image.\n",
    "\n",
    "**Conclusion**\n",
    "Data augmentation helps increase the diversity of the training dataset, making CNNs more robust and improving their performance on unseen data. Many deep learning frameworks (like TensorFlow and PyTorch) provide built-in support for these augmentation techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Handling Overfitting**:\n",
    "1. **Data Augmentation**: Increase training data diversity (e.g., rotations, flipping, zoom, shifts, etc.) to expose the model to more variations.\n",
    "2. **Early Stopping**: Stop training when validation performance starts degrading to prevent the model from memorizing the training data.\n",
    "3. **Regularization (L2, Dropout, L1)**: \n",
    "   - **L2 Regularization**: Penalizes large weights (Ridge).\n",
    "   - **Dropout**: Randomly drops units during training to prevent dependency on specific neurons.\n",
    "   - **L1 Regularization**: Encourages sparsity in weights, often leading to simpler models.\n",
    "4. **Cross-Validation**: Split the dataset into multiple folds and train on each to assess model generalization.\n",
    "5. **Reduce Model Complexity**: Use fewer layers or parameters to prevent the model from becoming too complex and overfitting.\n",
    "6. **Transfer Learning**: Fine-tune pre-trained models (from large datasets like ImageNet) to leverage learned features.\n",
    "7. **Batch Normalization**: Helps by normalizing inputs to layers, reducing internal covariate shift, and adding slight regularization.\n",
    "8. **Ensemble Methods**: Combine multiple models to reduce variance and improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### **Handling Underfitting**:\n",
    "1. **Increase Model Complexity**: Use deeper or more complex architectures (more layers, neurons) to capture complex patterns.\n",
    "2. **Improve Feature Extraction**: Use better or more relevant features (e.g., CNN-based feature extraction) or pre-trained models.\n",
    "3. **Increase Training Time**: Train longer to allow the model to learn more complex patterns in the data.\n",
    "4. **Use Non-linear Models**: Employ non-linear activation functions (like ReLU, Leaky ReLU, etc.) instead of linear ones for better capacity to learn complex patterns.\n",
    "5. **Remove Regularization (if too much)**: If using too much regularization, it can make the model too simple and unable to capture the data's complexity.\n",
    "6. **Better Data Quality**: Ensure that your data is clean and rich in detail for better feature learning.\n",
    "7. **Feature Engineering**: Manually engineer more informative features to help the model capture key patterns.\n",
    "8. **Reduce Data Noise**: Clean noisy data that may confuse the model during training.\n",
    "\n",
    " \n",
    "\n",
    "---\n",
    "---\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Shot Learning\n",
    "\n",
    "**One Shot Learning** is a machine learning approach that enables a recognition system to identify or classify objects based on a single example or image. This is particularly challenging in face recognition, where traditionally, deep learning models require large datasets to achieve good performance.\n",
    "\n",
    "**Definition**\n",
    "- **One Shot Learning**: A recognition system can recognize a person by learning from just one image.\n",
    "\n",
    "**Challenges**\n",
    "Historically, deep learning has not performed well when the amount of training data is small. One Shot Learning addresses this challenge by learning a **similarity function** rather than traditional classification.\n",
    "\n",
    "**Similarity Function**\n",
    "To evaluate the similarity between two images, we define a function \\( d \\):\n",
    "$$\n",
    "d(\\text{img1}, \\text{img2}) = \\text{degree of difference between img1 and img2}\n",
    "$$\n",
    "Where:\n",
    "- **img1** and **img2** are the images being compared.\n",
    "- **d** outputs a value representing how similar or different the images are.\n",
    "\n",
    "**Key Points:**\n",
    "- A lower value of \\( d \\) indicates that the images are likely of the same person (i.e., faces are similar).\n",
    "- We introduce a threshold \\( T \\) to make a decision:\n",
    "$$\n",
    "\\text{If } d(\\text{img1}, \\text{img2}) \\leq T \\text{, then the faces are considered the same.}\n",
    "$$\n",
    "\n",
    "**Advantages of One Shot Learning**\n",
    "- **Efficiency**: It allows for effective recognition with minimal training data, which is crucial in scenarios where data collection is limited.\n",
    "- **Robustness**: The similarity function can generalize well to new inputs, making it adaptable to various situations.\n",
    "\n",
    "**Conclusion**\n",
    "One Shot Learning provides a solution to the challenge of recognizing individuals from very limited data. By focusing on learning a similarity function, it allows for effective face recognition even with just a single example image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "Yes, CNNs **do require hyperparameter tuning** to achieve optimal performance. Some key hyperparameters that impact CNN performance include:  \n",
    "\n",
    "### **1. Network Architecture Hyperparameters**  \n",
    "- **Number of Layers** ‚Üí More layers increase capacity but may lead to overfitting.  \n",
    "- **Number of Filters (Channels) per Layer** ‚Üí Affects feature extraction capability.  \n",
    "- **Kernel Size** ‚Üí Larger kernels capture more context but reduce spatial resolution.  \n",
    "- **Pooling Size (e.g., MaxPooling(2x2))** ‚Üí Controls downsampling strength.  \n",
    "\n",
    "### **2. Training Hyperparameters**  \n",
    "- **Learning Rate (`lr`)** ‚Üí Determines how fast weights update (too high = unstable, too low = slow convergence).  \n",
    "- **Batch Size** ‚Üí Affects stability and GPU memory usage (small batches generalize better, large batches train faster).  \n",
    "- **Optimizer Choice** ‚Üí (SGD, Adam, RMSprop, etc.).  \n",
    "- **Regularization (`L1/L2`, Dropout)** ‚Üí Prevents overfitting.  \n",
    "\n",
    "### **3. Data Augmentation & Preprocessing Hyperparameters**  \n",
    "- **Augmentation Strength** (rotation, flipping, brightness adjustments).  \n",
    "- **Normalization Strategy** (Mean/Std normalization, Min-Max scaling).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Does PyTorch Lightning Handle Hyperparameter Tuning?**\n",
    "PyTorch Lightning **makes training easier** but does **not automatically tune hyperparameters**. However, it **integrates well** with hyperparameter tuning frameworks like:  \n",
    "- **Optuna** ‚Üí Auto-optimizes learning rates, dropout rates, etc.  \n",
    "- **Ray Tune** ‚Üí Scales tuning across multiple GPUs.  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here‚Äôs a brief explanation of **Rank-1** and **Rank-5 accuracy** without code:\n",
    "\n",
    "### **Rank-1 Accuracy**:\n",
    "- **Definition**: The **Rank-1 accuracy** measures how often the **top predicted class** is exactly the same as the true class label.\n",
    "- **Use Case**: Commonly used in classification tasks (e.g., image classification), where the model is expected to provide the most likely class label. If the top predicted class matches the true label, it's considered a correct prediction.\n",
    "\n",
    "### **Rank-5 Accuracy**:\n",
    "- **Definition**: The **Rank-5 accuracy** measures how often the true class label is found among the **top-5 predicted classes**. This metric is especially useful in cases where the model might not get the top-1 prediction right but still has the correct class within the top-5 predictions.\n",
    "- **Use Case**: Often used in large-scale classification problems (e.g., ImageNet) where there are many possible classes. Even if the model's top prediction is incorrect, if the correct label is within the top-5 predictions, it counts as a correct result.\n",
    "\n",
    "### **Summary**:\n",
    "- **Rank-1 Accuracy**: Focuses on the model‚Äôs ability to predict the correct class as its top choice.\n",
    "- **Rank-5 Accuracy**: Evaluates if the correct class is among the model's top 5 predictions.\n",
    "\n",
    "These metrics are used to understand how well a model is performing in terms of classification accuracy, especially in multi-class problems where there are many possible classes to choose from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **evaluation metrics** used for **computer vision (CV)** :\n",
    "\n",
    "\n",
    "## **1. Image Classification**\n",
    "- **Accuracy**: Percentage of correctly classified images out of total images.\n",
    "- **Precision**: The proportion of true positives out of all predicted positives (how many predicted positive labels are correct).\n",
    "- **Recall (Sensitivity)**: The proportion of true positives out of all actual positives (how many actual positive labels were detected).\n",
    "- **F1-Score**: Harmonic mean of Precision and Recall, balancing the two.\n",
    "- **Confusion Matrix**: A table showing the number of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "\n",
    "\n",
    "## **2. Object Detection**\n",
    "- **Mean Average Precision (mAP)**: Average precision across all classes and IoU thresholds, a standard for object detection tasks.\n",
    "- **Intersection over Union (IoU)**: Measures the overlap between the predicted bounding box and the ground truth bounding box.\n",
    "- **Precision at K (P@K)**: Measures how many true positives are in the top K predictions.\n",
    "- **Recall at K (R@K)**: Measures how many ground truths are covered by the top K predictions.\n",
    "\n",
    "\n",
    "## **3. Image Segmentation**\n",
    "- **Dice Coefficient**: Measures overlap between predicted and true masks, similar to IoU.\n",
    "- **IoU (Intersection over Union)**: Measures the ratio of overlap between predicted and true masks divided by their union.\n",
    "- **Pixel Accuracy**: Proportion of correctly classified pixels.\n",
    "- **Mean Pixel Accuracy**: Average accuracy per class.\n",
    "- **Boundary F1-Score**: Measures the quality of object boundaries between predicted and true segmentations.\n",
    "\n",
    "\n",
    "## **4. Image Retrieval**\n",
    "- **Mean Average Precision (mAP)**: Average precision across all queries in a retrieval system.\n",
    "- **Recall at K**: Measures the number of relevant images retrieved in the top K search results.\n",
    "- **Precision at K**: Measures the proportion of retrieved images that are relevant.\n",
    "\n",
    "\n",
    "## **5. Face Recognition**\n",
    "- **True Positive Rate (TPR)**: Proportion of correctly recognized faces.\n",
    "- **False Positive Rate (FPR)**: Proportion of incorrectly identified faces.\n",
    "- **Equal Error Rate (EER)**: The point at which the False Accept Rate (FAR) equals the False Reject Rate (FRR).\n",
    "\n",
    "\n",
    "\n",
    "## **6. Generative Models (e.g., GANs)**\n",
    "- **Inception Score (IS)**: Evaluates the quality of generated images based on the Inception model.\n",
    "- **Frechet Inception Distance (FID)**: Measures the distance between distributions of generated and real images, with lower values indicating better quality.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Loss\n",
    "\n",
    "Triplet loss is a loss function commonly used in deep learning, particularly in tasks involving similarity learning, such as face recognition and image retrieval. It aims to ensure that the distance between an anchor sample and a positive sample (similar) is smaller than the distance between the anchor sample and a negative sample (dissimilar) by a predefined margin. \n",
    "\n",
    "**Definition**\n",
    "- Given three inputs: an anchor $x_a$, a positive sample $x_p$ (similar to the anchor), and a negative sample $x_n$ (dissimilar to the anchor), the triplet loss can be defined as:\n",
    "\n",
    "$$\n",
    "L(x_a, x_p, x_n) = \\max(0, d(x_a, x_p) - d(x_a, x_n) + \\alpha)\n",
    "$$\n",
    "\n",
    "where:\n",
    "-  $d(x_i, x_j)$ is a distance metric (e.g., Euclidean distance) between samples $x_i$ and $x_j$,\n",
    "-  $\\alpha$ is the margin that is enforced between positive and negative pairs.\n",
    "\n",
    "**Importance for CNNs**\n",
    "- **Learning Discriminative Features**: Triplet loss helps CNNs learn embeddings that are well-separated for different classes while bringing similar classes closer together in the feature space. This is particularly useful in applications where distinguishing between classes is challenging.\n",
    "- **Robustness to Variations**: It provides a robust mechanism for the model to learn invariant features despite variations in pose, lighting, or other conditions, making it suitable for real-world applications.\n",
    "\n",
    "**Applications of Triplet Loss**\n",
    "1. **Face Recognition**: In face recognition systems, triplet loss can be used to ensure that images of the same person are close in the embedding space, while images of different people are far apart.\n",
    "2. **Image Retrieval**: For systems that retrieve images based on similarity, triplet loss helps improve the ranking of images based on user queries.\n",
    "3. **Object Tracking**: In object tracking, triplet loss can help to distinguish the target object from background clutter or other objects.\n",
    "4. **Speaker Verification**: In audio processing, triplet loss can be applied to ensure that recordings of the same speaker are closer together than recordings from different speakers.\n",
    "\n",
    "By applying triplet loss in CNNs, models can achieve higher accuracy and robustness in distinguishing between classes based on learned embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### EDA Questions for CV (Image Data)\n",
    "\n",
    "1. What are the common dimensions of the images (width, height)?\n",
    "2. How does the aspect ratio vary across the dataset?\n",
    "3. What is the color distribution across images?\n",
    "4. Are there differences in brightness or contrast among the images?\n",
    "5. What is the edge distribution in the images (sharp vs. smooth regions)?\n",
    "6. What common textures or patterns are present in different image categories?\n",
    "7. Is there a class imbalance in the number of images per category?\n",
    "8. What are the most common objects detected in the images?\n",
    "9. Are there patterns in metadata, such as capture date, location, or resolution?\n",
    "10. Do images have similarities in background, lighting, or occlusion within classes? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Timeline of Key Video Models\n",
    "\n",
    "- 2014: C3D - 3D convolutions for video classification.\n",
    "- 2015: Two-Stream Networks - Combines CNNs for spatial and optical flow for temporal features.\n",
    "- 2015: LSTMs - RNNs for modeling temporal dependencies.\n",
    "- 2016: Convolutional LSTM - Hybrid of CNNs and LSTMs.\n",
    "- 2017: Temporal Convolutional Networks (TCNs) - Uses convolutions for temporal modeling.\n",
    "- 2018: I3D - Inflated 3D ConvNets for better temporal feature learning.\n",
    "- 2019: SlowFast Networks - Two-branch model for capturing spatial and motion information.\n",
    "- 2020: Vision Transformers (ViTs) - Transformer-based models for video.\n",
    "- 2021: TimeSformer - Attention-based video processing model.\n",
    "- 2021: Video GPT - Transformer for video generation.\n",
    "- 2022: Self-supervised Learning - Video models trained without labeled data.\n",
    "\n",
    "Training on **videos** involves a few additional complexities compared to images because videos have both **temporal** and **spatial** components. Here's how you can approach training on videos in computer vision:\n",
    "\n",
    "\n",
    " **1. Data Representation**\n",
    "- **Frames Extraction**: Videos are typically represented as a sequence of frames (images). You break a video into individual frames (images) and process them like you would process image data.\n",
    "- **Optical Flow**: Instead of just looking at frames individually, optical flow captures motion information between frames, useful for tasks involving motion analysis.\n",
    "\n",
    "\n",
    "\n",
    "**2. Temporal Information**\n",
    "Videos contain **temporal dependencies** (time-based relationships between frames). These dependencies need to be captured to understand the context over time. Common approaches:\n",
    "\n",
    " **Recurrent Neural Networks (RNNs)**\n",
    "- **LSTMs (Long Short-Term Memory)** or **GRUs (Gated Recurrent Units)**: These networks are designed to capture **temporal sequences**, helping the model remember previous frames and recognize patterns across time.\n",
    "- **Bi-directional RNNs**: Capture temporal dependencies in both directions (future and past).\n",
    "\n",
    " **3D Convolutional Networks (3D CNNs)**\n",
    "- **3D Convolutions**: Instead of applying 2D convolutions to individual frames, **3D convolutions** operate on 3D data (spatial + temporal), allowing the model to capture motion and changes over time.\n",
    "  - **Example**: C3D (Convolutional 3D) networks or I3D (Inflated 3D ConvNets) that extend 2D filters into 3D filters.\n",
    "  \n",
    " **Two-Stream Networks**\n",
    "- **Spatial Stream**: Uses 2D CNNs for individual frame analysis (like image classification).\n",
    "- **Temporal Stream**: Uses motion data (e.g., optical flow) to understand the temporal component.\n",
    "- These networks combine the two streams to handle both spatial and temporal information.\n",
    "\n",
    "\n",
    "\n",
    " **3. Data Augmentation for Videos**\n",
    "- **Frame-level augmentation**: Similar to images (e.g., rotation, flipping, scaling).\n",
    "- **Temporal augmentation**: Random cropping or sampling of frames from different points in the video to simulate different lengths and timings.\n",
    "- **Motion-based augmentation**: Simulate varying motions or distortions in the video to make the model more robust.\n",
    "\n",
    "\n",
    " **4. Training Strategies**\n",
    " **Supervised Learning**:  \n",
    "- For **video classification** or **action recognition**, the model learns from labeled video data, where each video is classified into a specific category.\n",
    "  \n",
    " **Unsupervised Learning**:  \n",
    "- In tasks like **video clustering**, unsupervised methods can learn temporal patterns or motions without explicit labels.\n",
    "\n",
    " **Semi-Supervised and Self-Supervised Learning**:\n",
    "- **Temporal Contrastive Learning** or **video representation learning** methods can help the model learn from unlabeled video data, focusing on relationships between frames.\n",
    "\n",
    "\n",
    "\n",
    "**5. Transfer Learning on Videos**\n",
    "- **Pretrained 2D Networks**: You can still use pretrained image networks (like ResNet or VGG) for individual frame feature extraction and then combine them with RNNs or 3D CNNs for temporal understanding.\n",
    "  \n",
    "- **Pretrained 3D Models**: Models like **C3D**, **I3D**, or **SlowFast Networks** (developed for video tasks) are pretrained on large video datasets, and fine-tuning these models for your task can provide better results than training from scratch.\n",
    "\n",
    "\n",
    "\n",
    " **Common Video Tasks**\n",
    "- **Video Classification**: Classifying the entire video (e.g., action recognition, event detection).\n",
    "- **Object Tracking**: Tracking an object across frames (e.g., in sports videos, surveillance).\n",
    "- **Activity Recognition**: Recognizing complex actions or behaviors in the video (e.g., gesture recognition, human actions).\n",
    "- **Video Segmentation**: Segmenting different parts of a video (e.g., foreground-background segmentation, human segmentation).\n",
    "\n",
    "\n",
    "\n",
    " **Summary**\n",
    "- **Frames** are extracted from videos to treat each frame as an image, but models need to capture **temporal relationships** between frames.\n",
    "- Common models used are **RNNs** (for sequential learning), **3D CNNs** (for spatial + temporal analysis), and **Two-Stream Networks**.\n",
    "- Data augmentation for videos includes frame-level and motion-based techniques, while training strategies can be supervised, unsupervised, or self-supervised.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical Example:\n",
    "red object before...after some brightness is more\n",
    "Let‚Äôs compare how RGB and HSV react to changes in lighting:\n",
    "\n",
    "1. RGB Before and After Lighting Change:\n",
    "Before: A bright red object has RGB values (255, 0, 0).\n",
    "After: Under dim lighting, the same red object might have RGB values like (150, 0, 0), where the object is still perceived as red but the RGB values have changed drastically.\n",
    "2. HSV Before and After Lighting Change:\n",
    "Before: The object is a bright red, so its HSV values might be (0¬∞, 255, 255).\n",
    "After: Under dim lighting, the object‚Äôs HSV values might change to (0¬∞, 100, 100), but the Hue (H) remains the same (0¬∞), indicating that it is still red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how **dlib** detects faces and facial features like lips and nose:\n",
    "\n",
    "### 1. **Face Detection in dlib**:\n",
    "   - **HOG-based or CNN-based detectors** are used for detecting faces.\n",
    "   - The detector works by scanning the image for faces using a sliding window approach.\n",
    "   - A **classifier** (typically an SVM) is applied to classify each region as a face or not.\n",
    "   - It outputs **bounding boxes** around detected faces.\n",
    "\n",
    "### 2. **Facial Landmark Detection**:\n",
    "   - After detecting the face, **dlib's shape predictor** (e.g., `shape_predictor_68_face_landmarks.dat`) detects specific **facial landmarks**.\n",
    "   - The model identifies **68 key points** on the face, including eyes, eyebrows, nose, lips, and jawline.\n",
    "\n",
    "### 3. **Marking Facial Features**:\n",
    "   - **Lips**: Points 48-67.\n",
    "   - **Nose**: Points 27-35.\n",
    "   - Each landmark point corresponds to a specific location on the face (e.g., corners of the lips, tip of the nose).\n",
    "   - These points are marked by drawing small circles or used for further facial analysis.\n",
    "\n",
    "### 4. **Applications**:\n",
    "   - **Emotion recognition** based on lip and facial expression analysis.\n",
    "   - **Face alignment** for improving face recognition accuracy.\n",
    "   - **Augmented Reality (AR)** for overlaying virtual makeup or other features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here‚Äôs an updated list with multiple useful functions for each topic:\n",
    "\n",
    "### 1. Getting Started with OpenCV  \n",
    "**Definition:** OpenCV is an open-source computer vision library that provides tools for image and video processing.  \n",
    "```python\n",
    "import cv2  # Import OpenCV\n",
    "image = cv2.imread(\"image.jpg\")  # Read an image\n",
    "cv2.imshow(\"Image\", image)  # Display an image\n",
    "cv2.imwrite(\"output.jpg\", image)  # Save an image\n",
    "cv2.waitKey(0)  # Wait for a key press\n",
    "```  \n",
    "\n",
    "### 2. Grey-scaling Images  \n",
    "**Definition:** Converting a color image to grayscale reduces it to a single intensity channel.  \n",
    "```python\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "cv2.imshow(\"Gray Image\", gray)  # Show grayscale image\n",
    "gray_inverted = cv2.bitwise_not(gray)  # Invert grayscale image\n",
    "blurred_gray = cv2.GaussianBlur(gray, (5,5), 0)  # Apply Gaussian blur\n",
    "cv2.waitKey(0)\n",
    "```  \n",
    "\n",
    "### 3. Color Spaces (HSV & RGB)  \n",
    "**Definition:** Changing color representations between RGB, HSV, and other formats.  \n",
    "```python\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # Convert to HSV\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "h, s, v = cv2.split(hsv)  # Split into Hue, Saturation, and Value channels\n",
    "image_hue = cv2.merge([h, s, v])  # Reconstruct HSV image\n",
    "cv2.imshow(\"HSV Image\", hsv)  # Show HSV image\n",
    "```  \n",
    "\n",
    "### 4. Drawing on Images  \n",
    "**Definition:** OpenCV allows drawing shapes like lines, circles, and rectangles on images.  \n",
    "```python\n",
    "cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Draw a line\n",
    "cv2.rectangle(image, (50, 50), (200, 200), (0, 255, 0), 3)  # Draw a rectangle\n",
    "cv2.circle(image, (center_x, center_y), radius, (0, 0, 255), 3)  # Draw a circle\n",
    "cv2.putText(image, \"Hello\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)  # Write text\n",
    "cv2.imshow(\"Drawings\", image)  # Show image with drawings\n",
    "```  \n",
    "\n",
    "### 5. Transformations - Translations and Rotations  \n",
    "**Definition:** Moving or rotating an image using affine transformations.  \n",
    "```python\n",
    "M = cv2.getRotationMatrix2D((center_x, center_y), angle, scale)  # Get rotation matrix\n",
    "rotated = cv2.warpAffine(image, M, (width, height))  # Apply rotation\n",
    "M_translation = np.float32([[1, 0, 100], [0, 1, 50]])  # Define translation matrix\n",
    "translated = cv2.warpAffine(image, M_translation, (width, height))  # Apply translation\n",
    "cv2.imshow(\"Rotated\", rotated)  # Show rotated image\n",
    "```  \n",
    "\n",
    "### 6. Scaling, Resizing, and Cropping  \n",
    "**Definition:** Changing image size and extracting a region of interest.  \n",
    "```python\n",
    "resized = cv2.resize(image, (width, height))  # Resize image\n",
    "cropped = image[50:200, 50:200]  # Crop region of interest\n",
    "scaled = cv2.resize(image, None, fx=0.5, fy=0.5)  # Scale image by 50%\n",
    "aspect_ratio_resized = cv2.resize(image, (width, int(height * 0.5)))  # Maintain aspect ratio\n",
    "cv2.imshow(\"Cropped Image\", cropped)  # Show cropped image\n",
    "```  \n",
    "\n",
    "### 7. Arithmetic and Bitwise Operations  \n",
    "**Definition:** Performing pixel-wise operations like addition, subtraction, AND, OR, XOR.  \n",
    "```python\n",
    "result_add = cv2.add(image1, image2)  # Add two images\n",
    "result_sub = cv2.subtract(image1, image2)  # Subtract image2 from image1\n",
    "result_and = cv2.bitwise_and(image1, image2)  # Bitwise AND\n",
    "result_or = cv2.bitwise_or(image1, image2)  # Bitwise OR\n",
    "result_xor = cv2.bitwise_xor(image1, image2)  # Bitwise XOR\n",
    "```  \n",
    "\n",
    "### 8. Convolutions, Blurring, and Sharpening  \n",
    "**Definition:** Applying filters to smooth or sharpen images using kernels.  \n",
    "```python\n",
    "blurred = cv2.GaussianBlur(image, (5,5), 0)  # Apply Gaussian blur\n",
    "sharpened = cv2.filter2D(image, -1, kernel_sharpen)  # Apply sharpening filter\n",
    "median_blurred = cv2.medianBlur(image, 5)  # Apply median blur\n",
    "bilateral_blurred = cv2.bilateralFilter(image, 9, 75, 75)  # Apply bilateral filter\n",
    "cv2.imshow(\"Blurred Image\", blurred)  # Show blurred image\n",
    "```  \n",
    "\n",
    "### 9. Thresholding & Binarization  \n",
    "**Definition:** Converting an image to binary using a threshold.  \n",
    "```python\n",
    "_, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)  # Apply binary threshold\n",
    "_, binary_inv = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)  # Inverted binary threshold\n",
    "adaptive_thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)  # Adaptive threshold\n",
    "otsu_thresh, otsu_binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)  # Otsu's thresholding\n",
    "cv2.imshow(\"Thresholded\", binary)  # Show thresholded image\n",
    "```  \n",
    "\n",
    "### 10. Dilation, Erosion, and Edge Detection  \n",
    "**Definition:** Morphological operations to enhance or remove image features.  \n",
    "```python\n",
    "dilated = cv2.dilate(binary, None, iterations=2)  # Apply dilation\n",
    "eroded = cv2.erode(binary, None, iterations=2)  # Apply erosion\n",
    "edges = cv2.Canny(image, 100, 200)  # Apply Canny edge detection\n",
    "grad_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)  # Apply Sobel edge detection in X direction\n",
    "grad_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)  # Apply Sobel edge detection in Y direction\n",
    "```  \n",
    "\n",
    "### 11. Contours - Drawing, Hierarchy, and Modes  \n",
    "**Definition:** Finding and drawing contours (outlines) of objects in an image.  \n",
    "```python\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)  # Find contours\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 3)  # Draw contours\n",
    "cv2.drawContours(image, contours, 2, (0, 0, 255), 2)  # Draw specific contour\n",
    "cv2.imshow(\"Contours\", image)  # Show image with contours\n",
    "```  \n",
    "\n",
    "### 12. Moments, Matching, and Sorting Contours  \n",
    "**Definition:** Extracting shape features, matching contours, and sorting them.  \n",
    "```python\n",
    "M = cv2.moments(contour)  # Get moments of a contour\n",
    "cx = int(M['m10'] / M['m00'])  # Find centroid X\n",
    "cy = int(M['m01'] / M['m00'])  # Find centroid Y\n",
    "sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)  # Sort contours by area\n",
    "cv2.drawContours(image, sorted_contours, 0, (255, 0, 0), 2)  # Draw sorted contours\n",
    "cv2.imshow(\"Sorted Contours\", image)  # Show sorted contours\n",
    "```  \n",
    "\n",
    "### 13. Line, Circle, Blob Detection  \n",
    "**Definition:** Detecting geometric shapes in an image.  \n",
    "```python\n",
    "circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 20)  # Detect circles\n",
    "cv2.circle(image, (x, y), radius, (0, 255, 0), 4)  # Draw detected circle\n",
    "lines = cv2.HoughLinesP(image, 1, np.pi / 180, 100, minLineLength=50, maxLineGap=10)  # Detect lines\n",
    "cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Draw detected line\n",
    "```  \n",
    "\n",
    "### 14. Counting Circles, Ellipses, and Finding Waldo  \n",
    "**Definition:** Identifying circular and elliptical objects using contour properties.  \n",
    "```python\n",
    "ellipse = cv2.fitEllipse(contour)  # Fit ellipse to contour\n",
    "cv2.ellipse(image, ellipse, (0, 255, 0), 2)  # Draw ellipse\n",
    "circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 20)  # Detect circles\n",
    "cv2.circle(image, (x, y), radius, (255, 0, 0), 2)  # Draw circle\n",
    "```  \n",
    "\n",
    "### 15. Finding Corners  \n",
    "**Definition:** Detecting corners using Harris or Shi-Tomasi corner detection.  \n",
    "```python\n",
    "corners = cv2.goodFeaturesToTrack(gray, 100, 0.01, 10)  # Shi-Tomasi corner detection\n",
    "for corner in corners:\n",
    "    x, y = corner.ravel()  # Get corner coordinates\n",
    "    cv2.circle(image, (x, y), 3, 255, -1)  # Draw corner\n",
    "cv2.imshow(\"Corners\", image)  # Show image with corners\n",
    "```  \n",
    "\n",
    "### 16. Face and Eye Detection with HAAR Cascade Classifiers  \n",
    "**Definition:** Detecting faces and eyes using pre-trained HAAR cascades.  \n",
    "```python\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)  # Detect faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x\n",
    "\n",
    " + w, y + h), (255, 0, 0), 2)  # Draw face bounding box\n",
    "eyes = eye_cascade.detectMultiScale(gray)  # Detect eyes\n",
    "for (ex, ey, ew, eh) in eyes:\n",
    "    cv2.rectangle(image, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)  # Draw eye bounding box\n",
    "cv2.imshow(\"Face and Eye Detection\", image)  # Show image with detections\n",
    "```  \n",
    "\n",
    "### 17. Vehicle & Pedestrian Detection  \n",
    "**Definition:** Identifying cars and people in images using pre-trained classifiers.  \n",
    "```python\n",
    "pedestrians = hog.detectMultiScale(image, winStride=(4, 4))  # Detect pedestrians\n",
    "vehicles = car_cascade.detectMultiScale(gray, 1.1, 3)  # Detect vehicles\n",
    "```  \n",
    "\n",
    "### 18. Perspective Transforms  \n",
    "**Definition:** Adjusting the perspective of an image using four points.  \n",
    "```python\n",
    "warped = cv2.warpPerspective(image, M, (width, height))  # Apply perspective transform\n",
    "```  \n",
    "\n",
    "### 19. Histograms and K-means Clustering for Finding Dominant Colors  \n",
    "**Definition:** Analyzing pixel distributions and clustering colors.  \n",
    "```python\n",
    "hist = cv2.calcHist([image], [0], None, [256], [0,256])  # Calculate histogram\n",
    "```  \n",
    "\n",
    "### 20. Comparing Images with MSE and Structural Similarity  \n",
    "**Definition:** Measuring the similarity between two images.  \n",
    "```python\n",
    "score, diff = ssim(image1, image2, full=True)  # Compute SSIM\n",
    "```  \n",
    "- **MSE** measures pixel-level differences, penalizing small changes even if they're not perceptually significant.\n",
    "- **MSE** gives a more direct numerical error but lacks human-like interpretation.\n",
    "- **SSIM** evaluates structural, luminance, and contrast changes, aligning more with human perception.\n",
    "- **SSIM** reflects perceptual similarity, making it more reliable for visual quality assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the requested topics, expanded with useful functions for each one:\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.transform import resize\n",
    "from pyzbar.pyzbar import decode\n",
    "import pytesseract\n",
    "import easyocr\n",
    "import time\n",
    "\n",
    "# 21. Filtering Colors  \n",
    "lower_blue = np.array([100, 50, 50])  # Define lower bound for blue color\n",
    "upper_blue = np.array([140, 255, 255])  # Define upper bound for blue color\n",
    "mask = cv2.inRange(hsv, lower_blue, upper_blue)  # Filter blue color\n",
    "filtered = cv2.bitwise_and(image, image, mask=mask)  # Apply filter\n",
    "result = cv2.bitwise_not(mask)  # Invert the mask\n",
    "cv2.imshow('Filtered', filtered)  # Display filtered result\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 22. Watershed Algorithm marker-based image segmentation  \n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  \n",
    "_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)  # Binary inverse threshold\n",
    "dist_transform = cv2.distanceTransform(thresh, cv2.DIST_L2, 5)  # Compute distance transform\n",
    "_, markers = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)  # Marker image\n",
    "markers = np.int32(markers)  # Convert to int32\n",
    "cv2.watershed(image, markers)  # Apply watershed\n",
    "image[markers == -1] = [0, 0, 255]  # Mark boundaries\n",
    "cv2.imshow('Watershed Segmentation', image)  # Display segmented image\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 23. Background and Foreground Subtraction  \n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()  # Background subtractor\n",
    "fgmask = fgbg.apply(image)  # Get foreground mask\n",
    "cv2.imshow('Foreground Mask', fgmask)  # Show the mask\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 24. Motion tracking using Mean Shift and CAM-Shift-The Mean Shift algorithm iterates and adjusts the position of the search window until the window is centered on the region with the highest likelihood of matching the object's features.\n",
    "roi = (200, 200, 100, 100)  # Define region of interest (ROI)\n",
    "hsv_roi = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # Convert ROI to HSV\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0, 180])  # Compute histogram\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)  # Normalize histogram\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)  # Termination criteria\n",
    "track_window = roi  # Initial tracking window\n",
    "ret, frame = video_capture.read()  # Read video frame\n",
    "hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)  # Convert to HSV\n",
    "dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)  # Back projection\n",
    "ret, track_window = cv2.CamShift(dst, track_window, term_crit)  # Apply CAM-Shift\n",
    "cv2.rectangle(frame, (track_window[0], track_window[1]), \n",
    "              (track_window[0] + track_window[2], track_window[1] + track_window[3]), \n",
    "              (0, 0, 255), 2)  # Draw rectangle around tracked object\n",
    "cv2.imshow('Tracking', frame)  # Display frame with tracking\n",
    "cv2.waitKey(1)\n",
    "\n",
    "# 25. Optical Flow Object Tracking  \n",
    "old_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)  # Feature parameters\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)  # Detect features\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))  \n",
    "p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, gray, p0, None, **lk_params)  # Calculate optical flow\n",
    "cv2.line(image, (p0[0][0], p0[0][1]), (p1[0][0], p1[0][1]), (0, 255, 0), 2)  # Draw line for tracking\n",
    "cv2.imshow('Optical Flow', image)  # Display optical flow result\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 26. Simple Object Tracking by Colour  \n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # Convert to HSV\n",
    "lower_color = np.array([30, 150, 50])  # Lower bound for color tracking\n",
    "upper_color = np.array([85, 255, 255])  # Upper bound for color tracking\n",
    "mask = cv2.inRange(hsv, lower_color, upper_color)  # Mask for tracking color\n",
    "res = cv2.bitwise_and(image, image, mask=mask)  # Apply mask to original image\n",
    "cv2.imshow('Color Tracking', res)  # Show tracked object\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 27. Facial Landmarks Detection with Dlib  \n",
    "detector = dlib.get_frontal_face_detector()  # Face detector\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Landmark predictor\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "faces = detector(gray)  # Detect faces\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)  # Get landmarks\n",
    "    for n in range(0, 68):\n",
    "        x, y = landmarks.part(n).x, landmarks.part(n).y  # Get coordinates\n",
    "        cv2.circle(image, (x, y), 1, (0, 255, 0), -1)  # Mark landmarks\n",
    "cv2.imshow('Facial Landmarks', image)  # Display image with landmarks\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 28. Face Swapping with Dlib  \n",
    "image1 = cv2.imread(\"face1.jpg\")  # Load first image\n",
    "image2 = cv2.imread(\"face2.jpg\")  # Load second image\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)  # Convert first image to grayscale\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)  # Convert second image to grayscale\n",
    "faces1 = detector(gray1)  # Detect faces in image1\n",
    "faces2 = detector(gray2)  # Detect faces in image2\n",
    "landmarks1 = predictor(gray1, faces1[0])  # Get landmarks from image1\n",
    "landmarks2 = predictor(gray2, faces2[0])  # Get landmarks from image2\n",
    "# Example swapping steps would include facial feature matching and blending (omitted for simplicity)\n",
    "\n",
    "# 29. Tilt Shift Effect  \n",
    "def tilt_shift(image):\n",
    "    mask = np.zeros_like(image)  # Create blank mask\n",
    "    mask[150:350, 150:350] = 255  # Define region for tilt-shift\n",
    "    blurred = cv2.GaussianBlur(image, (15, 15), 0)  # Apply blur\n",
    "    result = cv2.addWeighted(image, 0.7, blurred, 0.3, 0)  # Combine images\n",
    "    return result\n",
    "\n",
    "tilt_shifted = tilt_shift(image)  # Apply tilt-shift effect\n",
    "cv2.imshow('Tilt Shift', tilt_shifted)  # Display result\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 30. Grabcut Algorithm for Background Removal  \n",
    "mask = np.zeros(image.shape[:2], np.uint8)  # Create mask\n",
    "bgd_model = np.zeros((1, 65), np.float64)  # Background model\n",
    "fgd_model = np.zeros((1, 65), np.float64)  # Foreground model\n",
    "rect = (50, 50, 450, 290)  # Define rectangle for grabcut\n",
    "cv2.grabCut(image, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)  # Apply grabcut\n",
    "mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')  # Mask out background\n",
    "grabcut_result = image * mask2[:, :, np.newaxis]  # Apply mask to image\n",
    "cv2.imshow('Grabcut Result', grabcut_result)  # Display result\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 31. OCR with PyTesseract and EasyOCR  \n",
    "text_pytesseract = pytesseract.image_to_string(image)  # OCR with pytesseract\n",
    "reader = easyocr.Reader(['en'])  # Initialize EasyOCR reader\n",
    "text_easyocr = reader.readtext(image)  # OCR with EasyOCR\n",
    "print(\"PyTesseract Text:\", text_pytesseract)\n",
    "print(\"EasyOCR Text:\", text_easyocr)\n",
    "\n",
    "# 32. Barcode and QR generation and reading  \n",
    "# Generate Barcode\n",
    "barcode = cv2.barcode_BarcodeDetector()\n",
    "# Read QR code\n",
    "decoded_objects = decode(image)  # QR decoding\n",
    "for obj in decoded_objects:\n",
    "    print(f\"Data: {obj.data.decode('utf-8')}\")\n",
    "    print(f\"Type: {obj.type}\")\n",
    "\n",
    "# 33. YOLOv3 in OpenCV  \n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")  # Load YOLOv3 model\n",
    "layer_names = net.getLayerNames()  # Get layer names\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getLayers()]  # Get output layers\n",
    "blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)  # Prepare image for YOLO\n",
    "net.setInput(blob)  # Set input\n",
    "outs = net.forward(output_layers)  # Run forward\n",
    "\n",
    " pass\n",
    "\n",
    "# 34. Neural Style Transfer with OpenCV  \n",
    "# Apply neural style transfer using pre-trained model (omitted for simplicity)\n",
    "\n",
    "# 35. SSDs in OpenCV  \n",
    "net_ssd = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"res10_300x300_ssd_iter_140000.caffemodel\")  # Load SSD\n",
    "# Apply SSD object detection on image\n",
    "\n",
    "# 36. Colorise Black and White Photos  \n",
    "# Neural network approach for colorizing black and white photos\n",
    "\n",
    "# 37. Repair Damaged Photos with Inpainting  \n",
    "damaged_image = cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)  # Inpainting\n",
    "cv2.imshow('Inpainting Result', damaged_image)  # Display inpainting result\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 38. Add and remove Noise, Fix Contrast with Histogram Equalisation  \n",
    "noise_img = cv2.randn(image.copy(), (0, 0, 0), (20, 20, 20))  # Add noise\n",
    "contrast_img = cv2.equalizeHist(image)  # Histogram equalization\n",
    "\n",
    "# 39. Detect Blur in Images  \n",
    "blurred = cv2.Laplacian(image, cv2.CV_64F).var()  # Detect blur\n",
    "if blurred < 100:\n",
    "    print(\"Image is blurry\")\n",
    "else:\n",
    "    print(\"Image is clear\")\n",
    "\n",
    "# 40. Facial Recognition  \n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()  # Initialize face recognizer\n",
    "```\n",
    "\n",
    "These are common approaches you can take for each topic. Let me know if you'd like a more detailed breakdown of any particular one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTORCH\n",
    "\n",
    "In computer vision tasks, PyTorch provides a range of **functions and utilities** that are commonly used across various stages of the **Exploratory Data Analysis (EDA)** and **model evaluation** pipelines. Below is a list of the most commonly used functions in each phase:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Data Loading and Augmentation**\n",
    "- **`torchvision.datasets`**  \n",
    "  - **`torchvision.datasets.ImageFolder`**: Loads datasets from directories with a folder structure.\n",
    "  - **`torchvision.datasets.CIFAR10`, `CIFAR100`, `MNIST`, etc.**: Provides popular benchmark datasets for testing models.\n",
    "\n",
    "- **`torchvision.transforms`**  \n",
    "  - **`transforms.ToTensor()`**: Converts a PIL image or NumPy array to a PyTorch tensor.\n",
    "  - **`transforms.Normalize(mean, std)`**: Normalizes image tensors using the mean and standard deviation.\n",
    "  - **`transforms.Resize(size)`**: Resizes image to a given size.\n",
    "  - **`transforms.RandomHorizontalFlip()`**: Randomly flips the image horizontally for augmentation.\n",
    "  - **`transforms.RandomRotation(degrees)`**: Randomly rotates the image for augmentation.\n",
    "  - **`transforms.ColorJitter(brightness, contrast, saturation, hue)`**: Augments image colors by adjusting brightness, contrast, saturation, or hue.\n",
    "  - **`transforms.RandomAffine(degrees, translate, scale, shear)`**: Applies affine transformations like scaling and rotation.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Data Preparation (Preprocessing)**\n",
    "- **`torch.utils.data.DataLoader`**  \n",
    "  - **`DataLoader()`**: A class that combines a dataset and a sampler to load data in batches efficiently.\n",
    "  - **`shuffle=True`**: Shuffles the data each epoch to avoid model bias.\n",
    "  - **`batch_size`**: Defines the number of samples per batch.\n",
    "\n",
    "- **`torchvision.transforms.Compose`**  \n",
    "  - **`transforms.Compose()`**: Chains multiple transformation operations together for seamless preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Model Construction (CNNs, RNNs, etc.)**\n",
    "- **`torch.nn.Module`**  \n",
    "  - **`nn.Conv2d(in_channels, out_channels, kernel_size)`**: Defines a 2D convolution layer.\n",
    "  - **`nn.MaxPool2d(kernel_size)`**: Defines a max-pooling layer.\n",
    "  - **`nn.ReLU()`**: Applies the ReLU activation function.\n",
    "  - **`nn.Linear(in_features, out_features)`**: Defines a fully connected layer.\n",
    "  - **`nn.BatchNorm2d(num_features)`**: Applies batch normalization to a 2D input.\n",
    "\n",
    "- **`torch.nn.Sequential`**  \n",
    "  - **`nn.Sequential()`**: A container for stacking layers in order, simplifying the model architecture.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Training (Loss Calculation and Optimizer)**\n",
    "- **`torch.optim`**  \n",
    "  - **`optim.SGD(params, lr)`**: Implements the stochastic gradient descent optimizer.\n",
    "  - **`optim.Adam(params, lr)`**: Implements the Adam optimizer (commonly used for CNNs).\n",
    "  - **`optim.lr_scheduler.StepLR(optimizer, step_size, gamma)`**: A learning rate scheduler that reduces learning rate by a factor every `step_size` epochs.\n",
    "\n",
    "- **Loss Functions**\n",
    "  - **`nn.CrossEntropyLoss()`**: Used for classification tasks.\n",
    "  - **`nn.BCEWithLogitsLoss()`**: Binary classification or multi-label classification loss.\n",
    "  - **`nn.MSELoss()`**: Mean squared error loss for regression tasks.\n",
    "  - **`nn.SmoothL1Loss()`**: Often used in tasks like object detection.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Training Loop**\n",
    "- **`torch.no_grad()`**  \n",
    "  - **`torch.no_grad()`**: Used during inference to disable gradient calculation and save memory.\n",
    "\n",
    "- **`model.train()`** and **`model.eval()`**  \n",
    "  - **`model.train()`**: Puts the model in training mode (affects layers like Dropout, BatchNorm).\n",
    "  - **`model.eval()`**: Puts the model in evaluation mode, ensuring proper behavior during inference.\n",
    "\n",
    "- **`optimizer.zero_grad()`**  \n",
    "  - Clears the gradients before each optimization step.\n",
    "\n",
    "- **`loss.backward()`**  \n",
    "  - Computes gradients for each parameter.\n",
    "\n",
    "- **`optimizer.step()`**  \n",
    "  - Updates the model parameters based on computed gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Evaluation and Metrics**\n",
    "- **Accuracy Calculation**\n",
    "  - **`torch.max()`**: To get the predicted class label from the model‚Äôs output:  \n",
    "    ```python\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    ```\n",
    "\n",
    "- **`sklearn.metrics`** (for external metrics)  \n",
    "  - **`accuracy_score(y_true, y_pred)`**: Accuracy score for classification.\n",
    "  - **`confusion_matrix(y_true, y_pred)`**: Creates confusion matrix.\n",
    "  - **`classification_report(y_true, y_pred)`**: Detailed precision, recall, F1-score report.\n",
    "\n",
    "- **Mean IoU (Intersection over Union) for segmentation**  \n",
    "  - **`torch.sum()`**: For calculating pixel-level intersection and union for IoU metric.\n",
    "\n",
    "- **AP (Average Precision) for Object Detection**  \n",
    "  - Commonly used libraries like **`torchmetrics`** provide AP calculation utilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Visualization (For EDA & Debugging)**\n",
    "- **`matplotlib` and `PIL`**  \n",
    "  - **`matplotlib.pyplot.imshow()`**: To visualize images.\n",
    "  - **`PIL.Image.open()`**: To load and display images.\n",
    "  \n",
    "- **`torchvision.utils.make_grid()`**  \n",
    "  - Converts a batch of images into a grid for easy visualization.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Saving and Loading Models**\n",
    "- **`torch.save()`**  \n",
    "  - **`torch.save(model.state_dict(), 'model.pth')`**: Saves model weights.\n",
    "  - **`torch.save(model, 'model_complete.pth')`**: Saves the entire model (architecture + weights).\n",
    "\n",
    "- **`torch.load()`**  \n",
    "  - **`model.load_state_dict(torch.load('model.pth'))`**: Loads model weights into a model architecture.\n",
    "\n",
    "- **`torchvision.models`**  \n",
    "  - **`models.resnet50(pretrained=True)`**: Load a pre-trained ResNet50 model.\n",
    "  - **`models.vgg16(pretrained=True)`**: Load a pre-trained VGG16 model.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Transfer Learning**\n",
    "- **`model.fc = nn.Linear(in_features, num_classes)`**  \n",
    "  - Replaces the final classification layer with a new one for transfer learning.\n",
    "\n",
    "- **`model.eval()` and `model.train()`**  \n",
    "  - Switch between evaluation and training mode when fine-tuning pre-trained models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Key PyTorch Functions for CV EDA and Evaluation:**\n",
    "- **Data Loading**: `torchvision.datasets`, `torch.utils.data.DataLoader`, `transforms.Compose()`.\n",
    "- **Model Building**: `torch.nn.Conv2d`, `torch.nn.MaxPool2d`, `torch.nn.Linear`, `torch.nn.BatchNorm2d`.\n",
    "- **Training**: `torch.optim.SGD`, `torch.optim.Adam`, `loss.backward()`, `optimizer.step()`, `optimizer.zero_grad()`.\n",
    "- **Evaluation**: `accuracy_score`, `confusion_matrix`, `classification_report`, `torch.max()`.\n",
    "- **Visualization**: `matplotlib.pyplot.imshow()`, `torchvision.utils.make_grid()`.\n",
    "- **Saving/Loading Models**: `torch.save()`, `torch.load()`, `model.load_state_dict()`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the **methods** and **functions** commonly used for plotting images and generating videos in computer vision with PyTorch and related libraries:\n",
    "\n",
    "\n",
    "### **1. Plotting Images** (Matplotlib + PyTorch)\n",
    "\n",
    "- **Matplotlib Functions:**\n",
    "  - `matplotlib.pyplot.imshow()` - Display an image.\n",
    "  - `matplotlib.pyplot.axis()` - Set axis properties (e.g., `axis('off')` to hide axis).\n",
    "  - `matplotlib.pyplot.show()` - Render the plot.\n",
    "\n",
    "- **TorchVision Functions:**\n",
    "  - `torchvision.utils.make_grid()` - Create a grid of images from a batch.\n",
    "  \n",
    "\n",
    "\n",
    "### **2. Generating and Saving Videos** (OpenCV + imageio)\n",
    "\n",
    "- **OpenCV Functions:**\n",
    "  - `cv2.VideoWriter()` - Initialize video writer for saving videos.\n",
    "  - `cv2.VideoWriter_fourcc()` - Specify codec for video file format.\n",
    "  - `cv2.imwrite()` - Save a single image frame.\n",
    "  - `cv2.VideoCapture()` - Open a video file for frame-by-frame processing.\n",
    "  - `cv2.imshow()` - Display a video frame in a window.\n",
    "  - `cv2.waitKey()` - Wait for a key event (for frame-by-frame display).\n",
    "\n",
    "- **imageio Functions:**\n",
    "  - `imageio.get_writer()` - Initialize video writer.\n",
    "  - `imageio.append_data()` - Add a frame to the video.\n",
    "  - `imageio.imread()` - Read image data (can be used for frame extraction).\n",
    "  \n",
    "\n",
    "\n",
    "### **3. Displaying and Manipulating Videos** (OpenCV)\n",
    "\n",
    "- **OpenCV Functions:**\n",
    "  - `cv2.VideoCapture()` - Open a video file for reading frames.\n",
    "  - `cv2.waitKey()` - Control video playback speed (frame interval).\n",
    "  - `cv2.destroyAllWindows()` - Close all OpenCV windows.\n",
    "\n",
    "\n",
    "\n",
    "### **4. Annotating Videos** (OpenCV)\n",
    "\n",
    "- **OpenCV Functions:**\n",
    "  - `cv2.rectangle()` - Draw a rectangle on a frame (e.g., for bounding boxes).\n",
    "  - `cv2.circle()` - Draw a circle (e.g., for keypoints).\n",
    "  - `cv2.putText()` - Add text annotations to frames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS-TENSORFLOW\n",
    "\n",
    "### **1. Data Loading and Augmentation**\n",
    "- **`tensorflow.keras.preprocessing.image`**  \n",
    "  - **`ImageDataGenerator()`**: Generates batches of image data with real-time augmentation.\n",
    "  - **`flow_from_directory()`**: Loads image data from a directory with folder structure.\n",
    "\n",
    "- **`tensorflow.keras.preprocessing.image.ImageDataGenerator`**  \n",
    "  - **`rescale=1./255`**: Scales pixel values to [0, 1] by dividing by 255.\n",
    "  - **`rotation_range`, `width_shift_range`, `height_shift_range`**: Various augmentation parameters like rotation and shift.\n",
    "  - **`horizontal_flip`**: Randomly flips the image horizontally for augmentation.\n",
    "  - **`zoom_range`**: Zoom in or out during augmentation.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Data Preparation (Preprocessing)**\n",
    "- **`tensorflow.keras.preprocessing.image.load_img()`**  \n",
    "  - **`load_img(path, target_size=(width, height))`**: Loads an image and resizes it to the target size.\n",
    "  \n",
    "- **`tensorflow.keras.preprocessing.image.img_to_array()`**  \n",
    "  - **`img_to_array(img)`**: Converts a PIL image to a NumPy array.\n",
    "\n",
    "- **`tensorflow.keras.applications`**  \n",
    "  - **`preprocess_input()`**: Preprocesses the input for a specific pre-trained model (e.g., VGG16, ResNet50).\n",
    "\n",
    "\n",
    "\n",
    "### **3. Model Construction (CNNs, RNNs, etc.)**\n",
    "- **`tensorflow.keras.models.Sequential`**  \n",
    "  - **`Sequential()`**: A linear stack of layers for model construction.\n",
    "  \n",
    "- **`tensorflow.keras.layers`**  \n",
    "  - **`Conv2D(filters, kernel_size)`**: Defines a 2D convolution layer.\n",
    "  - **`MaxPooling2D(pool_size)`**: Defines a max-pooling layer.\n",
    "  - **`Dense(units)`**: Defines a fully connected layer.\n",
    "  - **`Flatten()`**: Flattens the input for dense layer connection.\n",
    "  - **`Dropout(rate)`**: Adds dropout regularization to prevent overfitting.\n",
    "  - **`BatchNormalization()`**: Normalizes activations for each mini-batch.\n",
    "  - **`GlobalAveragePooling2D()`**: Global average pooling layer for feature aggregation.\n",
    "\n",
    "- **`tensorflow.keras.layers.Activation`**  \n",
    "  - **`Activation('relu')`**: Applies ReLU activation function.\n",
    "  - **`Activation('softmax')`**: Applies Softmax activation for multi-class classification.\n",
    "\n",
    "\n",
    "\n",
    "### **4. Training (Loss Calculation and Optimizer)**\n",
    "- **`tensorflow.keras.optimizers`**  \n",
    "  - **`Adam(learning_rate)`**: Adam optimizer (commonly used for CNNs).\n",
    "  - **`SGD(learning_rate)`**: Stochastic Gradient Descent optimizer.\n",
    "  - **`RMSprop(learning_rate)`**: RMSProp optimizer for training.\n",
    "\n",
    "- **Loss Functions**  \n",
    "  - **`categorical_crossentropy`**: Used for multi-class classification tasks.\n",
    "  - **`binary_crossentropy`**: Used for binary classification tasks.\n",
    "  - **`mean_squared_error`**: Used for regression tasks.\n",
    "  \n",
    "\n",
    "\n",
    "### **5. Training Loop**\n",
    "- **`model.fit()`**  \n",
    "  - **`model.fit(x_train, y_train, epochs=10, batch_size=32)`**: Trains the model for a fixed number of epochs.\n",
    "  \n",
    "- **`model.evaluate()`**  \n",
    "  - **`model.evaluate(x_test, y_test)`**: Evaluates the model performance on a test set.\n",
    "  \n",
    "- **`model.predict()`**  \n",
    "  - **`model.predict(x_input)`**: Makes predictions using the trained model.\n",
    "\n",
    "- **Callbacks**\n",
    "  - **`tensorflow.keras.callbacks.EarlyStopping(patience=3)`**: Stops training early when the model stops improving.\n",
    "  - **`tensorflow.keras.callbacks.ModelCheckpoint()`**: Saves the best model during training based on validation performance.\n",
    "\n",
    "\n",
    "\n",
    "### **6. Evaluation and Metrics**\n",
    "- **Accuracy Calculation**\n",
    "  - **`tensorflow.keras.metrics.Accuracy()`**: Computes accuracy metric during training and evaluation.\n",
    "  - **`confusion_matrix()`**: Creates a confusion matrix.\n",
    "  - **`classification_report()`**: Provides a detailed precision, recall, and F1-score report.\n",
    "\n",
    "- **Mean IoU (Intersection over Union) for segmentation**  \n",
    "  - **`tensorflow.keras.metrics.MeanIoU(num_classes)`**: Computes the mean intersection over union for segmentation tasks.\n",
    "\n",
    "- **AP (Average Precision) for Object Detection**  \n",
    "  - **`tensorflow.keras.metrics.AveragePrecision()`**: Computes the average precision for object detection tasks.\n",
    "\n",
    "\n",
    "\n",
    "### **7. Visualization (For EDA & Debugging)**\n",
    "- **`matplotlib.pyplot.imshow()`**  \n",
    "  - **`imshow()`**: To visualize images.\n",
    "  \n",
    "- **`tensorflow.keras.preprocessing.image.array_to_img()`**  \n",
    "  - **`array_to_img(array)`**: Converts a NumPy array back to a PIL image.\n",
    "\n",
    "- **`model.summary()`**  \n",
    "  - **`summary()`**: Prints the model architecture summary (layer names, output shapes, parameters).\n",
    "\n",
    " \n",
    "\n",
    "### **8. Saving and Loading Models**\n",
    "- **`model.save()`**  \n",
    "  - **`model.save('model.h5')`**: Saves the entire model (architecture + weights).\n",
    "  \n",
    "- **`tensorflow.keras.models.load_model()`**  \n",
    "  - **`load_model('model.h5')`**: Loads a saved model from a file.\n",
    "\n",
    " \n",
    "\n",
    "### **9. Transfer Learning**\n",
    "- **`tensorflow.keras.applications`**  \n",
    "  - **`VGG16(weights='imagenet')`**: Loads a pre-trained VGG16 model with ImageNet weights.\n",
    "  - **`ResNet50(weights='imagenet')`**: Loads a pre-trained ResNet50 model with ImageNet weights.\n",
    "\n",
    "- **Fine-tuning**\n",
    "  - **`model.layers`**: Access layers for fine-tuning (e.g., freeze initial layers and train later layers).\n",
    "\n",
    " \n",
    "\n",
    "### **Summary of Key Keras Functions for CV EDA and Evaluation:**\n",
    "- **Data Loading**: `ImageDataGenerator()`, `flow_from_directory()`, `load_img()`, `img_to_array()`.\n",
    "- **Model Building**: `Conv2D`, `MaxPooling2D`, `Dense`, `Dropout`, `Activation('relu')`, `Sequential()`.\n",
    "- **Training**: `Adam()`, `SGD()`, `fit()`, `evaluate()`, `predict()`, `EarlyStopping()`, `ModelCheckpoint()`.\n",
    "- **Evaluation**: `confusion_matrix()`, `classification_report()`, `Accuracy()`, `MeanIoU()`.\n",
    "- **Visualization**: `imshow()`, `array_to_img()`, `summary()`.\n",
    "- **Saving/Loading Models**: `model.save()`, `load_model()`.\n",
    "- **Transfer Learning**: `VGG16()`, `ResNet50()`, `model.layers`.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "Here‚Äôs a breakdown of these topics:  \n",
    "\n",
    "---\n",
    "\n",
    "## **Keras: LeNet and AlexNet**  \n",
    "### **LeNet (LeNet-5) in Keras**\n",
    "- Designed by **Yann LeCun** for digit classification (e.g., MNIST).  \n",
    "- Architecture:  \n",
    "  - **2 Convolutional Layers** (with activation functions like tanh/ReLU).  \n",
    "  - **2 Subsampling (Pooling) Layers**.  \n",
    "  - **Fully Connected Layers** leading to softmax output.  \n",
    "\n",
    "### **AlexNet in Keras**\n",
    "- Developed by **Alex Krizhevsky** (2012) for ImageNet classification.  \n",
    "- Architecture:  \n",
    "  - **5 Convolutional Layers** with ReLU activation.  \n",
    "  - **Max Pooling Layers**.  \n",
    "  - **3 Fully Connected Layers**, with the last one using softmax for classification.  \n",
    "  - **Dropout Regularization** to prevent overfitting.  \n",
    "  - **Local Response Normalization (LRN)** (not commonly used today).  \n",
    "\n",
    "---\n",
    "\n",
    "## **PyTorch Pretrained Networks**\n",
    "PyTorch provides several **pre-trained models** via `torchvision.models`. These are trained on **ImageNet** and can be used for feature extraction or fine-tuning:\n",
    "- **Common Models in PyTorch**:  \n",
    "  - **ResNet** (e.g., `resnet50`, `resnet101`)  \n",
    "  - **VGG** (e.g., `vgg16`, `vgg19`)  \n",
    "  - **DenseNet** (`densenet121`)  \n",
    "  - **Inception v3** (`inception_v3`)  \n",
    "  - **EfficientNet** (`efficientnet_b0`)  \n",
    "  - **MobileNet** (`mobilenet_v2`, `mobilenet_v3`)  \n",
    "\n",
    "- **Loading a Pretrained Model**:\n",
    "  - `model = torchvision.models.resnet50(pretrained=True)`  \n",
    "  - Modify the **fully connected (FC) layer** for custom classification tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Keras Pretrained Networks**\n",
    "Keras provides a variety of **pretrained models** in `tensorflow.keras.applications`:\n",
    "- **Common Pretrained Models**:  \n",
    "  - **VGG** (`VGG16`, `VGG19`)  \n",
    "  - **ResNet** (`ResNet50`, `ResNet101`, `ResNet152`)  \n",
    "  - **Inception** (`InceptionV3`)  \n",
    "  - **Xception** (better version of Inception)  \n",
    "  - **EfficientNet** (`EfficientNetB0`, `EfficientNetB7`)  \n",
    "  - **MobileNet** (`MobileNetV2`, `MobileNetV3`)  \n",
    "\n",
    "- **Loading a Pretrained Model**:\n",
    "  - `model = tensorflow.keras.applications.ResNet50(weights=\"imagenet\")`  \n",
    "  - Modify the **last layers** for custom tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Top-1 and Top-5 Accuracies**\n",
    "- **Top-1 Accuracy**: The model's highest probability prediction must match the correct class.  \n",
    "- **Top-5 Accuracy**: The correct class should be among the top 5 highest probability predictions.  \n",
    "\n",
    "- **Example (ImageNet)**:\n",
    "  - **ResNet-50 Top-1 Accuracy ‚âà 76%**  \n",
    "  - **ResNet-50 Top-5 Accuracy ‚âà 93%**  \n",
    "  - **AlexNet Top-1 Accuracy ‚âà 57%**  \n",
    "  - **AlexNet Top-5 Accuracy ‚âà 80%**  \n",
    "\n",
    "---\n",
    "\n",
    "## **PyTorch Rank-N Accuracy**\n",
    "- **Rank-N Accuracy**: Measures how often the correct class appears in the top N predictions.  \n",
    "- **PyTorch Implementation**:  \n",
    "  - Uses `torch.topk()` to extract top-N predictions.  \n",
    "  - Example: `torch.topk(output, 5, dim=1)` for **Top-5 accuracy**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Keras Rank-N Accuracy**\n",
    "- **Keras handles Rank-N accuracy via `top_k_categorical_accuracy()`**.  \n",
    "- Example: `tensorflow.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=5)` for **Top-5 accuracy**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Callbacks with PyTorch**\n",
    "PyTorch **does not have built-in callbacks** like Keras, but similar functionality can be implemented using:\n",
    "- **Early Stopping** (custom loop with patience-based stopping).  \n",
    "- **Learning Rate Scheduler** (`torch.optim.lr_scheduler.StepLR`).  \n",
    "- **Model Checkpointing** (`torch.save()`).  \n",
    "- **Logging** (using `tensorboardX` or `wandb`).  \n",
    "\n",
    "---\n",
    "\n",
    "## **Callbacks with Keras**\n",
    "Keras provides **built-in callbacks** for easy training control:\n",
    "- **`EarlyStopping(patience=3)`** ‚Üí Stops training when validation loss stops improving.  \n",
    "- **`ModelCheckpoint(filepath, save_best_only=True)`** ‚Üí Saves the best model.  \n",
    "- **`ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)`** ‚Üí Reduces learning rate if validation loss stagnates.  \n",
    "- **`TensorBoard(log_dir='logs/')`** ‚Üí Logs training metrics for visualization.  \n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the content condensed into a single table:\n",
    "\n",
    "| **Feature**                   | **Autoencoders (AEs)**                                                                                   | **Generative Adversarial Networks (GANs)**                                               | **Fine-Tuning vs Transfer Learning**                                                     |\n",
    "|-------------------------------|---------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| **Goal**                       | Learn to **reconstruct** input data.                                                                    | Generate **new, realistic** images from noise.                                           | **Transfer Learning**: Use a **pretrained model** for a new task without modifying most layers. **Fine-Tuning**: **Train** some or all layers of a **pretrained model** for better adaptation. |\n",
    "| **Architecture**               | Encoder-Decoder Network                                                                                 | Generator-Discriminator Network                                                           | **Transfer Learning**: Mostly **freeze** layers and reuse pretrained weights. **Fine-Tuning**: **Unfreeze** some layers and adjust them for the new task.                  |\n",
    "| **Working Principle**          | Compress input into a latent space and then reconstruct it.                                              | Generator creates fake images, Discriminator distinguishes real from fake.               | **Transfer Learning**: Use **pretrained features** directly. **Fine-Tuning**: Adjust pretrained model for specific tasks.   |\n",
    "| **Loss Function**              | Mean Squared Error (MSE) or Binary Cross-Entropy.                                                      | Adversarial loss (Binary Cross-Entropy for Discriminator & special losses for Generator).| **Transfer Learning**: Lower computational cost, retains pretrained features. **Fine-Tuning**: Requires **more data** to avoid overfitting. |\n",
    "| **Output Quality**             | Often blurry and lacks diversity.                                                                      | Sharp, high-quality, and diverse images.                                                  | **Transfer Learning**: Works well with small datasets. **Fine-Tuning**: Effective when **new dataset is large** and differs from original dataset. |\n",
    "| **Use Cases**                  | Feature extraction, anomaly detection, image denoising, dimensionality reduction.                      | Image synthesis, deepfake generation, style transfer, super-resolution.                  | **Fine-Tuning**: Customizes pretrained models for **specific tasks** like medical imaging or self-driving cars. |\n",
    "| **Diversity of Output**        | Limited ‚Äì primarily reconstructs what it has seen.                                                      | High ‚Äì can generate completely new and diverse images.                                    | **Fine-Tuning**: Adapts pretrained models to **new, specific data**.                        |\n",
    "| **Training Stability**         | Easier to train, requires only one network.                                                            | Difficult to train, suffers from mode collapse and instability.                          | **Transfer Learning**: Easier, fewer changes. **Fine-Tuning**: More complex, with higher computational cost. |\n",
    "| **Data Requirement**           | Can work with limited data.                                                                             | Requires a large dataset for good results.                                               | **Transfer Learning**: Works with a **small dataset**. **Fine-Tuning**: Requires **more data** to avoid overfitting. |\n",
    "| **Computational Cost**         | Lower ‚Äì requires only an encoder-decoder.                                                               | Higher ‚Äì requires adversarial training between two networks.                             | **Transfer Learning**: **Lower** computational cost due to frozen layers. **Fine-Tuning**: **Higher** computational cost since layers are updated. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### **Deep Dream** \n",
    "\n",
    "is a technique developed by Google that uses convolutional neural networks (CNNs) to modify and enhance images in surreal, dream-like ways. It works by feeding an input image through a pre-trained CNN model and then maximizing the activation of specific neurons within the network using **gradient ascent**. The idea is to amplify the features that the network has learned to recognize, which often leads to visually striking and abstract images. The **loss function** in Deep Dream is designed to maximize the activations in certain layers of the network, with the result being an image where the features the network detects (e.g., edges, textures, shapes) become exaggerated and more pronounced. This process is repeated iteratively, gradually transforming the image into something that appears more \"dream-like\" or \"hallucinatory.\" It‚Äôs often used for **visualizing the inner workings of neural networks**, creating **artistic images**, and **enhancing certain features** in a manner that reflects the network's learned representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Siamese Networks** \n",
    "\n",
    "are a type of neural network architecture that consists of two or more identical subnetworks that share the same parameters. These networks are typically used for **similarity learning** tasks, where the goal is to determine how similar two input samples are. \n",
    "\n",
    "Here's a brief overview of what Siamese Networks can do:\n",
    "\n",
    "1. **Image Similarity**: Determine if two images are similar or not (e.g., face verification, signature verification).\n",
    "2. **One-shot Learning**: Identify an object or class from a single example (e.g., recognizing a face in a new image with only one training example).\n",
    "3. **Metric Learning**: Learn a similarity or distance metric for comparing samples.\n",
    "4. **Anomaly Detection**: Identify whether a sample is anomalous or not based on learned similarity metrics.\n",
    "5. **Object Tracking**: Track an object in consecutive frames based on similarity metrics learned from the network.\n",
    "\n",
    "Siamese networks work by calculating a similarity score between two inputs using a distance function (like Euclidean distance) after passing them through the shared subnetwork. If the outputs are similar, it means the two inputs are likely to belong to the same class or have high similarity.\n",
    "\n",
    "**Algorithm working:**\n",
    "\n",
    "1. **`create_embedding_network(input_shape)`**: Define the shared neural network to extract embeddings from inputs (e.g., Conv2D, Flatten, Dense layers).\n",
    "2. **`euclidean_distance(vects)`**: Compute the Euclidean distance between the two embeddings to measure similarity.\n",
    "3. **`create_siamese_network(input_shape)`**: Create a Siamese network by defining two input layers, applying the shared embedding network, and calculating the distance between the embeddings.\n",
    "4. **`compile()`**: Compile the Siamese network with a loss function (e.g., binary cross-entropy) and an optimizer (e.g., Adam).\n",
    "5. **`fit()`**: Train the model on pairs of images with corresponding similarity labels (1 for similar, 0 for different).\n",
    "6. **`predict()`**: Use the trained model to predict if two input images are similar based on their embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Big Transfer (BiT)** is a **large-scale transfer learning approach** introduced by Google Brain, where models pretrained on extremely large datasets (like JFT-300M) are fine-tuned for new tasks. It uses **ResNet-based architectures** with minimal modifications and benefits from **scaling up both model size and dataset size** for better generalization. BiT achieves **state-of-the-art performance** on various vision benchmarks with minimal task-specific tuning. üöÄ\n",
    "\n",
    "- **Vision Transformer (ViT)** is a deep learning model that applies **transformers** to image recognition tasks. Instead of using CNNs, ViT **splits an image into patches**, flattens them, and processes them like tokens in NLP using **self-attention**. It achieves **state-of-the-art results** on large datasets (like ImageNet) but requires **more data** and **pretraining** to outperform CNNs. üöÄ\n",
    "\n",
    "- **Depth Estimation** is the process of predicting the distance of objects from the camera in an image or video. It is used in **3D reconstruction, autonomous driving, augmented reality (AR), and robotics**. Techniques include:  \n",
    "\n",
    "    **Stereo Vision** (using two cameras to calculate disparity).  \n",
    "    **Monocular Depth Estimation** (predicting depth from a single image using deep learning).  \n",
    "    **LiDAR-based Depth Estimation** (using laser sensors for precise depth maps).  \n",
    "    **Structure from Motion (SfM)** (inferring depth from multiple images taken from different angles).  \n",
    "\n",
    "Deep learning models like **MiDaS, DPT, and NeRF** have improved depth estimation accuracy significantly. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point Cloud Segmentation** is the process of classifying or clustering individual points in a **3D point cloud** into meaningful regions or objects. It is widely used in **autonomous driving (LiDAR), robotics, AR/VR, and medical imaging**.  \n",
    "\n",
    "### **Types of Point Cloud Segmentation**  \n",
    "1. **Semantic Segmentation** ‚Äì Classifies each point into categories (e.g., road, car, pedestrian).  \n",
    "2. **Instance Segmentation** ‚Äì Identifies individual objects in the scene (e.g., separate different cars).  \n",
    "3. **Panoptic Segmentation** ‚Äì Combines both semantic and instance segmentation for a complete understanding.  \n",
    "\n",
    "### **Methods Used**  \n",
    "- **Traditional Approaches** ‚Äì K-Means Clustering, Region Growing, RANSAC.  \n",
    "- **Deep Learning-Based Approaches** ‚Äì PointNet, PointNet++, RandLA-Net, KPConv, and Transformer-based models.  \n",
    "\n",
    "üöÄ **Applications** include self-driving cars (object detection from LiDAR), 3D mapping, and AR scene understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DeepSORT (Deep Simple Online and Realtime Tracker)** is an advanced multi-object tracking (MOT) algorithm that builds upon **SORT (Simple Online and Realtime Tracker)** by integrating deep learning-based appearance features for improved tracking accuracy.  \n",
    "\n",
    "### **Key Features of DeepSORT**  \n",
    "1. **Kalman Filter** ‚Äì Predicts object locations in subsequent frames based on motion.  \n",
    "2. **Hungarian Algorithm** ‚Äì Matches detections to existing tracked objects efficiently.  \n",
    "3. **Appearance Embeddings** ‚Äì Uses a **deep learning-based** feature extractor (e.g., CNN or ReID network) to improve tracking robustness in case of occlusions.  \n",
    "4. **IoU and Mahalanobis Distance** ‚Äì Helps associate detections with existing tracks using spatial information.  \n",
    "5. **Re-Identification (ReID)** ‚Äì Allows tracking objects even if they temporarily disappear from view.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
