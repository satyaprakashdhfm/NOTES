{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f144ffe",
   "metadata": {},
   "source": [
    "\n",
    "## **PromptTemplate**\n",
    "\n",
    "* A core LangChain abstraction for structuring prompts.\n",
    "* Lets you define a **template string** with placeholders (like `{product}`), then fill them dynamically at runtime.\n",
    "* Useful because LLMs respond better to structured, consistent prompts instead of messy string concatenation.\n",
    "* Example:\n",
    "\n",
    "  ```python\n",
    "  from langchain.prompts import PromptTemplate\n",
    "\n",
    "  prompt = PromptTemplate(\n",
    "      input_variables=[\"product\"],\n",
    "      template=\"Write a tagline for {product}.\"\n",
    "  )\n",
    "  print(prompt.format(product=\"smartwatch\"))\n",
    "  ```\n",
    "* In real systems, PromptTemplate is critical for **standardizing prompts across pipelines**â€”for chatbots, knowledge assistants, or content generation tools.\n",
    "\n",
    "---\n",
    "\n",
    "## **Runnable (LCEL) vs LangGraph**\n",
    "\n",
    "1. **Runnable / LCEL (LangChain Expression Language)**\n",
    "\n",
    "   * LCEL makes LangChain components composable, like building blocks in a pipeline.\n",
    "   * Each step is a **Runnable** (prompt â†’ model â†’ parser).\n",
    "   * Encourages functional chaining: data flows through transforms.\n",
    "   * Example: `prompt | llm | parser`\n",
    "   * Ideal when you want **linear flows**â€”like passing user query â†’ LLM â†’ JSON output.\n",
    "   * Industry use: building **ETL-like LLM workflows** (parse documents, enrich, then output summaries).\n",
    "\n",
    "2. **LangGraph**\n",
    "\n",
    "   * Built on top of LCEL but for **graphs instead of chains**.\n",
    "   * Lets you design **state machines / DAGs (directed acyclic graphs)** where control flow depends on conditions.\n",
    "   * Can implement loops, branching, memory, retries.\n",
    "   * Example: Customer support bot where path depends on sentiment: positive â†’ FAQ answer; negative â†’ escalate to human.\n",
    "   * Industry use: **multi-agent orchestration** or complex **decision trees with LLMs**.\n",
    "\n",
    "---\n",
    "\n",
    "Simple metaphor:\n",
    "\n",
    "* **LCEL** = a train track, straight line, predictable stops.\n",
    "* **LangGraph** = a metro map, multiple routes, branches, loops, and junctions.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4247f57a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "- Langserver\n",
    "- Langsmith\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabe764",
   "metadata": {},
   "source": [
    "## Memory: \n",
    "## sessions(message History):\n",
    "## hub & agent executor:\n",
    "## tools (all including yt tool, serach tool, math tools, embeeding tools, indexing tools ):\n",
    "## sql toolkit vs mcp : \n",
    "## stuff document chain text summarization vs map reduce summarization technique with single prompt and multiple prompt template vs refine chain summarization:\n",
    "## huggingfaceXlangchain features:\n",
    "##  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc98808",
   "metadata": {},
   "source": [
    "# LangChain Complete Guide: Level 1-3\n",
    "\n",
    "## ðŸŽ¯ Level 1: Fundamentals\n",
    "\n",
    "### 1.1 What is LangChain?\n",
    "Framework for building LLM-powered applications with composable components.\n",
    "\n",
    "### 1.2 Installation\n",
    "```bash\n",
    "pip install langchain langchain-openai langchain-community\n",
    "pip install python-dotenv  # for API keys\n",
    "```\n",
    "\n",
    "### 1.3 Basic Setup\n",
    "```python\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "```\n",
    "\n",
    "### 1.4 Simple Chat\n",
    "```python\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is Python?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "### 1.5 Prompt Templates\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Basic template\n",
    "template = \"Tell me a {adjective} joke about {topic}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"topic\"])\n",
    "\n",
    "# Generate prompt\n",
    "formatted = prompt.format(adjective=\"funny\", topic=\"cats\")\n",
    "print(formatted)\n",
    "```\n",
    "\n",
    "### 1.6 Chat Prompt Templates\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {role}\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    role=\"travel guide\",\n",
    "    user_input=\"Best places in Japan?\"\n",
    ")\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "```\n",
    "\n",
    "### 1.7 Simple Chains (LCEL)\n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Chain: Prompt â†’ LLM â†’ Output Parser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "result = chain.invoke({\"adjective\": \"silly\", \"topic\": \"programmers\"})\n",
    "print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Level 2: Intermediate\n",
    "\n",
    "### 2.1 Output Parsers\n",
    "\n",
    "**String Parser**\n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "```\n",
    "\n",
    "**JSON Parser**\n",
    "```python\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell a joke.\\n{format_instructions}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "result = chain.invoke({})\n",
    "```\n",
    "\n",
    "### 2.2 Memory\n",
    "\n",
    "**Conversation Buffer Memory**\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"Hello! How can I help?\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "```\n",
    "\n",
    "**Conversation with Memory**\n",
    "```python\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "print(conversation.predict(input=\"Hi, I'm Alice\"))\n",
    "print(conversation.predict(input=\"What's my name?\"))\n",
    "```\n",
    "\n",
    "**Window Memory (Last N messages)**\n",
    "```python\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2)  # Keep last 2 interactions\n",
    "```\n",
    "\n",
    "### 2.3 Document Loading & Processing\n",
    "\n",
    "**Load Documents**\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "\n",
    "# Text file\n",
    "loader = TextLoader(\"document.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# PDF\n",
    "pdf_loader = PyPDFLoader(\"document.pdf\")\n",
    "pages = pdf_loader.load_and_split()\n",
    "```\n",
    "\n",
    "**Text Splitting**\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "```\n",
    "\n",
    "### 2.4 Embeddings & Vector Stores\n",
    "\n",
    "**Create Embeddings**\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Save/Load\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "```\n",
    "\n",
    "**Similarity Search**\n",
    "```python\n",
    "query = \"What is the main topic?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n",
    "```\n",
    "\n",
    "### 2.5 Retrieval QA\n",
    "\n",
    "**Basic RAG**\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "answer = qa_chain.invoke(\"What is the document about?\")\n",
    "print(answer['result'])\n",
    "```\n",
    "\n",
    "**With Custom Prompt**\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following context to answer the question.\n",
    "If you don't know, say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "```\n",
    "\n",
    "### 2.6 Agents (Basic)\n",
    "\n",
    "**Simple Agent**\n",
    "```python\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "# Load tools\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run\n",
    "result = agent.run(\"What is the population of Tokyo in 2023?\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¥ Level 3: Advanced\n",
    "\n",
    "### 3.1 Custom Tools\n",
    "\n",
    "**Create Custom Tool**\n",
    "```python\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Word Length\",\n",
    "        func=get_word_length,\n",
    "        description=\"Useful for getting the length of a word\"\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
    "```\n",
    "\n",
    "**Using @tool Decorator**\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_api(query: str) -> str:\n",
    "    \"\"\"Search for information using an API.\"\"\"\n",
    "    # Your API logic here\n",
    "    return f\"Results for: {query}\"\n",
    "\n",
    "tools = [search_api]\n",
    "```\n",
    "\n",
    "### 3.2 Custom Chains\n",
    "\n",
    "**Sequential Chain**\n",
    "```python\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "\n",
    "# Chain 1: Generate synopsis\n",
    "synopsis_prompt = PromptTemplate(\n",
    "    input_variables=[\"title\"],\n",
    "    template=\"Write a synopsis for a movie titled '{title}'\"\n",
    ")\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=synopsis_prompt, output_key=\"synopsis\")\n",
    "\n",
    "# Chain 2: Generate review\n",
    "review_prompt = PromptTemplate(\n",
    "    input_variables=[\"synopsis\"],\n",
    "    template=\"Write a review based on this synopsis:\\n{synopsis}\"\n",
    ")\n",
    "review_chain = LLMChain(llm=llm, prompt=review_prompt, output_key=\"review\")\n",
    "\n",
    "# Combine\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[synopsis_chain, review_chain],\n",
    "    input_variables=[\"title\"],\n",
    "    output_variables=[\"synopsis\", \"review\"]\n",
    ")\n",
    "\n",
    "result = overall_chain({\"title\": \"The AI Revolution\"})\n",
    "```\n",
    "\n",
    "**Router Chain**\n",
    "```python\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "\n",
    "# Define specialized prompts\n",
    "physics_template = \"\"\"You are a physics expert. Answer: {input}\"\"\"\n",
    "math_template = \"\"\"You are a math expert. Answer: {input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\"name\": \"physics\", \"description\": \"Good for physics questions\", \"prompt_template\": physics_template},\n",
    "    {\"name\": \"math\", \"description\": \"Good for math questions\", \"prompt_template\": math_template}\n",
    "]\n",
    "\n",
    "# Create router\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    prompt = PromptTemplate(template=p_info['prompt_template'], input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[p_info[\"name\"]] = chain\n",
    "\n",
    "router_chain = MultiPromptChain(...)  # Simplified\n",
    "```\n",
    "\n",
    "### 3.3 Conversational RAG\n",
    "\n",
    "**With Chat History**\n",
    "```python\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "# First question\n",
    "result = qa({\"question\": \"What is LangChain?\", \"chat_history\": chat_history})\n",
    "chat_history.append((result['question'], result['answer']))\n",
    "\n",
    "# Follow-up\n",
    "result = qa({\"question\": \"Can you elaborate?\", \"chat_history\": chat_history})\n",
    "```\n",
    "\n",
    "### 3.4 Multi-Query Retrieval\n",
    "\n",
    "```python\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Generates multiple queries for better retrieval\n",
    "docs = retriever.get_relevant_documents(\"What is machine learning?\")\n",
    "```\n",
    "\n",
    "### 3.5 Streaming Responses\n",
    "\n",
    "```python\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "for chunk in chain.stream({\"topic\": \"AI\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "### 3.6 Custom Retriever\n",
    "\n",
    "```python\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    documents: list[Document]\n",
    "    \n",
    "    def get_relevant_documents(self, query: str) -> list[Document]:\n",
    "        # Custom retrieval logic\n",
    "        return [doc for doc in self.documents if query.lower() in doc.page_content.lower()]\n",
    "    \n",
    "    async def aget_relevant_documents(self, query: str) -> list[Document]:\n",
    "        return self.get_relevant_documents(query)\n",
    "\n",
    "retriever = CustomRetriever(documents=chunks)\n",
    "```\n",
    "\n",
    "### 3.7 Complex Agent with Multiple Tools\n",
    "\n",
    "```python\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Setup tools\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "tools = [\n",
    "    wikipedia,\n",
    "    search_api,  # Your custom tool\n",
    "]\n",
    "\n",
    "# Create agent\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "result = agent_executor.invoke({\"input\": \"Tell me about LangChain and search for latest news\"})\n",
    "```\n",
    "\n",
    "### 3.8 Self-Querying Retriever\n",
    "\n",
    "```python\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(name=\"source\", description=\"The source document\", type=\"string\"),\n",
    "    AttributeInfo(name=\"page\", description=\"The page number\", type=\"integer\"),\n",
    "]\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vectorstore,\n",
    "    document_contents=\"Research papers on AI\",\n",
    "    metadata_field_info=metadata_field_info\n",
    ")\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"Papers from 2023 about GPT\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Key Concepts Summary\n",
    "\n",
    "### LCEL (LangChain Expression Language)\n",
    "```python\n",
    "# Chain components with |\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Parallel execution with RunnableParallel\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    joke=joke_chain,\n",
    "    poem=poem_chain\n",
    ")\n",
    "```\n",
    "\n",
    "### Callbacks\n",
    "```python\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "chain.invoke(input_data, config={\"callbacks\": [StdOutCallbackHandler()]})\n",
    "```\n",
    "\n",
    "### Caching\n",
    "```python\n",
    "from langchain.cache import InMemoryCache\n",
    "import langchain\n",
    "\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Best Practices\n",
    "\n",
    "1. **Always handle errors** with try-except blocks\n",
    "2. **Use environment variables** for API keys\n",
    "3. **Chunk documents appropriately** (1000-2000 chars)\n",
    "4. **Add metadata** to documents for better retrieval\n",
    "5. **Use streaming** for better UX with long responses\n",
    "6. **Monitor token usage** to control costs\n",
    "7. **Test prompts** iteratively for best results\n",
    "8. **Use appropriate memory** type for your use case\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Common Patterns\n",
    "\n",
    "**RAG Pipeline**\n",
    "```\n",
    "Load â†’ Split â†’ Embed â†’ Store â†’ Retrieve â†’ Generate\n",
    "```\n",
    "\n",
    "**Agent Flow**\n",
    "```\n",
    "Input â†’ Reasoning â†’ Tool Selection â†’ Tool Execution â†’ Response\n",
    "```\n",
    "\n",
    "**Chain Types**\n",
    "- `stuff`: Put all docs in prompt (simple, token-limited)\n",
    "- `map_reduce`: Summarize each doc, then combine\n",
    "- `refine`: Iteratively refine answer with each doc\n",
    "- `map_rerank`: Score each doc's answer, return best\n",
    "\n",
    "---\n",
    "\n",
    "This guide covers essential LangChain concepts from basics to advanced patterns. Practice each level before moving to the next!\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe8ad6",
   "metadata": {},
   "source": [
    "# LangGraph Complete Guide: Level 1-3\n",
    "\n",
    "## ðŸŽ¯ Level 1: Fundamentals\n",
    "\n",
    "### 1.1 What is LangGraph?\n",
    "Framework for building **stateful, multi-actor applications** with LLMs using **graphs**. Think of it as state machines for AI agents.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Nodes**: Functions that process state\n",
    "- **Edges**: Connections between nodes\n",
    "- **State**: Shared data structure\n",
    "- **Graph**: The workflow definition\n",
    "\n",
    "### 1.2 Installation\n",
    "```bash\n",
    "pip install langgraph langchain langchain-openai\n",
    "```\n",
    "\n",
    "### 1.3 Basic Setup\n",
    "```python\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "```\n",
    "\n",
    "### 1.4 Define State\n",
    "```python\n",
    "class State(TypedDict):\n",
    "    messages: list[str]\n",
    "    counter: int\n",
    "```\n",
    "\n",
    "### 1.5 Simple Graph - Hello World\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define state\n",
    "class State(TypedDict):\n",
    "    message: str\n",
    "\n",
    "# Define node function\n",
    "def say_hello(state: State) -> State:\n",
    "    return {\"message\": \"Hello, World!\"}\n",
    "\n",
    "# Build graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"hello\", say_hello)\n",
    "workflow.set_entry_point(\"hello\")\n",
    "workflow.add_edge(\"hello\", END)\n",
    "\n",
    "# Compile and run\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\"message\": \"\"})\n",
    "print(result)  # {'message': 'Hello, World!'}\n",
    "```\n",
    "\n",
    "### 1.6 Linear Chain\n",
    "```python\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "\n",
    "def node_1(state: State) -> State:\n",
    "    return {\"output\": f\"Processed: {state['input']}\"}\n",
    "\n",
    "def node_2(state: State) -> State:\n",
    "    return {\"output\": f\"{state['output']} -> Enhanced\"}\n",
    "\n",
    "# Build\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"process\", node_1)\n",
    "workflow.add_node(\"enhance\", node_2)\n",
    "\n",
    "workflow.set_entry_point(\"process\")\n",
    "workflow.add_edge(\"process\", \"enhance\")\n",
    "workflow.add_edge(\"enhance\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\"input\": \"Hello\"})\n",
    "print(result['output'])  # Processed: Hello -> Enhanced\n",
    "```\n",
    "\n",
    "### 1.7 Conditional Edges\n",
    "```python\n",
    "class State(TypedDict):\n",
    "    number: int\n",
    "    result: str\n",
    "\n",
    "def check_number(state: State) -> State:\n",
    "    return state\n",
    "\n",
    "def positive_path(state: State) -> State:\n",
    "    return {\"result\": \"Number is positive\"}\n",
    "\n",
    "def negative_path(state: State) -> State:\n",
    "    return {\"result\": \"Number is negative\"}\n",
    "\n",
    "# Conditional routing function\n",
    "def route_number(state: State) -> str:\n",
    "    if state[\"number\"] > 0:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "# Build graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"check\", check_number)\n",
    "workflow.add_node(\"positive\", positive_path)\n",
    "workflow.add_node(\"negative\", negative_path)\n",
    "\n",
    "workflow.set_entry_point(\"check\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check\",\n",
    "    route_number,\n",
    "    {\n",
    "        \"positive\": \"positive\",\n",
    "        \"negative\": \"negative\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"positive\", END)\n",
    "workflow.add_edge(\"negative\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "print(app.invoke({\"number\": 5}))   # positive\n",
    "print(app.invoke({\"number\": -3}))  # negative\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Level 2: Intermediate\n",
    "\n",
    "### 2.1 Chat Agent with LLM\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# State with message reducer\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def chatbot(state: AgentState) -> AgentState:\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", chatbot)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Use\n",
    "result = app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Tell me a joke\")]\n",
    "})\n",
    "print(result[\"messages\"][-1].content)\n",
    "```\n",
    "\n",
    "### 2.2 Memory with Checkpointing\n",
    "```python\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Use with thread_id for conversation\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation-1\"}}\n",
    "\n",
    "result1 = app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"My name is Alice\")]\n",
    "}, config)\n",
    "\n",
    "result2 = app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's my name?\")]\n",
    "}, config)\n",
    "\n",
    "print(result2[\"messages\"][-1].content)  # Should remember \"Alice\"\n",
    "```\n",
    "\n",
    "### 2.3 Agent with Tools\n",
    "```python\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "tools = [multiply, add]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def agent(state: AgentState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"end\"\n",
    "\n",
    "# Build graph\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Use\n",
    "result = app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What is 5 multiplied by 3?\")]\n",
    "})\n",
    "print(result[\"messages\"][-1].content)\n",
    "```\n",
    "\n",
    "### 2.4 Human-in-the-Loop\n",
    "```python\n",
    "from langgraph.graph import END\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    approval: str\n",
    "\n",
    "def agent_node(state: State):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def human_review(state: State):\n",
    "    # This will pause and wait for human input\n",
    "    return state\n",
    "\n",
    "def check_approval(state: State) -> str:\n",
    "    if state.get(\"approval\") == \"approved\":\n",
    "        return \"continue\"\n",
    "    return \"end\"\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"human\", human_review)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_edge(\"agent\", \"human\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"human\",\n",
    "    check_approval,\n",
    "    {\n",
    "        \"continue\": \"agent\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.compile(checkpointer=MemorySaver(), interrupt_before=[\"human\"])\n",
    "\n",
    "# Use\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "result = app.invoke({\"messages\": [HumanMessage(\"Generate a report\")]}, config)\n",
    "\n",
    "# Later, after review\n",
    "app.update_state(config, {\"approval\": \"approved\"})\n",
    "result = app.invoke(None, config)\n",
    "```\n",
    "\n",
    "### 2.5 Subgraphs\n",
    "```python\n",
    "# Create a subgraph\n",
    "def create_research_graph():\n",
    "    class ResearchState(TypedDict):\n",
    "        query: str\n",
    "        results: list[str]\n",
    "    \n",
    "    def search(state: ResearchState):\n",
    "        return {\"results\": [f\"Result for {state['query']}\"]}\n",
    "    \n",
    "    sub_workflow = StateGraph(ResearchState)\n",
    "    sub_workflow.add_node(\"search\", search)\n",
    "    sub_workflow.set_entry_point(\"search\")\n",
    "    sub_workflow.add_edge(\"search\", END)\n",
    "    \n",
    "    return sub_workflow.compile()\n",
    "\n",
    "# Use in main graph\n",
    "research_graph = create_research_graph()\n",
    "\n",
    "class MainState(TypedDict):\n",
    "    topic: str\n",
    "    research_data: list[str]\n",
    "\n",
    "def research_node(state: MainState):\n",
    "    result = research_graph.invoke({\"query\": state[\"topic\"]})\n",
    "    return {\"research_data\": result[\"results\"]}\n",
    "\n",
    "workflow = StateGraph(MainState)\n",
    "workflow.add_node(\"research\", research_node)\n",
    "workflow.set_entry_point(\"research\")\n",
    "workflow.add_edge(\"research\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "```\n",
    "\n",
    "### 2.6 Parallel Execution\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    output_a: str\n",
    "    output_b: str\n",
    "    final: str\n",
    "\n",
    "def task_a(state: State):\n",
    "    return {\"output_a\": f\"Task A: {state['input']}\"}\n",
    "\n",
    "def task_b(state: State):\n",
    "    return {\"output_b\": f\"Task B: {state['input']}\"}\n",
    "\n",
    "def combine(state: State):\n",
    "    return {\"final\": f\"{state['output_a']} + {state['output_b']}\"}\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"task_a\", task_a)\n",
    "workflow.add_node(\"task_b\", task_b)\n",
    "workflow.add_node(\"combine\", combine)\n",
    "\n",
    "workflow.set_entry_point(\"task_a\")\n",
    "workflow.set_entry_point(\"task_b\")  # Both start simultaneously\n",
    "workflow.add_edge(\"task_a\", \"combine\")\n",
    "workflow.add_edge(\"task_b\", \"combine\")\n",
    "workflow.add_edge(\"combine\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¥ Level 3: Advanced\n",
    "\n",
    "### 3.1 Multi-Agent System\n",
    "```python\n",
    "class MultiAgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    next_agent: str\n",
    "    task_complete: bool\n",
    "\n",
    "# Define specialist agents\n",
    "researcher_llm = llm.bind_tools([]).with_structured_output({\"research\": \"string\"})\n",
    "writer_llm = llm.bind_tools([]).with_structured_output({\"draft\": \"string\"})\n",
    "critic_llm = llm.bind_tools([]).with_structured_output({\"feedback\": \"string\"})\n",
    "\n",
    "def researcher(state: MultiAgentState):\n",
    "    prompt = f\"Research this topic: {state['messages'][-1].content}\"\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=result.content, name=\"Researcher\")],\n",
    "        \"next_agent\": \"writer\"\n",
    "    }\n",
    "\n",
    "def writer(state: MultiAgentState):\n",
    "    research = state[\"messages\"][-1].content\n",
    "    prompt = f\"Write an article based on: {research}\"\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=result.content, name=\"Writer\")],\n",
    "        \"next_agent\": \"critic\"\n",
    "    }\n",
    "\n",
    "def critic(state: MultiAgentState):\n",
    "    draft = state[\"messages\"][-1].content\n",
    "    prompt = f\"Critique this draft: {draft}\"\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Simplified: assume always approved\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=result.content, name=\"Critic\")],\n",
    "        \"next_agent\": \"end\",\n",
    "        \"task_complete\": True\n",
    "    }\n",
    "\n",
    "def router(state: MultiAgentState) -> str:\n",
    "    return state.get(\"next_agent\", \"end\")\n",
    "\n",
    "# Build\n",
    "workflow = StateGraph(MultiAgentState)\n",
    "workflow.add_node(\"researcher\", researcher)\n",
    "workflow.add_node(\"writer\", writer)\n",
    "workflow.add_node(\"critic\", critic)\n",
    "\n",
    "workflow.set_entry_point(\"researcher\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"researcher\",\n",
    "    router,\n",
    "    {\"writer\": \"writer\", \"end\": END}\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"writer\",\n",
    "    router,\n",
    "    {\"critic\": \"critic\", \"end\": END}\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"critic\",\n",
    "    router,\n",
    "    {\"writer\": \"writer\", \"end\": END}\n",
    ")\n",
    "\n",
    "app = workflow.compile()\n",
    "```\n",
    "\n",
    "### 3.2 Self-Reflection Loop\n",
    "```python\n",
    "class ReflectionState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    iterations: int\n",
    "    max_iterations: int\n",
    "    is_good: bool\n",
    "\n",
    "def generate(state: ReflectionState):\n",
    "    prompt = state[\"messages\"][-1].content if state[\"messages\"] else \"Write a story\"\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=result.content, name=\"Generator\")],\n",
    "        \"iterations\": state.get(\"iterations\", 0) + 1\n",
    "    }\n",
    "\n",
    "def reflect(state: ReflectionState):\n",
    "    content = state[\"messages\"][-1].content\n",
    "    prompt = f\"Critique and improve this: {content}\"\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Simple quality check\n",
    "    is_good = \"excellent\" in result.content.lower() or state[\"iterations\"] >= state[\"max_iterations\"]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=result.content, name=\"Reflector\")],\n",
    "        \"is_good\": is_good\n",
    "    }\n",
    "\n",
    "def should_continue(state: ReflectionState) -> str:\n",
    "    if state.get(\"is_good\", False):\n",
    "        return \"end\"\n",
    "    if state.get(\"iterations\", 0) >= state.get(\"max_iterations\", 3):\n",
    "        return \"end\"\n",
    "    return \"continue\"\n",
    "\n",
    "workflow = StateGraph(ReflectionState)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"reflect\", reflect)\n",
    "\n",
    "workflow.set_entry_point(\"generate\")\n",
    "workflow.add_edge(\"generate\", \"reflect\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"reflect\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"generate\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "result = app.invoke({\n",
    "    \"messages\": [HumanMessage(\"Write a short story about AI\")],\n",
    "    \"max_iterations\": 3\n",
    "})\n",
    "```\n",
    "\n",
    "### 3.3 Planning and Execution Agent\n",
    "```python\n",
    "class PlanExecuteState(TypedDict):\n",
    "    input: str\n",
    "    plan: list[str]\n",
    "    past_steps: list[tuple[str, str]]\n",
    "    current_step: int\n",
    "    final_response: str\n",
    "\n",
    "def planner(state: PlanExecuteState):\n",
    "    prompt = f\"Create a step-by-step plan for: {state['input']}\"\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Parse plan (simplified)\n",
    "    plan = [line.strip() for line in result.content.split('\\n') if line.strip()]\n",
    "    \n",
    "    return {\n",
    "        \"plan\": plan,\n",
    "        \"current_step\": 0\n",
    "    }\n",
    "\n",
    "def executor(state: PlanExecuteState):\n",
    "    current_step = state[\"current_step\"]\n",
    "    step = state[\"plan\"][current_step]\n",
    "    \n",
    "    prompt = f\"Execute this step: {step}\\nContext: {state['past_steps']}\"\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    past_steps = state.get(\"past_steps\", [])\n",
    "    past_steps.append((step, result.content))\n",
    "    \n",
    "    return {\n",
    "        \"past_steps\": past_steps,\n",
    "        \"current_step\": current_step + 1\n",
    "    }\n",
    "\n",
    "def should_continue(state: PlanExecuteState) -> str:\n",
    "    if state[\"current_step\"] >= len(state[\"plan\"]):\n",
    "        return \"finalize\"\n",
    "    return \"execute\"\n",
    "\n",
    "def finalizer(state: PlanExecuteState):\n",
    "    prompt = f\"Summarize the results: {state['past_steps']}\"\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"final_response\": result.content}\n",
    "\n",
    "workflow = StateGraph(PlanExecuteState)\n",
    "workflow.add_node(\"planner\", planner)\n",
    "workflow.add_node(\"executor\", executor)\n",
    "workflow.add_node(\"finalizer\", finalizer)\n",
    "\n",
    "workflow.set_entry_point(\"planner\")\n",
    "workflow.add_edge(\"planner\", \"executor\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"executor\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"execute\": \"executor\",\n",
    "        \"finalize\": \"finalizer\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"finalizer\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "```\n",
    "\n",
    "### 3.4 Dynamic Routing with Context\n",
    "```python\n",
    "from typing import Literal\n",
    "\n",
    "class RouterState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    intent: str\n",
    "    data: dict\n",
    "\n",
    "def classifier(state: RouterState):\n",
    "    content = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Use LLM to classify intent\n",
    "    prompt = f\"Classify this request into: 'search', 'calculate', or 'chat': {content}\"\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    intent = \"chat\"  # default\n",
    "    if \"search\" in result.content.lower():\n",
    "        intent = \"search\"\n",
    "    elif \"calculate\" in result.content.lower():\n",
    "        intent = \"calculate\"\n",
    "    \n",
    "    return {\"intent\": intent}\n",
    "\n",
    "def search_handler(state: RouterState):\n",
    "    # Simulate search\n",
    "    return {\"messages\": [AIMessage(content=\"Search results...\")]}\n",
    "\n",
    "def calculate_handler(state: RouterState):\n",
    "    return {\"messages\": [AIMessage(content=\"Calculation result...\")]}\n",
    "\n",
    "def chat_handler(state: RouterState):\n",
    "    result = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [result]}\n",
    "\n",
    "def route_intent(state: RouterState) -> str:\n",
    "    return state.get(\"intent\", \"chat\")\n",
    "\n",
    "workflow = StateGraph(RouterState)\n",
    "workflow.add_node(\"classifier\", classifier)\n",
    "workflow.add_node(\"search\", search_handler)\n",
    "workflow.add_node(\"calculate\", calculate_handler)\n",
    "workflow.add_node(\"chat\", chat_handler)\n",
    "\n",
    "workflow.set_entry_point(\"classifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"classifier\",\n",
    "    route_intent,\n",
    "    {\n",
    "        \"search\": \"search\",\n",
    "        \"calculate\": \"calculate\",\n",
    "        \"chat\": \"chat\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"search\", END)\n",
    "workflow.add_edge(\"calculate\", END)\n",
    "workflow.add_edge(\"chat\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "```\n",
    "\n",
    "### 3.5 Streaming Responses\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# Streaming events\n",
    "async def stream_example():\n",
    "    async for event in app.astream({\"messages\": [HumanMessage(\"Hello\")]}):\n",
    "        print(event)\n",
    "\n",
    "# Streaming tokens\n",
    "async def stream_tokens():\n",
    "    async for chunk in app.astream_log({\"messages\": [HumanMessage(\"Tell me a story\")]}):\n",
    "        print(chunk)\n",
    "\n",
    "# Synchronous streaming\n",
    "for event in app.stream({\"messages\": [HumanMessage(\"Hello\")]}, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "```\n",
    "\n",
    "### 3.6 Time Travel / State Replay\n",
    "```python\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Execute multiple steps\n",
    "app.invoke({\"messages\": [HumanMessage(\"Step 1\")]}, config)\n",
    "app.invoke({\"messages\": [HumanMessage(\"Step 2\")]}, config)\n",
    "app.invoke({\"messages\": [HumanMessage(\"Step 3\")]}, config)\n",
    "\n",
    "# Get all checkpoints\n",
    "checkpoints = list(app.get_state_history(config))\n",
    "\n",
    "# Go back to previous state\n",
    "for checkpoint in checkpoints:\n",
    "    print(f\"Step: {checkpoint.values}\")\n",
    "    \n",
    "# Resume from a specific checkpoint\n",
    "app.invoke(None, checkpoints[1].config)\n",
    "```\n",
    "\n",
    "### 3.7 Custom Checkpointer (Database)\n",
    "```python\n",
    "from langgraph.checkpoint.base import BaseCheckpointSaver\n",
    "from typing import Optional\n",
    "\n",
    "class DatabaseCheckpointer(BaseCheckpointSaver):\n",
    "    def __init__(self, db_connection):\n",
    "        self.db = db_connection\n",
    "        super().__init__()\n",
    "    \n",
    "    def put(self, config, checkpoint, metadata):\n",
    "        # Save to database\n",
    "        thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "        # self.db.save(thread_id, checkpoint, metadata)\n",
    "        pass\n",
    "    \n",
    "    def get(self, config):\n",
    "        # Load from database\n",
    "        thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "        # return self.db.load(thread_id)\n",
    "        pass\n",
    "\n",
    "# Usage\n",
    "# db_checkpoint = DatabaseCheckpointer(your_db)\n",
    "# app = workflow.compile(checkpointer=db_checkpoint)\n",
    "```\n",
    "\n",
    "### 3.8 Hierarchical Agent Teams\n",
    "```python\n",
    "class TeamState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    team: str\n",
    "    task_results: dict\n",
    "\n",
    "# Team 1: Research Team\n",
    "def create_research_team():\n",
    "    def researcher(state):\n",
    "        return {\"messages\": [AIMessage(content=\"Research done\", name=\"Researcher\")]}\n",
    "    \n",
    "    def analyst(state):\n",
    "        return {\"messages\": [AIMessage(content=\"Analysis done\", name=\"Analyst\")]}\n",
    "    \n",
    "    workflow = StateGraph(TeamState)\n",
    "    workflow.add_node(\"researcher\", researcher)\n",
    "    workflow.add_node(\"analyst\", analyst)\n",
    "    workflow.set_entry_point(\"researcher\")\n",
    "    workflow.add_edge(\"researcher\", \"analyst\")\n",
    "    workflow.add_edge(\"analyst\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Team 2: Writing Team\n",
    "def create_writing_team():\n",
    "    def writer(state):\n",
    "        return {\"messages\": [AIMessage(content=\"Draft written\", name=\"Writer\")]}\n",
    "    \n",
    "    def editor(state):\n",
    "        return {\"messages\": [AIMessage(content=\"Edited\", name=\"Editor\")]}\n",
    "    \n",
    "    workflow = StateGraph(TeamState)\n",
    "    workflow.add_node(\"writer\", writer)\n",
    "    workflow.add_node(\"editor\", editor)\n",
    "    workflow.set_entry_point(\"writer\")\n",
    "    workflow.add_edge(\"writer\", \"editor\")\n",
    "    workflow.add_edge(\"editor\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Supervisor\n",
    "research_team = create_research_team()\n",
    "writing_team = create_writing_team()\n",
    "\n",
    "class SupervisorState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    next_team: str\n",
    "\n",
    "def supervisor(state: SupervisorState):\n",
    "    # Decide which team to use\n",
    "    prompt = f\"Which team should handle: {state['messages'][-1].content}? (research/writing)\"\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    if \"research\" in result.content.lower():\n",
    "        return {\"next_team\": \"research\"}\n",
    "    return {\"next_team\": \"writing\"}\n",
    "\n",
    "def research_node(state):\n",
    "    result = research_team.invoke(state)\n",
    "    return {\"messages\": result[\"messages\"]}\n",
    "\n",
    "def writing_node(state):\n",
    "    result = writing_team.invoke(state)\n",
    "    return {\"messages\": result[\"messages\"]}\n",
    "\n",
    "def route_team(state: SupervisorState) -> str:\n",
    "    return state.get(\"next_team\", \"end\")\n",
    "\n",
    "workflow = StateGraph(SupervisorState)\n",
    "workflow.add_node(\"supervisor\", supervisor)\n",
    "workflow.add_node(\"research_team\", research_node)\n",
    "workflow.add_node(\"writing_team\", writing_node)\n",
    "\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    route_team,\n",
    "    {\n",
    "        \"research\": \"research_team\",\n",
    "        \"writing\": \"writing_team\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"research_team\", END)\n",
    "workflow.add_edge(\"writing_team\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Key Patterns\n",
    "\n",
    "### State Management\n",
    "```python\n",
    "# Reducer functions for state updates\n",
    "from operator import add\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Append to list\n",
    "    messages: Annotated[list, add_messages]\n",
    "    \n",
    "    # Replace value\n",
    "    counter: int\n",
    "    \n",
    "    # Custom reducer\n",
    "    scores: Annotated[list[int], add]\n",
    "```\n",
    "\n",
    "### Error Handling\n",
    "```python\n",
    "def safe_node(state: State):\n",
    "    try:\n",
    "        # Your logic\n",
    "        return {\"result\": \"success\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"result\": \"failed\"}\n",
    "```\n",
    "\n",
    "### Conditional Logic\n",
    "```python\n",
    "def router(state: State) -> Literal[\"path_a\", \"path_b\", \"end\"]:\n",
    "    if state[\"condition\"]:\n",
    "        return \"path_a\"\n",
    "    elif state[\"other_condition\"]:\n",
    "        return \"path_b\"\n",
    "    return \"end\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ Best Practices\n",
    "\n",
    "1. **State Design**: Keep state minimal and clear\n",
    "2. **Type Hints**: Always use TypedDict for state\n",
    "3. **Node Functions**: Keep them pure and focused\n",
    "4. **Error Handling**: Wrap nodes in try-except\n",
    "5. **Checkpointing**: Use for production deployments\n",
    "6. **Testing**: Test individual nodes before graph\n",
    "7. **Visualization**: Use `app.get_graph().draw_mermaid()` to visualize\n",
    "8. **Memory**: Choose appropriate checkpointer (Memory/SQLite/Postgres)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Debugging\n",
    "\n",
    "### Visualize Graph\n",
    "```python\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Or as Mermaid text\n",
    "print(app.get_graph().draw_mermaid())\n",
    "```\n",
    "\n",
    "### Inspect State\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "current_state = app.get_state(config)\n",
    "print(current_state.values)\n",
    "print(current_state.next)  # Next node to execute\n",
    "```\n",
    "\n",
    "### Debug Mode\n",
    "```python\n",
    "# Add debug info to nodes\n",
    "def debug_node(state: State):\n",
    "    print(f\"Current state: {state}\")\n",
    "    result = process(state)\n",
    "    print(f\"Output: {result}\")\n",
    "    return result\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Common Use Cases\n",
    "\n",
    "1. **Chatbot with Memory**: Simple conversation agent\n",
    "2. **RAG Agent**: Retrieval + Generation with tools\n",
    "3. **Multi-Agent Research**: Specialized agents collaborate\n",
    "4. **Plan-Execute**: Break down and execute complex tasks\n",
    "5. **Human-in-Loop**: Review and approve agent actions\n",
    "6. **Reflection**: Self-improving outputs\n",
    "7. **Hierarchical Teams**: Manager + worker agents\n",
    "8. **State Machine**: Complex workflows with conditions\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Quick Reference\n",
    "\n",
    "```python\n",
    "# Basic Structure\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"name\", function)\n",
    "workflow.set_entry_point(\"name\")\n",
    "workflow.add_edge(\"from\", \"to\")\n",
    "workflow.add_conditional_edges(\"from\", router, {\"path\": \"node\"})\n",
    "app = workflow.compile()\n",
    "\n",
    "# With Memory\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Invoke\n",
    "result = app.invoke(input, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "# Stream\n",
    "for event in app.stream(input):\n",
    "    print(event)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This guide covers LangGraph from basics to advanced patterns. Master Level 1 before moving to complex multi-agent systems! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
